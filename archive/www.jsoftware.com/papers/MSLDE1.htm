<html>
<head><meta http-equiv="content-type" content="text/html;charset=utf-8">
<title>MSLDE</title>
<link href="jdoc.css" rel=stylesheet>
</head>

<body>

<br>

<table width=520 align=center><tr><td>

<p align=center><font size=+2>Machine Solutions of Linear Differential Equations<br>
Applications to a Dynamic Economic Model</font><br>
<br>&nbsp;</p>

<p align=center>A thesis presented by<br><br>
Kenneth E. Iverson<br><br>
to<br><br>
The Division of Applied Science<br>
in partial fulfullment of the requirements<br>
for the degree of<br>
Doctor of Philosophy<br>
in the subject of Applied Mathematics
<br>&nbsp;</p>

<p align=center>Harvard University<br> 
Cambridge, Massachusetts, January, 1954
<br>&nbsp;</p>

<br><hr>




&nbsp;

<a name="preface"></a>
<p><b>Preface</b></p>

<p>The present use of the Mark IV Calculator in the solution
of a new dynamic economic model continues the previous work
of Dr. H.F. Mitchell in this field.
The numerical results obtained should be of interest
to economists, 
and it is believed that a number of the methods employed 
will prove useful to computers.</p>

<p>The writer wishes to express his appreciation 
to Professor Howard H. Aiken for his helpful guidance and encouragement,
and to Professor Wassily W. Leontief who proposed the problem
and offered many valuable suggests for its solution.
The advice and assistance of numerous members 
of the Staff of the Computation Laboratory is gratefully acknowledged,
in particular that of Dr. Robert C. Minnick and
Mr. Anthony G. Oettinger.
To Mr. James R. Terrell the writer is indebted for profitable discussions
arising out of his use of the matrix subroutines described herein.</p>

<p>The writer also wishes to express his thanks
to Miss Jacquelin Sanborn who typed the plates for printing
and to Miss Carmela Ciampa, Miss Joan Boyle, Mr. Robert Burns
and Mr. Paul Donaldson,
all of whom assisted in the preparation of the manuscript.
<br>&nbsp;<br>&nbsp;</p>





<a name="synopsis"></a>
<p><b>Synopsis</b></p>

<p>The present study was undertaken to obtain solutions
to a dynamic economic model proposed by Leontief,
and to develop methods of computation suitable 
to the Mark IV Computer.</p>

<p>The economic model as presented in Chapter 1 is described
by a system of linear differential equations with constant coefficients.
The data for six systems of order 5, 6, 10, 11, 20 and 21 were supplied
by the Bureau of Labor Statistics 
and the Harvard Economic Research Project.</p>

<p>The general solution for each system was sought.
In Chapter 3 the complementary function is shown to be
expressible in terms of the latent roots and principal vectors
of a certain matrix related to the coefficient matrices
of the given system.
An expression is also given for the particular integrals
appropriate to a set of demand functions of interest
in economic theory.</p>

<p>The so-called Frame method, described in Chapter 4,
was chosen for obtaining the latent roots and principal vectors.
The Frame process generates the characteristic equation
of the matrix and the latent roots of the matrix 
are then obtained as the roots of the characteristic equation.
Methods of solution of polynomial equations of higher degree
are therefore discussed in Chapter 5.</p>

<p>In Chapter 5 a modification of the quadratic factor method
is described which has the advantage of greater uniformity
for machine computation.
Extensions to a higher order process and to the extraction
of quartic factors are considered and a new method for the solution
of the quartic is described.
A machine method based on a combination of the Graeffe process
and the modified quadratic factor method is also proposed.
The method is very general in its application 
and offers several advantages,
the most important among them being:</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</d>
 <td>No preliminary scaling of coefficients or bounds
 on the roots are required;</td></tr>
<tr><td valign=top>2. &nbsp;</d>
 <td>The number of multiplications required for the evaluation
 of all the roots of a polynomial of degree n is of the order
 n<sup>2</sup>.</td></tr>
</table>

<p>The above method has been used for the solution 
of several equations of degrees up to twenty.</p>

<p>Machine programs for the matrix operations required
were organized around a set of matrix subroutines
described in Chapter 6.
These subroutines are programmed for systems of a general order
and are believed to be sufficiently flexible and easy to apply
to be of value in general use.
They have already been profitably used by another member
of the Staff of the Computation Laboratory
in an unrelated program.</p>

<p>In addition to the standard matrix operations programmed,
a new machine method of inversion is described.
For machine computation,
it has the advantages of uniformity, low storage requirements,
and low round-off error.
Several possible extensions to complex matrices are considered.
A number of special machine techniques for vector algebra are suggested;
among them a process called &ldquo;floating vector operation&rdquo;
which offers most of the advantages of floating decimal point operations
at a fraction of the computing time required by the latter.
This process was applied in the calculation of the principal vectors.
The requisite details of Mark IV programming 
are considered in Chapter 2.</p>

<p>Particular solutions were obtained for the six systems described.
The results appear in the appendices and are described in Chapter 7.
The complete complementary function was obtained for the systems
of order 5, 6, 10 and 11.
Because of round-off error accumulation only a partial solution
was obtained for the system of order 20
and a complete solution would necessitate rerunning with greater accuracy.
In view of the solutions of the similar lower order systems already obtained,
the computation required is not warranted by the economic significance 
of the missing terms.
<br>&nbsp;<br>&nbsp;</p>





<a name="1"></a>
<p><b>Chapter 1 &nbsp; A Dynamic Economic Model</b></p>

<p>Mathematical models have long been used with striking success
in the physical sciences and hence attempts have been made 
to apply them to such diverse fields as economics, biology, and psychology.
Early attempts at the application of mathematical models to the field
of economics were hampered by (1) the lack of empirical data and
(2) the lack of mechanical devices to carry out the extensive
calculations required.
As a result, economic models were either so over-simplified
that they no longer adequately represented real phenomena,
or else so complex as to render impossible the calculation
of numerical results based upon them.</p>

<p>This situation has been radically changed by two factors:
(1) the vast amount of statistical data now collected 
and made available by government and other agencies; and
(2) the advent of the large scale automatic calculator
which makes feasible the construction of models of
much greater complexity than heretofore.</p>

<p>The present study was undertaken to obtain
general solutions of a dynamic model proposed by Leontief.
This is an extension of his earlier static input-output model.
Both models are fully discussed in 
<acronym title="1. Leontief, W., The Structure of American Economy, 1919-1939, 
Oxford University Press, 1951.">references 1</acronym>
and 
<acronym title="2. Leontief, W., Studies in the Structure of American Economy, 
Oxford University Press, 1953.">2</acronym>
and only a brief outline will be attempted here.</p>

<p>The variables of the static input-output model
are the outputs of the various &ldquo;sectors&rdquo;
or &ldquo;industries&rdquo; which make up the economy.
It is assumed that each sector of the economy will require inputs
from some or all of the other sectors,
and that a state of equilibrium exists.
That is, if x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>
are the outputs of the several sectors,
X<sub>ij</sub> is the input from the ith sector
to the jth sector,
and z<sub>i</sub> is the &ldquo;final demand&rdquo;
or amount of goods to be made available for use outside
the system considered, then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>i</sub> &ndash;<img src="MSLDEimg/sigmajn.bmp" align=middle> X<sub>ij</sub> = z<sub>i</sub> .
</td></tr></table>

<p>The inputs to each sector are assumed<sup><a href="MSLDE1.htm#note1a">[a]</a></sup>
to be linear functions of the output of that sector,
i.e. X<sub>ij</sub> = a<sub>ij</sub>x<sub>j</sub>.
The &ldquo;flow&rdquo; coefficients a<sub>ij</sub>
are assumed to be constants so that the equilibrium
condition may now be stated as a set of
n linear equations in n unknowns,</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>i</sub> &ndash;<img src="MSLDEimg/sigmajn.bmp" align=middle> a<sub>ij</sub>x<sub>j</sub> = z<sub>i</sub> ,
&nbsp; &nbsp; i = 1,2,&#133;,n.
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> 
(1.1)
</td></tr></table>

<p>For a given final demand z<sub>i</sub>, 
the required outputs x<sub>i</sub> of each sector
may now be obtained as a solution of (1.1).</p>

<p>The division of the economy into sectors is somewhat arbitrary.
In general, the greater the number of sectors considered,
the more detailed is the information available in the solution.
Since, however, the amount of calculation required varies
approximately as the cube of the number of sectors considered,
a compromise must be made.
Solutions<sup><acronym title="3. Mitchell, H.F., “The Machine Solution of Simultaneous Linear Systems”, Doctoral Theses, Harvard University, 1948.">3</acronym>&nbsp;</sup>
have been obtained for systems of various orders,
the largest being a system of 192 sectors.</p>

<p>Equations (1.1) describe a static model since no dependence
on time is implied.
Dependence on time may be introduced by making
either the coefficient a<sub>ij</sub>
or the demand function z<sub>i</sub> (or both)
functions of time.
The simple linear model may also be extended in other ways;
e.g. by including terms of higher degree in the variables
or by adding terms in the derivatives
<i>d</i>x<sub>i</sub>/<i>d</i>t.
For reasons discussed in
<acronym title="2. Leontief, W., Studies in the Structure of American Economy, Oxford University Press, 1953.">reference 2</acronym>
the following dynamic model is chosen<sup><a href="MSLDE1.htm#note1c">[b]</a>:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>i</sub> &ndash;<img src="MSLDEimg/sigmajn.bmp" align=middle> a<sub>ij</sub>x<sub>j</sub> 
&ndash;<img src="MSLDEimg/sigmajn.bmp" align=middle> b<sub>ij</sub></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center><i>d</i>x<sub>j</sub></td></tr>
 <tr><td><img src="MSLDEimg/22x1.bmp"></td></tr>
 <tr><td align=center><i>d</i>t</td></tr>
 </table></td>
<td>= z<sub>i</sub> ;
&nbsp; &nbsp; i = 1,2,&#133;,n.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(1.2)
</td></tr></table>

<p>The flow coefficients a<sub>ij</sub> and the capital coefficients
b<sub>ij</sub> are assumed to be constants;
the outputs x<sub>i</sub> and the final demands z<sub>i</sub>
are explicit functions of time.</p>

<p>The system of first-order linear differential equations
represented by (1.2) may be solved in either of two ways:</p>

<table>
<tr><td valign=top>(1) &nbsp;</td><td>
 Given a set of initial conditions x<sub>i</sub>(0) = x<sub>i0</sub>,
 a particular solution may be tabulated for successive values
 of t by methods of numerical integration.
 </td></tr>
<tr><td valign=top>(2)</td><td>
 A general solution may be obtained containing in general
 n arbitrary parameters which may be chosen so as
 to satisfy any<sup><a href="MSLDE1.htm#note1c">[c]</a></sup>
 desired initial conditions.
 </td></tr>
</table>

<p>The general solution is more difficult to obtain;
however, it is of much greater value in studying 
the behavior of the system than is the particular solution.
In the first place,
the general solution is expressible as a sum
of exponential functions.
As in the analogous case of a coupled mechanical system
[also described by (1.2)]
a knowledge of the so-called &ldquo;normal modes&rdquo;
or terms of the general solution provides 
greater insight into the behavior of the system
than any tabulated solution,
however extensive.
Secondly, the particular solution corresponding
to any given initial conditions may be obtained
fairly readily from the general solution
by evaluating the arbitrary parameters.</p>

<p>A number of difficulties may arise in connection 
with the general solution.
They may be classified either as mathematical difficulties
or as difficulties of interpretation of the model.
The mathematical difficulties are problems arising 
in the actual solution of the system of equations
represented by (1.2).
They are given consideration in the following chapters.
The difficulties of interpretation arise if in certain cases
the model does not adequately represent the economic
phenomena it purports to describe.
These matters will not be discussed here,
and again the reader is referred to
<acronym title="2. Leontief, W., Studies in the Structure of American Economy, Oxford University Press, 1953.">reference 
2</acronym>.</p>

<p>Presumably a final demand can be considered 
as the input to an additional sector of the economy
and by including this sector in the system
the equation (1.2) become homogeneous.
Such a system is referred to as &ldquo;closed&rdquo;.
In order to separate the less stable<sup><a href="MSLDE1.htm#note1d">[d]</a></sup>
elements of the economy from the more stable,
it is convenient to remove the sector classified as
&ldquo;households&rdquo;from the system 
and treat it as a final demand on the remaining system.
Such a system will be termed an &ldquo;open&rdquo; system.</p>

<p>The demand function may in general be any function of time,
but the functions of most interest are of the form of exponentials,
i.e. z<sub>i</sub> = g<sub>i</sub>e<sup>&mu;t</sup> where
the g<sub>i</sub> are constants.
Solutions for demand functions of this kind
are discussed in section 3F.</p>

<p>Flow coefficients for a system of 192 sectors were provided
by the Bureau of Labor Statistics and formed the basis 
for the present calculations.
Capital coefficients were supplied by the Harvard Economic Research Project.
By a processor of &ldquo;aggregation&rdquo; described 
in Chapter IX of
<acronym title="2. Leontief, W., Studies in the Structure of American Economy, Oxford University Press, 1953.">reference 
2</acronym>,
the coefficients for several systems of different orders 
were computed from the original data.
The orders of the systems chosen were 6, 11 and 21.
Comparison of the solutions for the various systems 
may be expected to give some indication of the distortion
introduced by aggregation.
Three &ldquo;open&rdquo; systems of order 5, 10 and 20
were derived from the systems of order 6, 11 and 21 
by suppressing the &ldquo;households&rdquo; section.</p>

<p>General solutions for the six systems were obtained
using the Harvard Mark IV Calculator.
Particular integrals (see Section 3F)
were also computed for exponential functions e<sup>&mu;t</sup> for</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&mu; = 0, 0.015, 0.020, 0.025, 0.030, 0.035.
</td></tr></table>

<p>At the time this study was undertaken,
the Mark IV Calculator had just been completed.
The need for solving the extensive economic systems
mentioned above therefore provided 
an excellent opportunity for an investigation
of general matrix methods especially suitable for
application to Mark IV.</p>

<a name="notes1"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top><a name="note1a"></a>a. &nbsp;</td><td>The various assumptions made
 are justifiable only on the basis of economic theory and 
 will not be discussed here.
 They are given careful consideration 
 in the references cited.</td></tr>
<tr><td valign=top><a name="note1b"></a>b.</td><td>The primary motive
 for including derivative terms is that the capital requirements of a sector
 may be expected to be determined by the rate at which the output is changing,
 rather than directly by the output.
 Capital requirements are taken to include stocks on hand 
 as well as the means of production.</td></tr>
<tr><td valign=top><a name="note1c"></a>c.</td><td>Certain restrictions must be 
 placed on the initial conditions if the matrix of capital coefficents
 [b<sub>ij</sub>] is singular.  See Section 3D.</td></tr>
<tr><td valign=top><a name="note1d"></a>d.</td><td>A sector may be considered
 unstable if it is not linearly related to the remaining sectors.</td></tr>
</table>
<br>&nbsp;<br>





<a name="2"></a>
<p><b>Chapter 2 &nbsp; Problem Preparation for an Automatic Computer</b></p>

<p>Section A of this chapter will be devoted
to a consideration of the general aspects of problem preparation
for an automatic computer as a preliminary to the discussion
in Chapters 4 and 5
where possible methods for the solution of our economic model
are analyzed and compared on the basis 
of their suitability for machine computation.
Section B will be devoted
to programming details of the Harvard Mark IV Calculator
as a preliminary to the discussion of Chapters 6
where the programming of the selected methods
for the Mark IV Calculator is discussed.
Reading of Section B may well be deferred
until after Chapter 5.</p>

<a name="2A"></a>
<p><b>A. General Aspects of Problem Preparation</b><sup><a href="MSLDE1.htm#note2a">[a]</a></sup></p>

<a name="2A1"></a>
<p><b>(1) Introduction</b></p>

<p>The solution of virtually any problem of applied mathematics
may be reduced to a sequence of elementary operations involving
only addition, subtraction, multiplication, division,
and reference to previously computed quantities.
For this reason it is clear that a truly general purpose
calculator may be based on the following components:</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>An arithmetic unit capable of carrying out the operations
 of addition, subtraction, multiplication, or division on any pair of numbers.
 </td></tr>
<tr><td valign=top>2.</td><td>An input device for introducing numbers into the calculator.
 Among the various devices available are punched cards, paper tape,
 magnetic wire, and magnetic tape.
 </td></tr>
<tr><td valign=top>3.</td><td>A storage unit for storing intermediate results
 until they are needed in further computations.
 The storage unit is made up of a number of individual &ldquo;registers&rdquo;
 each capable of storing a single number.
 Each register in the storage unit is specified by a number
 or &ldquo;address&rdquo; assigned to it.
 </td></tr>
<tr><td valign=top>4.</td><td>An output device for recording in suitable form
 the results obtained by the computer.
 A typewriter or similar printing device is usually provided.
 Numbers may also be recorded on the same media used for input.
 This allows for unlimited &ldquo;external storage&rdquo;
 of intermediate results which may be read back
 into the machine at any time via the input device.</td></tr>
<tr><td valign=top>5.</td><td>A number transfer bus for transferring
 numbers among the various units mentioned above.
 </td></tr>
</table>

<p>The calculator may be made automatic by the addition
of a &ldquo;sequence unit&rdquo; 
which determines the sequence of operations
to be carried out.
The sequence unit operates by reading sequentially a set
of instructions presented to it,
and carrying out the prescribed operations
by determining which numbers are to be transferred,
read, or recorded at each step.
The sequence instructions or &ldquo;program&rdquo;
must be coded and recorded in a form suitable 
to the particular machine.</p>

<p>Since each step must be programmed it is clear 
that little or no labor is saved
by the automatic computer unless the operations
are made repetitive.
Such repetition is made possible by a special instruction
referred to as a &ldquo;call&rdquo; which interrupts
the sequence of instructions being read
and cause the sequence unit to begin reading instructions
from some point in the program specified by the call.
This provides the possibility of a program
consisting of a single closed loop which could,
for example, be used in the tabulation of a given function
as follows:
The initial value of the argument is stored in a certain
&ldquo;argument&rdquo; register
and the prescribed function of the number 
in this register is computed and recorded.
The number in the argument register is then incremented
and a call is made to the point in the program
immediately following the introduction
of the initial argument.</p>

<p>The computer is made much more versatile 
by providing another instruction termed 
&ldquo;conditional call&rdquo; which effects a call
if and only if the number in a special
&ldquo;conditional&rdquo; register is negative.
This allows a choice of programs to be made
depending on results obtained
in the course of the calculation.
For example, a series may be automatically truncated
at the first non-significant term as follows:
Suppose that a certain section of the program 
is terminated with a call so as to form a closed loop
which repeats indefinitely, 
and on each iteration adds a term of the series.
Let the call be replaced by a conditional call
and immediately preceding it let the number 
<nobr>|T| - R</nobr> be read into the condition register.
If T is the last term added and R is a certain tolerance,
the conditional call will fail when |T| becomes
less than the tolerance and the computer
will proceed to the next section of the program.</p>

<p>The conditional call is used frequently in programming
and, in effect, provides the possibility of making
any logical decision necessary in practice.
The pattern of the calls and conditional calls
occurring in the program divides it into groups
of operations called &ldquo;routines&rdquo;.</p>

<a name="2A2"></a>
<p><b>(2) Choice of Numerical Method</b></p>

<p>In general, several alternative numerical methods
will be available for any given problem;
and the first step in the preparation of the problem
will be the choice of the most suitable method.
This choice will require consideration of the routines
necessary to carry out each possible method.
The factors which enter into this consideration
will now be discussed.</p>

<p>The possible error present in computed results
must be known if the results are to be of value.
Loss of accuracy will be due to two types of errors;
those due to certain approximations,
and those due to &ldquo;round-off&rdquo;.</p>

<p>The first type of error occurs when a certain function
is approximated by some more easily computed function as,
for example, in the truncation of an infinite series.
When such an approximation is used,
an estimate of the &ldquo;truncation error&rdquo;
so committed must be available.</p>

<p>Because a computer is limited in the number of digits carried,
each number is correct only to a certain number of decimal places.
Errors due to this effect are called round-off errors.
The maximum round-off error introduced at each step
is known by the way in which errors from any source propagate 
depends on the subsequent operations.
The total error accumulated is therefore difficult
to estimate and depends on the particular method employed.
Upper bounds to the possible error can usually
be obtained
but these bounds are often grossly pessimistic 
and probable error bounds may have to be considered.</p>

<p>Errors of a completely different nature are those introduced
by a failure of some part of the computer or by a flaw in the program.
Such errors are usually called blunders.
They must either be prevented or corrected
before the computation is allowed to proceed.
For this purpose checks must be included in the program
and provision made to rerun the program from some previous point
when a blunder occurs.
Checks are usually arranged to stop the computer 
if two quantities which are supposed to agree
(because of some mathematical identity)
to within a certain tolerance do not agree.
Rather than make the machine stop due to an intermittent failure,
a rerun may be programmed to occur automatically
for a limited number of times before the machine stops.</p>

<p>Although a clear-out distinction is not always possible
it is useful to classify checks as current or final.
The purpose of a final check is to insure that the final results
recorded are correct.
Current checks are provided at frequent intervals,
usually at the end of each routine,
with the double purpose of reducing the amount of time lost
in further computation after an error occurs
and of aiding in the detection of a source of error
by localizing the trouble to a certain section of the program.
Checks should be designed to detect not only machine errors
but also any errors in programming.
Program errors are by far the most difficult to detect
and considerable ingenuity may be required 
to devise suitable checks.</p>

<p>Of two possible methods of solution,
the one more general in application is also to be
the more complex and the more time-consuming.
Other things being equal, the use of a more
general method is to be preferred because of
the possibility of using the same program 
in later problems where the greater generality
may be utilized.
In some cases, the generality required cannot
be completely determined in advance.
For example, in Chapter 4 the so-called
power method is seen to fail 
when certain types of multiplicities occur
in the roots of the matrix.</p>

<p>The amount of computation required is clearly
an important factor although it becomes relatively 
less important as speed and availability of automatic
computers [increase].
The time required for a given process may be evaluated
roughly by counting the number of multiplications required.
This gives a useful measure for comparison.
The amount of number storage required by a given method
must also be given consideration.</p>

<p>The complexity of the program determines the amount
of preparation time required of the mathematician,
and to a certain extent, 
the difficulty of running the problem on the machine.
When a program is first placed in operation
it must be carefully &ldquo;checked out&rdquo;
to remove any possible programming errors
and the more complex the program,
the more time this process is likely to take.
The probable check-out time should therefore be taken
into account in estimating the over-all machine time required.
The complexity of the program also determines 
the amount of storage required for sequencing instructions;
an important factor in the use of machines 
with limited storage capacity.</p>

<p>The complexity of the program required for a given process
depends to a surprising degree on the lack of uniformity
of the process.
That is to say, an otherwise simple program may
become very complex if exceptions must be made
for certain special cases.
The desirability of uniformity in machine methods
is stressed therefore
in the discussion of programs in Chapter 6.</p>

<p>In summary the major considerations entering
into the assessment of a given method are the following:</p>

<table>
<tr><td>1. &nbsp;</td><td>total error accumulation;</td></tr>
<tr><td>2.</td><td>checks available and ease of reruns;</td></tr>
<tr><td>3.</td><td>amount of computation required;</td></tr>
<tr><td>4.</td><td>generality of the method;</td></tr>
<tr><td>5.</td><td>complexity of the program; and</td></tr>
<tr><td>6.</td><td>number storage requirements.</td></tr>
</table>

<a name="2A3"></a>
<p><b>(3) High Accuracy Operation</b></p>

<p>The digits of a number are stored 
in successive &ldquo;columns&rdquo;
of a storage register.
In most automatic calculators the decimal point
is fixed between a given pair of columns,
and the numbers introduced must be suitably
scaled so that none of the numbers encountered
in the course of the calculation will exceed
the capacity of the storage registers.
In some calculators such as the Mark IV,
the decimal point is fixed during the running
of a problem but its location may be chosen 
at will by the operator.
In this case the location of the decimal point
must be chosen to prevent the occurrence of
over-capacity numbers.
In general, the decimal point should be located
as far to the left as is consistent with this requirement
since the round-off error is thereby reduced.</p>

<p>If very high accuracy is required or if the
accumulation round-off error is very severe,
the storage capacity of a single register
may not be adequate.
In this case each number may be stored 
in two registers;
the high order part of the number in one register,
and the low order part in the second.
This procedure is know as &ldquo;double accuracy&rdquo; or
&ldquo;high accuracy&rdquo; operation.
Provisions are made for the addition and multiplication
of such &ldquo;high accuracy&rdquo; numbers 
by suitable programming.</p>

<p>It may happen that the number of significant digits
required in each number does not exceed
the capacity of the registers
but that the magnitude of different numbers varies
so widely that they cannot be stored at the same
decimal point.
In this case it is usual to resort to the 
so-called &ldquo;floating point&rdquo;
operation in which each number z is represented
by two numbers y and p in the semi-logarithmic form
z = y &times; 10<sup>p</sup>.
The number y is &ldquo;normalized&rdquo;, i.e.
it is stored with the first non-zero digit in the
highest order column of the register.
Some calculators<sup><a href="MSLDE1.htm#note2b">[b]</a>&nbsp;</sup>
operate directly with numbers of this type
and reserve certain columns of each register
for representation of the exponent p.
In the use of a fixed point machine
the corresponding operations must be programmed.</p>

<a name="2A4"></a>
<p><b>(4) Simplification of Programming</b></p>

<p>The speed of modern computers has reached a point where, 
for many applications, the time required to program a problem
may be a more important factor than the actual computing time.
Attempts are made, therefore, to simplify the work of programming
in various ways.</p>

<p>The first step may be made by the programmer in seeking out
repetitive operations within the program.
Such a repetitive operation need be programmed but once
together with instructions to repeat the operation
the desired number of times.
Similarly an operation which recurs at several different points
in a program may be programmed once as a &ldquo;subroutine&rdquo;
to be called whenever required.
Such a subroutine may be arranged to call back automatically
to the point in the main program from which it has
itself been called.</p>

<p>It frequently happens that an operation is repetitive
except for a simple permutation of the registers involved.
For example, the summation of the numbers in the
successive registers 1-50 is performed by adding
successively the registers 1,2,&#133;50 to the previous sum.
The only difference in each of the fifty operations
is the address of the particular register to be added.
Modern machines allow the programmer to make such operations
repetitive by making provision for modifying instructions.
In the previous example it is only necessary to add one
to that part of the instruction specifying the address
of the new register to be added each time.</p>

<p>In most machines the sequence instructions and numbers
are stored in the same set of registers and instructions
may therefore be modified in the arithmetic unit.
In the Harvard machines the number storage
and sequence storage are completely separate
and the direct modification of instructions
is not possible.
Equivalent operation is obtained by the use
of certain control registers as follows:
Normally each instruction specifies directly
the address of the storage register concerned.
In Mark IV, a special order is provided 
which specifies storage address &ldquo;i<sub>j</sub>&rdquo;,
where i<sub>j</sub> is the number stored 
in a certain control register called the IJ 
register.<sup><a href="MSLDE1.htm#note2c">[c]</a></sup>
Thus, without changing the actual order stored
in the sequence storage, 
the operation effected by it may be modified
by changing the number resident 
in the IJ register.</p>

<p>The separation of number storage and sequence storage
offers important advantages in making reruns.
To make a rerun from any given point in a program
all the relevant storage must be restored 
to the conditions which obtained at that point.
For the number storage this may usually be done
by so planning the program that the numbers entered
into the calculation are still available
in the same registers at the time the check is made.
Although the number of orders modified 
in a certain routine may be rather large
and the pattern of the modifications rather complex
(due perhaps to repetitive loops within the routine),
yet the modifications may usually be expressed
as simple functions of a very limited number
of parameters or &ldquo;control numbers&rdquo;.
In Mark IV the desired functions of these
control numbers are computed and introduced into
the IJ register to produce the required modifications.
Since the sequence instructions are themselves unchanged,
the program may be restored to any point
by restoring the new control numbers to their correct values.
The rerun procedure is therefore relatively simple
and may easily be made to occur automatically.</p>

<p>On the other hand, the restoration
of the sequence instructions in a machine
which modifies them directly is in general so difficult
that the simplest procedure is to read the original instructions
again from the input device.
Automatic reruns become virtually impossible.
It should also be noted that easy reruns
greatly simplify the process of 
checking out a program.</p>

<p>A disadvantage of the separation of number
and sequence storage shows up in the case
of problems requiring a large amount of number storage
and little sequence storage (or vice-versa).
In this case the total storage capacity of the machine
cannot be utilized.
If, however, the storage device used
is relatively cheap, e.g. a magnetic drum,
the slight increase in total storage capacity
required by the separation of numbers
and instructions is amply justified
by the advantages described above.</p>

<p>The algebraic coding machine of Mark 
IV<sup><acronym title="5. Staff of the Computation Laboratory, A Description of the Mark IV Calculator, Annals of the Computation Laboratory, Vol. XXVIII, Harvard University Press (to be published).">5</acronym>&nbsp;</sup>
is one approach to the further simplification of programming.
Sequencing instructions in essentially algebraic form
may be introduced on the keyboard.
These instructions are automatically translated
into the more complicated program instructions required
to sequence the computer and are recorded on tape
for later use by the computer.
Only the most elementary of the algebraic operations
are available by high accuracy and complex operations
may also be coded directly.
In addition, the elementary functions &radic;x,
tan<sup>-1</sup>x, cos&nbsp;x, log<sub>10</sub>x,
and 10<sup>x</sup>, may be coded directly;
e.g. the operation
cos&nbsp;x<sub>0</sub> &rArr; c<sub>0</sub>
computes the cosine of the number in register
x<sub>0</sub> and stores it in register c<sub>0</sub>.</p>

<p>In the simpler operations described above
(namely, those requiring less than twelve instructions
for their completion)
the coding machine records the required sequence
instructions directly.
In the more complicated operations, e.g. cos&nbsp;x,
the coding machine provides only the instructions
which are necessary to call a &ldquo;permanent&rdquo;
subroutine<sup><acronym title="6. Staff of the Computation Laboratory, “Final Report on Functions for Mark IV”, Harvard Computation Laboratory, Progress Report No. 22, Section II, May 1952.">6</acronym>&nbsp;</sup>
which is normally recorded on the sequence drum
of the computer at all times.</p>

<p>The use of the algebraic coding machine can materially
reduce problem preparation time.
It also enables a mathematician to use
the computer without learning its arbitrary
sequence codes.</p>

<p>Two minor disadvantages are
(1) the coding machine, generally speaking,
does not program as efficienctly as is otherwise possible; and
(2) in checking out a program and locating errors effectively,
the detailed coding must be understood.
Even so a mathematician may,
with the help of a competent operator, 
check out a program satisfactorily 
without knowing the detailed coding.</p>

<p>The use of permanent subroutines for elementary functions
might well be extended to the use of an extensive library of subroutines.
Conceivably such a library could be made to include all the operations
likely to be encountered in the majority of problems
and programming would then be reduced to the preparation
of a relatively simple main program to piece together
the required 
subroutines.<sup><acronym title="7. Wilkes, M.V., Wheeler, D.J., and Gill, S., The Preparation of Programs for an Electronic Digital Computer, Addison-Wesley Press, 1951.">7</sup>
The value of a particular subroutine depends on both the ease 
and the frequency of its use.
On this basis the subroutines for the elementary functions
are in general the most valuable and subroutines for linear operations
probably take second place.
A set of subroutines for matrix operations on Mark IV
are discussed in Chapter 6.</p>

<a name="2B"></a>
<p><b>B. Mark IV Programming</b><sup><a href="MSLDE1.htm#note2d">[d]</a></sup></p>

<p>This section will be devoted to those details
of Mark IV programming<sup><a href="MSLDE1.htm#note2e">[e]</a></sup>
which are necessary to an understanding 
of the routines discussed in Chapter 6.
Programs for Mark IV may be prepared either 
on the algebraic coding machine mentioned in
section A
or on a &ldquo;decimal&rdquo; coding machine,
with which the elementary sequence codes are recorded.
Since the use of the decimal coding machine leads to
a slightly more efficient program,
it was chosen for the preparation 
of the subroutines of Chapter 6.
For this reason only the decimal coding will be 
considered in this section.

<a name="2B1"></a>
<p><b>Orders</b></p>

<p>The units of Mark IV are divided into
two groups called &beta; and &gamma;.
Under control of a single order a number
from any &beta; unit may be transferred
to any &gamma; unit or vice-versa.
Direct transfers between two units 
of the same group are not possible.
An order is made up of three parts called
the &alpha;, &beta;, and &gamma; codes,
which determine the operation 
to be performed as follows:</p>

<table>
<tr><td valign=top nowrap>&alpha;-code (2 digits):&nbsp;</td>
 <td>Determines the direction of transfer
 (&beta;&rarr;&gamma; or &gamma;&rarr;&beta;)
 and the sign with which the transfer takes place
 (+, &ndash;, +|&nbsp;| or &ndash;|&nbsp;|).
 </td></tr>
<tr><td valign=top>&beta;-code (4 digits):&nbsp;</td>
 <td>Determine the &beta; unit involved in the transfer.
 </td></tr>
<tr><td valign=top>&gamma;-code (2 digits):&nbsp;</td>
 <td>determine the &gamma; unit involved in the transfer.
 </td></tr>
</table>

<p>A complete list of the codes for Mark IV
appear at the end of this section.
The calculator has a storage capacity of 10,000 orders,
each of which is identified by a
&ldquo;line-number&rdquo;
in the range 0000-9999.</p>

<p>The function of certain &beta; codes differs
depending on whether a read-in or read-out
of the register is involved
(i.e., depending on the &alpha;-code with which
it is combined).
Hence it is sometimes necessary to specify
the function of a &beta;-code both for read-in and
for read-out.
In the coding list this is done by writing &ldquo;in&rdquo;
or &ldquo;out&rdquo; after the appropriate function.
Similar remarks apply to the &gamma;-codes.</p>

<p><b>Example</b>. The coding list gives for
&gamma; code 21: Multiplier in and multiply;
low order product out.
From this one may deduce the meanings 
of the following orders:</p>

<table>
<tr><td valign=top nowrap>00 0019 21 &nbsp;</td>
 <td>Read the number stored in &beta; register 0019
 into the multiply unit and multiply.
 </td></tr>
<tr><td valign=top nowrap>10 0019 21</td>
 <td>Read low order product out of the multiply unit
 into &beta; register 0019.
 </td></tr>
</table>

<p>If neither &ldquo;in&rdquo; nor &ldquo;out&rdquo;
is specified, the function of the code is the same
for either, except for direction of transfer.</p>

<a name="2B2"></a>
<p><b>Registers</b></p>

<p>Most of the registers of Mark IV have a capacity
of sixteen decimal digits and sign.
Such registers are referred to as &ldquo;regular&rdquo;
in contrast with other &ldquo;short&rdquo; registers
which do not have a full sixteen columns.
All short registers are so arranged that their first
(low order) column corresponds to the first column
of the regular registers.
Read-out from any short register to a regular register
places zeros in the unoccupied columns.</p>

<p>The &gamma; units consist of the &gamma; transfer
register and the arithmetic unit
(multiplier, divider, and two accumulators).
The &beta; units include two hundred fast
storage registers and other special registers.
The content of any register is not destroyed
on read-out.</p>

<a name="2B3"></a>
<p><b>Fast Storage Registers</b></p>

<p>The two hundred fast storage registers
are referred to by the numbers of the
&beta;-codes controlling them
and are numbered from 0000 to 0199.
It is often convenient to designate them
by a letter and subscript, the letter
determining the second and third digits
as follows:</p>

<table>
<tr>
 <td>a</td><td>b</td><td>c</td><td>d</td>
 <td>u</td><td>v</td><td>w</td><td>x</td>
 <td>y</td><td>z</td>
 <td width=28 nowrap>&nbsp;</td>
 <td>a'</td><td>b'</td><td>c'</td><td>d'</td>
 <td>u'</td><td>v'</td><td>w'</td><td>x'</td>
 <td>y'</td><td>z'</td></tr>
<tr>
 <td>00</td><td>01</td><td>02</td><td>03</td>
 <td>04</td><td>05</td><td>06</td><td>07</td>
 <td>08</td><td>09</td>
 <td width=28 nowrap>&nbsp;</td>
 <td>10</td><td>11</td><td>12</td><td>13</td>
 <td>14</td><td>15</td><td>16</td><td>17</td>
 <td>18</td><td>19</td>
</table>

<p>For example, b<sub>3</sub> &equiv; 0013
and d'<sub>8</sub> &equiv; 0138.</p>

<a name="2B4"></a>
<p><b>Function Registers</b></p>

<p>The function registers are ten in number
(&beta;0220-0229) and are designated as
f<sub>0-9</sub>.
In operation they are identical with the fast storage
registers but are normally reserved for use
by the permanent 
subroutines.<sup><acronym title="6. Staff of the Computation Laboratory, “Final Report on Functions for Mark IV”, Harvard Computation Laboratory, Progress Report No. 22, Section II, May 1952.">6</acronym>&nbsp;</sup></p>

<a name="2B5"></a>
<p><b>Slow Storage</b></p>

<p>Provision is made for the storage of four thousand numbers 
in &ldquo;slow storage&rdquo; positions numbered 0000-3999.
These numbers are not immediately available but may be transferred
to or from fast storage in blocks of ten numbers at a time.
Such transfers are made to or from 
two groups of ten special &beta; registers called
S and S' and numbered from S<sub>0</sub> &equiv; 0200 to
S'<sub>9</sub> &equiv; 0219.
The two groups S and S' are identical in function,
but the provision of two groups makes it possible
to use one for computation while the other is engaged
in a transfer.
Transfers may be made in either direction between any block
of ten consecutive slow storage positions
and either the S or S' group of registers.</p>

<p>The slow storage control register (SSCR) is a four-column
&beta; register capable of storing a number in the range 0000-3999.
The number storage in the SSCR determines the slow storage position
of the first number of the block of ten to be transferred.
For example, if the number 0328 stands in the &gamma; transfer register
(&gamma;00) the effects of the following orders are:</p>

<table>
<tr><td valign=top nowrap>10 1201 00 &nbsp;</td><td>
The number 0328 is read into the SSCR and transfer is made from
slow storage positions 0328-0337 into registers S<sub>0-9</sub>.
 </td></tr>
<tr><td valign=top nowrap>10 1211 00 &nbsp;</td><td>
The number 0328 is read into the SSCR and transfer is made from
registers S<sub>0-9</sub> into slow storage positions 0328-0337.
 </td></tr>
</table>

<p>&beta; codes 1221 and 1231 produce similar results but 
with the S' registers replacing the S registers.</p>

<p>The slow storage control register operates modulo 4,000.
Thus the four thousand slow storage positions form
a closed ring and may be run through in sequence
as many times as desired by simply augmenting the number
in the SSCR.</p>

<p>The SSCR has ordinary read-in and read-out
(without initiating a slow-fast transfer)
under &beta; code 3200.
The S and S' registers have regular read-in and read-out
and may be used as ordinary fast storage registers.</p>

<a name="2B6"></a>
<p><b>IJ Register</b></p>

<p>The IJ register is a three-column &beta; register
which can store numbers from 0 to 399.
The first (low order) column is referred to separately
as the J register,
since read-in or read-out may be made either to the whole
IJ register, (&beta;3003) or to the J register alone (&beta;3002).
The number standing in the IJ register is referred to as i<sub>j</sub>.
The IJ register controls or modifies the effects of certain
&beta; codes as follows:  (for this example assume i<sub>j</sub> = 124)</p>

<table>
<tr><td>&beta;-code <sub>&nbsp;</sub></td> <td>&beta;-register Selected &nbsp; <sub>&nbsp;</sub></td>
 <td>&beta;-code <sub>&nbsp;</sub></td> <td>&beta;-register Selected <sub>&nbsp;</sub></td>
 </tr>
<tr><td>3000(i<sub>j</sub>) <sup>&nbsp;</sup></td> <td>&nbsp; &nbsp; 124 <sup>&nbsp;</sup></td>
 <td>1200(S<sub>j</sub>) &nbsp;</td> <td>&nbsp; &nbsp; 204</td>
 </tr>
<tr><td>1000(a<sub>j</sub>)</td> <td>&nbsp; &nbsp; 004</td>
 <td>1210(S'<sub>j</sub>)</td> <td>&nbsp; &nbsp; 214</td>
 </tr>
<tr><td>1010(b<sub>j</sub>)</td> <td>&nbsp; &nbsp; 014</td>
 <td>1220(f<sub>j</sub>)</td> <td>&nbsp; &nbsp; 224</td>
 </tr>
<tr><td>&#133;</td> <td>&nbsp; &nbsp; &#133;</td>
 <td>2000(i<sub>0</sub>)</td> <td>&nbsp; &nbsp; 120</td>
 </tr>
<tr><td>1190(z'<sub>j</sub>)</td> <td>&nbsp; &nbsp; 194</td>
 <td>2001(i<sub>1</sub>)</td> <td>&nbsp; &nbsp; 121</td>
 </tr>
<tr><td colspan=2>&nbsp;</td>
 <td>&#133;</td> <td>&nbsp; &nbsp; &#133;</td>
 </tr>
<tr><td colspan=2>&nbsp;</td>
 <td>2009(i<sub>9</sub>)</td> <td>&nbsp; &nbsp; 129</td>
 </tr>
</table>

<a name="2B7"></a>
<p><b>Shift and Normalize</b></p>

<p>The shift register may be used to shift a number
either to the right or to the left by any number of places
determined by the number in the shift control register (SCR).
The SCR will store any number in the range &plusmn;19,
a positive number producing a shift to the left,
and a negative number producing a shift to the right.
The SCR has ordinary read-out and read-in under &beta; code 0302.
The shift register has two types of read-out and read-in as follows:</p>

<table>
<tr><td nowrap>Name of Order <sub>&nbsp;</sub></td>
 <td nowrap>&beta;-code <sub>&nbsp;</sub></td>
 <td>Operation <sub>&nbsp;</sub></td></tr>
<tr><td valign=top>Shift in</td> <td valign=top>&nbsp; 0300</td> 
 <td>Read into the shift register.</td></tr>
<tr><td valign=top>Non-shift out</td> <td valign=top>&nbsp; 0300</td> 
 <td>Read out of the shift register without shift.</td></tr>
<tr><td valign=top>Normalize in</td> <td valign=top>&nbsp; 0301</td> 
 <td>Read into the shift register and place in the shift control register
the amount of shift required to normalize the number.</td></tr>
<tr><td valign=top>Shift out</td> <td valign=top>&nbsp; 0301</td> 
 <td>Read out of the shift register with a shift determined 
by the number in the shift control register.</td></tr>
</table> 

<p>Under the first two orders the shift register behaves
as an ordinary fast storage register and is often used in this way
as a &ldquo;&beta;-transfer register&rdquo;;
i.e. as intermediary in a transfer between two &gamma; registers.</p>

<p>A number is normalized by reading it into the shift register
under &beta; code 0301 and out under &beta; code 0301.
The amount of shift required for normalization appears automatically
in the shift control register.
After placing the desired amount of shift in the shift control register
a number is shifted by reading it into the shift register 
under &beta; code 0300 and out under &beta; code 0301.
Note that a shift occurs on read-out only so that the original
number stands in the shift register regardless of which
read-in or read-out is used, 
and may be read out unshifted under &beta; code 0300 in either case.</p>

<a name="2B8"></a>
<p><b>Decimal Point Switch</b></p>

<p>The decimal point switch is a &beta; register storing a number
in the range 0-16.
This number determines the operating decimal point of the machine
and represents the number of columns to the right of the decimal point.
The decimal point switch is set manually and read-out only
is possible (&beta;3333).</p>

<a name="2B9"></a>
<p><b>Sign Register</b></p>

<p>The sign register is manually set to store 
a positive or negative zero
and is used to determine whether the decimal point is 
in the high or the low order part of a number 
in double accuracy operation.
It is also a convenient source of zeros.
Read-out only is possible (&beta;3332).</p>

<a name="2B10"></a>
<p><b>Read-in Switch</b></p>

<p>The read-in switch is a manually-operated regular register
from which it is possible to read out (&alpha;07)
to either a &beta; or a &gamma; register,
or to both simultaneously.</p>

<a name="2B11"></a>
<p><b>Call and Conditional Call</b></p>

<p>The line number register (LNR) is a four-column &beta; register
whose content determines the line number of the order to be performed.
The line number register is automatically augmented by one as each order
is performed so that orders are normally carried out in the sequence
determined by their line numbers.
The LNR has regular read-in (&beta;3010) which is used for calling.
If, for example, the number 3255 stands in the &gamma; transfer register
then line 3255 may be called by the following order: 10&nbsp;3010&nbsp;00.</p>

<p>Because the sequence orders are stored on a rotating drum
and may be read out only once per drum revolutions (25 cycles)
a call will introduce a wait until the called order passes
under the reading head.
The wait is such that any repetitive program loop consisting
of t lines will take r drum revolutions if</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
25(r&ndash;1) &le; t+1 &lt; 25r.
</td></tr></table>

<p>Note that the minimum time for a short repetitive loop
is one drum revolution and that a loop of twenty-four lines
will take twice as long as a loop of twenty-three lines.
Waits due to calls may be greatly reduced by careful programming.</p>

<p>The condition register stores only the sign
of a number read into it and is controlled by &beta; code 3012.
A conditional call (&beta;3011) performs the same operation
as a call if the condition register is negative,
and does nothing if it is positive.
Note that in the machine x - x = -0.</p>

<p>An order to read out of the line number register (&beta;3010)
reads out the line number of that same order.</p>

<a name="2B12"></a>
<p><b>&gamma; Transfer Register and External Transfer</b></p>

<p>The &gamma; transfer register is a fast storage register
provided in the &gamma; group and may be used as intermediary
in any transfer between two &beta; registers.
This register also has a special function 
in the introduction of numbers in the course of a problem.
This operation is called external transfer and
may be performed in either of two ways:</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
External transfer with reset (XT1).
Under &alpha; code 04 any desired positive four-digit number
may be introduced into the low order columns 
of the &gamma; transfer register,
the remaining column becoming zero.
 </td></tr>
<tr><td valign=top>2.</td><td>
External transfer with shift (XT2).
Under &alpha; code 05 any desired positive four-digit number
may be introduced into the low order columns 
of the &gamma; transfer register,
the number previously contained being shifted
up four places.
 </td></tr>
</table>

<p>In this way a complete positive sixteen-digit number
may be introduced by four orders,
the first with &alpha; code 04 followed 
by three with &alpha code 05.</p>

<a name="2B13"></a>
<p><b>Addition</b></p>

<p>Addition is performed in either of two accumulators
which operate independently except that they may be used
together for double accuracy operations.
Accumulator 1 is controlled by &gamma; code 10 and 11
and two different read-ins are possible.
Code 10 is a read-in which destroys the number
previously stored in the accumulator.
Read-out in either case is under code 11.
The similar operation of accumulator 2
is described in the coding list.</p>

<p><b>Example</b>. The following orders add the contents
of &beta; registers 0019 and 0134 and place the result
in &beta; register 0022.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>&alpha; <sub>&nbsp;</sub></td><td>&nbsp; &beta; <sub>&nbsp;</sub></td>
 <td>&gamma; <sub>&nbsp;</sub></td></tr>
<tr><td>00 &nbsp; &nbsp;</td> <td>0019 &nbsp; &nbsp;</td> <td>10</td></tr>
<tr><td>00</td> <td>0134 &nbsp;</td> <td>11</td></tr>
<tr><td>10</td> <td>0022 &nbsp;</td> <td>11</td></tr>
</table></td></tr></table>

<p>Note that any number subtracted from itself yields a negative zero.</p>

<a name="2B14"></a>
<p><b>High Accuracy Addition</b></p>

<p>Using the symbols HO and LO to represent the registers
containing respectively the high order and low order parts
of a given number, high accuracy addition may be described
as follows.
The first number to be added is read in with reset, thus:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>00 &nbsp;</td><td> LO &nbsp;</td><td> 10</td></tr>
<tr><td>00 &nbsp;</td><td> HO &nbsp;</td><td> 15 .</td></tr>
</table></td></tr></table>

<p>As many additional numbers as desired may now be added
by providing for each of them the two instructions</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>00 &nbsp;</td><td> LO &nbsp;</td><td> 11</td></tr>
<tr><td>00 &nbsp;</td><td> HO &nbsp;</td><td> 17 .</td></tr>
</table></td></tr></table>

<p>Finally, the sum may be read out by the following orders:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>10 &nbsp;</td><td> LO &nbsp;</td><td> 11</td></tr>
<tr><td>10 &nbsp;</td><td> HO &nbsp;</td><td> 17 .</td></tr>
</table></td></tr></table>

<a name="2B15"></a>
<p><b>Multiplication</b></p>

<p>The multiplicand (MC) must be read in first under
&gamma; code 20 and remains in the MC register.
The multiplier (MP) is read in on the next (or any later)
line under &gamma; code 21.
The product may be read out on the eighth (or any later)
line following the read-in of the MP
and in the intervening seven lines any orders
not involving the multiply unit may be performed.</p>

<p>Four different read-outs of the product are possible.
&gamma; codes 21 and 22 read the product out 
at the operating decimal point 
whereas &gamma; codes 23 and 24
read out at decimal point 15.
The high order (HO) product is the remainder 
of the 32-digit product formed by the multiply unit
and is used in high accuracy operations.</p>

<p>The MP is destroyed in the course of multiplication
but the MC remains and may be read out at any time
except when a multiplication is in progress.
Multiplication by a constant multiplicand may be effected
by reading the MC in once followed 
by the successive multiplier.</p>

<a name="2B16"></a>
<p><b>Division</b></p>

<p>The divisor must be read in first,
followed by the dividend within fifteen cycles.
The quotient may be read out on the twentieth line
following read-in of the dividend
and in the intervening nineteen lines
any orders not involving the divide unit
may be performed.
The divisor and dividend are both destroyed
in the process of division and may not be read out.
Read-out under &gamma; code 34 is not possible
following a read-out under &gamma; code 32.</p>

<a name="2B17"></a>
<p><b>Vacuous Codes</b></p>

<p>The vacuous order 17&nbsp;3355&nbsp;77 performs no operation
and is used to provide any desired delay,
as for example before read-out of a product
or quotient when it is impossible to interpose
enough useful orders.
The vacuous codes 3355 and 77 may also be used separately
whenever a &beta; (or &gamma;) register
is not involved in the particular order.</p>

<a name="2B18"></a>
<p><b>Coded Stops</b></p>

<p>Two stop codes are provided:
&beta;1312 stops the machine on a positive transfer
and &beta;1313 on a negative transfer.
The two stops are provided to give more reliable 
operation on a coded check.</p>

<p><b>Example</b>. Suppose that two computed results
stored in a<sub>0</sub> and a<sub>1</sub> are required
to agree to within a certain tolerance stored in a<sub>2</sub>.
The following routine is used to stop the machine 
if the agreement is not as required:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>Line <sub>&nbsp;</sub></td> <td>&alpha; <sub>&nbsp;</sub></td>
 <td>&nbsp; &beta; <sub>&nbsp;</sub></td>
 <td>&gamma; <sub>&nbsp;</sub></td></tr>
<tr><td>&nbsp; 1</td><td> 00 &nbsp;</td><td>0000 &nbsp;</td><td>10</td></tr>
<tr><td>&nbsp; 2</td><td> 01 &nbsp;</td><td>0001 &nbsp;</td><td>11</td></tr>
<tr><td>&nbsp; 3</td><td> 10 &nbsp;</td><td>0300 &nbsp;</td><td>11</td></tr>
<tr><td>&nbsp; 4</td><td> 03 &nbsp;</td><td>0300 &nbsp;</td><td>10</td></tr>
<tr><td>&nbsp; 5</td><td> 10 &nbsp;</td><td>1312 &nbsp;</td><td>11</td></tr>
<tr><td>&nbsp; 6</td><td> 00 &nbsp;</td><td>0002 &nbsp;</td><td>11</td></tr>
<tr><td>&nbsp; 7</td><td> 10 &nbsp;</td><td>1313 &nbsp;</td><td>11</td></tr>
</table></td></tr></table>

<p>Lines 1-4 place -|a<sub>0</sub> - a<sub>1</sub>| in accumulator 1.
Line 6 adds the tolerance, and so makes the accumulator 1 positive
if the agreement is as required.
Line 7 then stops the machine if the accumulator is negative.
The reason for including line 5 is that the accumulator 
may be defective and standing in a permanently positive condition,
thus passing the check regardless of the value 
of |a<sub>0</sub> - a<sub>1</sub>|. Lines 5 and 7 together show
that the accumulator is negative at one point and positive at
another thus indicating it is operative.</p>

<a name="2B19"></a>
<p><b>Input-Output</b></p>

<p>All data are read to and from the machine by means
of magnetic tape units which perform the following functions:</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</td><td>
Read in orders to be recorded in the machine 
before staring a problem.
 </td></tr>
<tr><td valign=top nowrap>2.</td><td>
Read numbers from tape under control of the sequence orders.
 </td></tr>
<tr><td valign=top nowrap>3.</td><td>
Record numbers on tape from the machine.
 </td></tr>
</table>

<p>&beta; codes 2301-11 select tape units 1-11 and with
a &beta;-in code will record on the appropriate tape the contents
of any &gamma; register desired.
Read from tape is accomplished by the same &beta; codes
combined with a &beta;-out code,
but the number is read always to the read registers,
numbers 1 and 2.
The read registers are &beta; registers with normal read-out
and read-in under &beta; codes 2321 and 2322.</p>

<p>It is also possible to select &ldquo;tape unit k&rdquo;,
where k is the number (1-11) stored in the tape control (K) register.
The functions are the same as described above.
The K register stores numbers from 0 to 19
and has normal read-in and read-out under &beta; code 2320.
Read to and from the record register without recording
is possible under &beta; code 2323.</p>

<table>
<tr><td valign=top><b>Example</b> &nbsp;</td>
 <td valign=top>(1)</td>
 <td>To record on tape 5 the number in accumulator 1: &nbsp;</td>
 <td nowrap>10 2305 11;</td></tr>
<tr><td>&nbsp;</td>
 <td valign=top>(2)</td>
 <td>To read from tape 10 into accumulator 1: &nbsp;</td>
 <td nowrap>00 2310 77</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td nowrap>00 2321 10 .</td></tr>
</table>

<a name="2B20"></a>
<p><b>Printing</b></p>

<p>If final output is to be in printed form,
the recorded tapes must be used to control a printer.
To accomplish this a &ldquo;serial&rdquo; number
must be recorded before each number to be printed.
This serial number serves to identify the number
and to control the printer.
The various columns (number right to left) 
of the serial number are used as follows:</p>

<table>
<tr><td nowrap>Column &nbsp;</td><td>&nbsp; &nbsp; Function</td></tr>
<tr><td valign=top>10</td><td>
Digit 0-9 determines the number of carriage returns.
 </td></tr>
<tr><td valign=top>9</td><td>
Digit 0-9 determines the number of tabs.
 </td></tr>
<tr><td valign=top>8</td><td>
Digit 0-9 determines the number of spaces.
 </td></tr>
<tr><td valign=top>7</td><td>
Selects the style of printing. 
(Any one of ten styles which are preset on a plugboard.)
 </td></tr>
<tr><td valign=top>1-5</td><td>
These columns form a five-digit number used to identify
the number to be printed.
 </td></tr>
<tr><td valign=top>Sign</td><td>
The sign column contains a special &ldquo;serial&rdquo; code
which distinguishes a serial number from a functional number.
This is provided by &alpha; code 14; 
i.e., every serial number is read to the tape unit 
under &alpha; code 14.
 </td></tr>
</table>

<p>Before printing the printer compares the first column 
of the following serial number with the preceding one.
If the digit has increased by one printing proceeds,
but if not the printer passes on to the next number
without printing.
Thus if any number has be rerecorded (due perhaps to a rerun)
only the last recorded value is printed.
All numbers are recorded in duplicate on the tape;
the printer compares the two recorded values 
and stops if they do not agree.</p>

<a name="2B21"></a>
<p><b>Waits</b></p>

<p>Certain units which require several cycles to operate
are provided with automatic waits to delay any order
involving them until the previous operation is completed.
This is not true of the multiplier and divider,
where the requisite number of orders must be interposed
between read-in and read-out.</p>

<p>Even where automatic waits are provided,
machine time may be saved by interposing other orders
between successive uses of any one
of the above-mentioned units.
The average time required for certain operations
is given below in terms of machine cycles.
A machine cycle is 1.2 milliseconds, 
the time required for a single order.</p>

<table>
<tr><td>&nbsp; Operation</td> <td>&nbsp;</td> <td nowrap>Time (machine cycles)</td></tr>
<tr><td>Transfer ten numbers between slow storage and the S or S' registers.</td>
 <td>&nbsp;</td>
 <td valign=top>37 (average)</td></tr>
<tr><td>Read one number from tape.</td>
 <td>&nbsp;</td>
 <td valign=top>22 (average)</td></tr>
<tr><td>Record one number on tape.</td>
 <td>&nbsp;</td>
 <td valign=top>22 (average)</td></tr>
<tr><td>Multiply (including read-in and read-out).</td>
 <td>&nbsp;</td>
 <td valign=top>10 (7 lines interposed)</td></tr>
<tr><td>Divide (including read-in and read-out).</td>
 <td>&nbsp;</td>
 <td valign=top>22 (19 lines interposed)</td></tr>
<tr><td valign=top>Call.</td>
 <td>&nbsp;</td>
 <td valign=top>No. of cycles skipped &mdash; maximum 26, minimum 2</td></tr>
</table>

<a name="codes"></a>

<table>
<tr><td colspan=2><b>&alpha;-Codes</b></td></tr>
<tr><td>Code</td> <td>Function or Register</td></tr>
<tr><td>00</td> <td>&beta; &rarr; &gamma; +</td></tr>
<tr><td>01</td> <td>&beta; &rarr; &gamma; &ndash;</td></tr>
<tr><td>02</td> <td>&beta; &rarr; &gamma; + | |</td></tr>
<tr><td>03</td> <td>&beta; &rarr; &gamma; &ndash; | |</td></tr>
<tr><td>04</td> <td>XT1 (with reset)</td></tr>
<tr><td>05</td> <td>XT2 (with shift)</td></tr>
<tr><td>07</td> <td>Read-in switch</td></tr>
<tr><td>10</td> <td>&beta; &larr; &gamma; +</td></tr>
<tr><td>11</td> <td>&beta; &larr; &gamma; &ndash;</td></tr>
<tr><td>12</td> <td>&beta; &larr; &gamma; + | |</td></tr>
<tr><td>13</td> <td>&beta; &larr; &gamma; &ndash; | |</td></tr>
<tr><td>14</td> <td>&beta; &larr; &gamma; with serial identification</td></tr>
<tr><td>17</td> <td>Vacuous code</td></tr>

<tr><td>&nbsp;</td></tr>

<tr><td colspan=2><b>&beta;-Codes</b></td></tr>
<tr><td>Code</td> <td>Function or Register</td></tr>
<tr><td>0000-0199 &nbsp; &nbsp;</td> <td>a<sub>0</sub>-z'<sub>9</sub></td></tr>
<tr><td>0200-0209</td> <td>S<sub>0-9</sub></td></tr>
<tr><td>0210-0219</td> <td>S'<sub>0-9</sub></td></tr>
<tr><td>0220-0229</td> <td>f<sub>0-9</sub></td></tr>
<tr><td>0300</td> <td>Shift in; non-shift out</td></tr>
<tr><td>0301</td> <td>Normalize in; shift out</td></tr>
<tr><td>0302</td> <td>Shift control register</td></tr>
<tr><td>1000</td> <td>a<sub>j</sub></td></tr>
<tr><td>1010</td> <td>b<sub>j</sub></td></tr>
<tr><td>&#133;</td> <td>&#133;</td></tr>
<tr><td>1190</td> <td>z'<sub>j</sub></td></tr>
<tr><td>1200</td> <td>S<sub>j</sub></td></tr>
<tr><td>1201</td> <td>SSCR in and S in from slow storage</td></tr>
<tr><td>1210</td> <td>S'<sub>j</sub></td></tr>
<tr><td>1211</td> <td>SSCR in and S out to slow storage</td></tr>
<tr><td>1220</td> <td>f<sub>j</sub></td></tr>
<tr><td>1221</td> <td>SSCR in and S' in from slow storage</td></tr>
<tr><td>1231</td> <td>SSCR in and S' out to slow storage</td></tr>
<tr><td>1312</td> <td>Stop on positive trfr.</td></tr>
<tr><td>1313</td> <td>Stop on negative trfr.</td></tr>
<tr><td>2000-2009</td> <td>i<sub>0-9</sub></td></tr>
<tr><td>2300</td>    <td>Unit k &nbsp; &nbsp; &nbsp;&nbsp; ] To record reg. and record;</td></tr>
<tr><td>2301-11</td> <td>Units 1-11 ] read from tape</td></tr>
<tr><td>2320</td> <td>Tape control (K) register</td></tr>
<tr><td>2321</td> <td>Read register #1</td></tr>
<tr><td>2322</td> <td>Read register #2</td></tr>
<tr><td>2323</td> <td>Record register</td></tr>
<tr><td>3000</td> <td>i<sub>j</sub></td></tr>
<tr><td>3002</td> <td>J</td></tr>
<tr><td>3003</td> <td>IJ</td></tr>
<tr><td>3010</td> <td>Call and call reg. in; LN out</td></tr>
<tr><td>3011</td> <td>Cond. call and call reg. in conditionally</td></tr>
<tr><td>3012</td> <td>Condition register</td></tr>
<tr><td>3200</td> <td>Slow storage control (SSCR)</td></tr>
<tr><td>3332</td> <td>Sign register out</td></tr>
<tr><td>3333</td> <td>Decimal point out</td></tr>
<tr><td>3355</td> <td>Vacuous code</td></tr>

<tr><td>&nbsp;</td></tr>

<tr><td colspan=2><b>&gamma;-Codes</b></td></tr>
<tr><td>Code</td> <td>Function or Register</td></tr>
<tr><td>00</td> <td>&gamma; transfer register</td></tr>
<tr><td>10</td> <td>Acc. 1 in</td></tr>
<tr><td>11</td> <td>Acc. 1 in and add; out</td></tr>
<tr><td>15</td> <td>Acc. 2 in</td></tr>
<tr><td>16</td> <td>Acc. 2 in and add; out</td></tr>
<tr><td>17</td> <td>Acc. 2 in and HA add; HA out</td></tr>
<tr><td>20</td> <td>MC</td></tr>
<tr><td>21</td> <td>MP in and multiply; LO product out</td></tr>
<tr><td>22</td> <td>HO product out</td></tr>
<tr><td>23</td> <td>LO product (DP 15) out</td></tr>
<tr><td>24</td> <td>HO product (DP 15) out</td></tr>
<tr><td>30</td> <td>Divisor (DR) in</td></tr>
<tr><td>31</td> <td>Dividend (DD) in and divide</td></tr>
<tr><td>32</td> <td>Quotient (Q) out</td></tr>
<tr><td>34</td> <td>Quotient (Q) out (DP 15)</td></tr>
<tr><td>77</td> <td>Vacuous code</td></tr></table>

<a name="notes2"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top><a name="note2a"></a>a. &nbsp;</td>
 <td>For an excellent and more detailed account of problem preparation
 the reader is referred to Chapter X of 
 <acronym title="4. Staff of the Computation Laboratory, Description of a Relay Calculator, Annals of the Computation Laboratory, Vol. XXIV, Harvard University Press, 1949.">reference 4</acronym>.
 </td></tr>
<tr><td valign=top><a name="note2b"></a>b.<sup>&nbsp;</sup></td>
 <td>The Harvard Mark II 
 Calculator<sup><acronym title="4. Staff of the Computation Laboratory, Description of a Relay Calculator, Annals of the Computation Laboratory, Vol. XXIV, Harvard University Press, 1949.">4</acronym>&nbsp;</sup>
 is an example of such a machine.
 </td></tr>
<tr><td valign=top><a name="note2c"></a>c.</td>
 <td>The IJ register is discussed further in section 2B,
 and examples of its use will be found in Chapter 6.
 </td></tr>
<tr><td valign=top><a name="note2d"></a>d.</td>
 <td>Parts of this section have appeared in Progress Report No. 23
 of the Harvard Computation Laboratory.
 </td></tr>
<tr><td valign=top><a name="note2e"></a>e.</td>
 <td>For full details, see
 <acronym title="5. Staff of the Computation Laboratory, A Description of the Mark IV Calculator, Annals of the Computation Laboratory, Vol. XXVIII, Harvard University Press (to be published).">reference 5</acronym>.
 </td></tr>
</table>
<br>&nbsp;<br>





<a name="3"></a>
<p><b>Chapter 3 &nbsp; Linear Differential Equations</b></p>

<a name="3A"></a>
<p><b>A. Introduction</b></p>

<p>The general solution of a system 
of ordinary linear differential equations is well known,
as are certain computational methods for evaluating
the parameters involved.
In this chapter the theory is developed in a manner
designed to provide the necessary background
for the discussion of computational methods in Chapter 4.
In order to make the structure of the solution clear
for the case of multiple roots,
the whole discussion is based on the Jordan canonical form
of the matrix of coefficients.
The complementary function is expressed in terms 
of the system of principal vectors,
and expressions for the principal vectors are derived
in terms of the adjoint of the characteristic matrix.
A knowledge of elementary matrix theory 
such as contained in the first three chapters of
<acronym title="8. Aitken, A.C., Determinants and Matrices, Oliver and Boyd, Seventh Ed., 1951.">reference 
8</acronym>
will be assumed.</p>

<p>In matrix notation the most general form
of a system of first 
order<sup><a href="MSLDE1.htm#note3a">[a]</a>&nbsp;</sup>
linear differential equations in n variables is</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Ax &ndash; B<img src="MSLDEimg/xdot.bmp"> 
= z 
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.1)
</td></tr></table>

<p>where A and B are square matrices of order n and</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td colspan=7>
x = (x<sub>1</sub>,x<sub>2</sub>,&#133;,x<sub>n</sub>),
 </td></tr>
<tr><td>&nbsp;</td>
<td><img src="MSLDEimg/xdot.bmp"> = (</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center><i>d</i>x<sub>1</sub></td></tr>
 <tr><td><img src="MSLDEimg/22x1.bmp"></td></tr>
 <tr><td align=center><i>d</i>t</td></tr>
 </table></td> 
<td>,</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center><i>d</i>x<sub>2</sub></td></tr>
 <tr><td><img src="MSLDEimg/22x1.bmp"></td></tr>
 <tr><td align=center><i>d</i>t</td></tr>
 </table></td> 
<td>,&#133;,</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center><i>d</i>x<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/22x1.bmp"></td></tr>
 <tr><td align=center><i>d</i>t</td></tr>
 </table></td>
<td>),</td>
 </tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td><td colspan=7>
z = (z<sub>1</sub>,z<sub>2</sub>,&#133;,z<sub>n</sub>)
 </td></tr>
</table>

<p>are vector functions of the independent variable t.
The vector z is a given function and x is the vector
to be determined.
In the case of constant coefficients,
to which this discussion is restricted,
the elements of A and B are constants.</p>

<p>The general solution of Eq. (3.1) consists of 
the sum of 
(1) a particular integral of (3.1), and
(2) the complementary function which is 
any solution involving n arbitrary constants,
of the associated homogeneous equation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Ax &ndash; B<img src="MSLDEimg/xdot.bmp"> 
= 0.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.2)
</td></tr></table>

<p>If B is non-singular Eq. (3.2) may be reduced to</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Dx &ndash; <img src="MSLDEimg/xdot.bmp"> 
= 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.3)
</td></tr></table>

<p>where D = B<sup>-1</sup>A.
The singular case is considered in Section D.
The problem of the complementary function 
will be considered first
and to this end certain results in the theory
of matrices are first developed.</p>

<p>In this chapter the following conventions are adopted:</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</td><td>
The letter n refers exclusively to the order of the matrix
under consideration.
The order of a submatrix will be denoted
by the letter n with a subscript.
 <br>&nbsp;</td></tr>

<tr><td valign=top>2.</td><td>
Matrices are denoted by capital letters A, B, or A(&lambda;),
B(&lambda;), etc.,
where the elements are functions of a parameter &lambda;.
A subscript n may be added to specify the order of the matrix.
 <br>&nbsp;</td></tr>

<tr><td valign=top>3.</td><td>
Vectors are denoted by lower case Roman letters x, y, etc.
 <br>&nbsp;</td></tr>

<tr><td valign=top>4.</td><td>
All equations are to be understood as matrix or vector relations
unless otherwise specified.
 <br>&nbsp;</td></tr>

<tr><td valign=top>5.</td><td>
The Kronecker delta is defined as:
<table><tr><td width=28 nowrap>&nbsp;</td>
<td>&delta;<sub>ij</sub>&nbsp;</td><td>= 0 if i &ne; j,</td></tr>
<tr><td colspan=2>&nbsp;</td><td>= 1 if i = j.<br>&nbsp;</td></tr>
</table>
 </td></tr>

<tr><td valign=top>6.</td><td>
The symbols e<sub>1</sub>,e<sub>2</sub>,&#133;e<sub>n</sub>
will be used to denote the so-called unit vectors defined by
<table><tr><td width=28 nowrap>&nbsp;</td><td>
e<sub>i</sub> = (e<sub>i1</sub>,e<sub>i2</sub>,&#133;e<sub>in</sub>)
</td></tr></table>
where e<sub>ij</sub> = &delta;<sub>ij</sub>.
 <br>&nbsp;</td></tr>

<tr><td valign=top>7.</td><td>
<p>The unit matrix is denoted by I and the matrices I<sub>k</sub>
are defined by</p>
<table><tr><td width=28 nowrap>&nbsp;</td><td>
I<sub>k</sub> = [&delta;<sub>i,j-k</sub>], &nbsp; &nbsp; k &ge; 0.
</td></tr></table>

<p>Thus I<sub>k</sub> is a matrix whose elements are zero except
for a diagonal of unit elements located k places 
above the main diagonal.
For example if n = 3</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3>I<sub>0</sub> = </td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>1 0 0</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>; &nbsp; &nbsp; &nbsp; I<sub>1</sub> = </td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>0 1 0</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>; &nbsp; &nbsp; &nbsp; I<sub>2</sub> = </td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>0 0 1</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>.</td></tr>
<tr><td>&nbsp;</td><td>0 1 0</td><td>0 0 1</td><td>0 0 0</td></tr>
<tr><td>&nbsp;</td><td>0 0 1</td><td>0 0 0</td><td>0 0 0</td></tr>
</tr></table>

<table>
<tr><td width=28 nowrap>If</td></tr>
<tr><td>&nbsp;</td><td nowrap>X = [X<sub>pq</sub>] = I<sub>k</sub>I<sub>m</sub>,</td></tr>
<tr><td>then</td></tr>
<tr><td>&nbsp;</td><td colspan=3>X<sub>pq</sub> = <img src="MSLDEimg/sigmajn.bmp" align=middle>
 &delta;<sub>p,j-k</sub>&delta;<sub>j,q-m</sub>
 = &delta;<sub>p,q-(m+k)</sub> .</sub>
 </td></tr>
<tr><td>Therefore</td></tr>
<tr><td>&nbsp;</td><td colspan=3>X = I<sub>k</sub>I<sub>m</sub> = I<sub>k+m</sub>.</td></tr>
<tr><td>Similarly,</td></tr>
<tr><td>&nbsp;</td><td>I<sub>m</sub>I<sub>k</sub> = I<sub>k+m</sub>.</td></tr>
<tr><td>Thus,</td></tr>
<tr><td>&nbsp;</td><td>I<sub>0</sub> = I</td>
 <td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
 <td rowspan=3 nowrap>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.4)</td></tr>
<tr><td>&nbsp;</td><td>I<sub>km</sub> = I<sub>mk</sub> = 
 I<sub>k+m</sub></td></tr>
<tr><td>&nbsp;</td><td>I<sub>m</sub> = 0 if m &ge; n &nbsp;</td></tr>
</table>
&nbsp;
 </td></tr>

<tr><td valign=top>8.</td><td><p>Since differentiation is a linear operation
it is easily extended to matrices and vectors.
Thus if</p>
<table><tr><td width=28 nowrap>&nbsp;</td><td>
A(&lambda;) = [a<sub>ij</sub>(&lambda;)]
</td></tr></table>

<p>is a matrix whose elements are functions of a scalar &lambda;,
the derivative <i>d</i>A/<i>d</i>&lambda; is defined as the matrix</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<i>d</i>A/<i>d</i>&lambda; = [<i>d</i>a<sub>ij</sub>/<i>d</i>&lambda;].
<br>&nbsp;
</td></tr></table>
 </td></tr>

<tr><td valign=top>9.</td><td>
<p>Two matrices A and B are said to be &ldquo;similar&rdquo;
if there exists a non-singular matrix S such that</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
B = S<sup>-1</sup>AS .
<br>&nbsp;
</td></tr></table>
 </td></tr>

<tr><td valign=top>10. &nbsp;</td><td>
<p>A matrix J<sub>n</sub>(&lambda;) of the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
J<sub>n</sub>(&lambda;) = &lambda;I + I<sub>1</sub>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.5)
</td></tr></table>

<p>will be called a Jordan box of order n, e.g.,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3>J<sub>3</sub>(&lambda;) =</td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>&lambda; 1 0</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>.</td></tr>
<tr><td>&nbsp;</td><td>0 &lambda; 1</td></tr>
<tr><td>&nbsp;</td><td>0 0 &lambda;</td></tr>
</table>
 &nbsp;</td></tr>

<tr><td valign=top>11.</td><td>
<p>A matrix J(&lambda;<sub>i</sub>,n<sub>i</sub>) made up of Jordan boxes
arranged along the main diagonal with zeros elsewhere
is said to be in Jordan canonical form 
and will be referred to briefly as a canonical matrix. Thus:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=4>J<sub>n</sub>(&lambda;<sub>i</sub>,n<sub>i</sub>) =</td>
<td rowspan=4><img src="MSLDEimg/matrixl4.bmp"></td>
<td>J<sub>n<sub>1</sub></sub>(&lambda;<sub>1</sub>)</td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=4><img src="MSLDEimg/matrixr4.bmp"></td>
<td rowspan=4><tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.6)
</tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>J<sub>n<sub>2</sub></sub>(&lambda;<sub>2</sub>)</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&#133;</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td>J<sub>n<sub>m</sub></sub>(&lambda;<sub>m</sub>)</td></tr>
</table>

<p>where</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td><img src="MSLDEimg/sigmaim.bmp" align=middle> n<sub>i</sub> = n.
</td></tr></table>

<p>For example, if n<sub>1</sub> = 3, n<sub>2</sub> = 2, n<sub>3</sub> = 1</p>  

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=6>J<sub>6</sub>(&lambda;<sub>i</sub>,3;&lambda;<sub>2</sub>,2;&lambda;<sub>3</sub>,1) =</td>
<td rowspan=6><img src="MSLDEimg/matrixl6.bmp"></td>
<td bgcolor=dddddd>&lambda;<sub>1</sub></td>
<td bgcolor=dddddd align=center>1<sub>&nbsp;</sub></td>
<td bgcolor=dddddd align=center>0<sub>&nbsp;</sub></td>
<td align=center>0<sub>&nbsp;</sub></td>
<td align=center>0<sub>&nbsp;</sub></td>
<td align=center>0<sub>&nbsp;</sub></td>
<td rowspan=6><img src="MSLDEimg/matrixr6.bmp"></td>
<td rowspan=6>.</td></tr>
<tr><td>&nbsp;</td><td bgcolor=dddddd>0<sub>&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>&lambda;<sub>1</sub></td>
 <td bgcolor=dddddd align=center>1<sub&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td></tr>
<tr><td>&nbsp;</td><td bgcolor=dddddd>0<sub>&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>0<sub&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>&lambda;<sub>1</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td></tr>
<tr><td>&nbsp;</td><td>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>&lambda;<sub>2</sub></td>
 <td bgcolor=dddddd align=center>1<sub&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td></tr>
<tr><td>&nbsp;</td><td>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>0<sub&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>&lambda;<sub>2</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td></tr>
<tr><td>&nbsp;</td><td>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td align=center>0<sub>&nbsp;</sub></td>
 <td bgcolor=dddddd align=center>&lambda;<sub>3</sub></td></tr>
</table>
&nbsp;</td></tr>

<tr><td valign=top>12.</td><td><p>The exponential function of a matrix
is defined formally by the following series,
convergent for any infinite matrix A:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
e<sup>A</sup> = I + A +<sup>&nbsp;</sup></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>A<sup>2</sup></td></tr>
 <tr><td><img src="MSLDEimg/20x1.bmp"></td></tr>
 <tr><td align=center>2!</td></tr>
 </table></td>
<td><sup>&nbsp;</sup>+<sup>&nbsp;</sup></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>A<sup>3</sup></td></tr>
 <tr><td><img src="MSLDEimg/20x1.bmp"></td></tr>
 <tr><td align=center>3!</td></tr>
 </table></td>
<td><sup>&nbsp;</sup>+ &#133; .
</td></tr></table>

<p>This function may be shown to have the usual properties
of the exponential function
(<acronym title="9. Frazer, R.A., Duncan, W.J., and Collar, R.A., Elementary Matrices, Cambridge University Press, 1950.">reference
9</acronym>, p. 41). In particular</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center><i>d</i>e<sup>At</sup></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center><i>d</i>t</td></tr>
 </table></td>
<td>= Ae<sup>At</sup> .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.7)
</td></tr></table>

<p>If A and B commute the following relation also holds:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
e<sup>A</sup>e<sup>B</sup> = e<sup>B</sup>e<sup>A</sup> = e<sup>A+B</sup> .
</td></tr></table>

<p>Consequently</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
e<sup>A</sup>e<sup>-A</sup> = I .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.8)</td></tr></table>

<p>For example:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>e<sup>J<sub>4</sub>(&lambda;)t</sup></td>
<td colspan=5>= e<sup>(&lambda;I+I<sub>1</sub>)t</sup> 
= e<sup>&lambda;t</sup>e<sup>I<sub>1</sub>t</sup> </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= e<sup>&lambda;t</sup> (I<sub>0</sub> + tI<sub>1</sub> +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>t<sup>2</sup></td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>2!</td></tr>
 </table></td>
<td>I<sub>2</sub> +<sup>&nbsp;</sup></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>t<sup>3</sup></td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>3!</td></tr>
 </table></td>
<td>I<sub>3</sub>).<sup>&nbsp;</sup></td>
</tr></table>

<p>Therefore</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=4>e<sup>J<sub>4</sub>(&lambda;)t</sup> = e<sup>&lambda;t</sup></td>
 <td rowspan=4><img src="MSLDEimg/matrixl5.bmp"></td>
 <td>1<sup>&nbsp;</sup></td>
 <td width=20 align=center>t<sup>&nbsp;</sup></td>
 <td width=20 align=center><table cellspacing=0 cellpadding=0>
  <tr><td align=center>t<sup>2</sup></td></tr>
  <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
  <tr><td align=center>2!</td></tr>
  </table></td>
 <td width=20 align=center><table cellspacing=0 cellpadding=0>
  <tr><td align=center>t<sup>3</sup></td></tr>
  <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
  <tr><td align=center>3!</td></tr>
  </table></td>
 <td rowspan=4><img src="MSLDEimg/matrixr5.bmp"></td>
 <td rowspan=4>.</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td align=center>1<sup>&nbsp;</sup></td>
 <td align=center>t<sup>&nbsp;</sup></td>
 <td align=center><table cellspacing=0 cellpadding=0>
  <tr><td align=center>t<sup>2</sup></td></tr>
  <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
  <tr><td align=center>2!</td></tr>
  </table></td>
  </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td align=middle>1<sup>&nbsp;</sup></td><td align=middle>t<sup>&nbsp;</sup></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td align=middle>1<sup>&nbsp;</sup></td></tr>
</table>
&nbsp;
 </td></tr>
</table>

<p>The proof of the following theorem, omitted here,
may be found on page 64 of
<acronyn title="10. Turnbull, H.W., and Aitken, A.C., The Theory of Canonical Matrices, Blackie and Son, Ltd., London, 1952.">reference 
10</acronym>.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<p><b>Theorem</b>: Every matrix D is similar 
to a matrix J in canonical form.
Except for the order of occurrence of Jordan boxes,
the canonical form is unique.</p>

<p>Referring to definition (9) this means that</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
D = SJS<sup>-1</sup>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;
</tt>(3.9)
</td></tr></table>

<p>for some non-singular matrix S.</p>
</td></tr></table>

<a name="3B"></a>
<p><b>B. The Complementary Function</b></p>

<p>Equation (3.3) is repeated here for reference</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Dx &ndash; <img src="MSLDEimg/xdot.bmp"> 
= 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.3)
</td></tr></table>

<p>Substitution of Eq. (3.9) gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
SJS<sup>-1</sup> &ndash; <img src="MSLDEimg/xdot.bmp"> = 0.
</td></tr></table>

<p>Using the fact that SIS<sup>-1</sup> = I 
and then premultiplying by S<sup>-1</sup> we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
JS<sup>-1</sup>x &ndash; S<sup>-1</sup><img src="MSLDEimg/xdot.bmp"> 
= 0.
</td></tr></table>

<p>Let y = S<sup>-1</sup>x. Then 
<img src="MSLDEimg/ydot.bmp" align=middle> = S<sup>-1</sup><img src="MSLDEimg/xdot.bmp">
and finally Eq. (3.3) becomes</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
J(&lambda;<sub>i</sub>,n<sub>i</sub>)y &ndash; <img src="MSLDEimg/ydot.bmp" align=middle> = 0.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt> 
(3.10)
</td></tr></table>

<p>Substitution of the expression</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y = e<sup>J(&lambda;<sub>i</sub>,n<sub>i</sub>)t</sup>c
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.11)
</td></tr></table>

<p>in the left-hand side of Eq. (3.10) gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
[ J(&lambda;<sub>i</sub>,n<sub>i</sub>)e<sup>J(&lambda;<sub>i</sub>,n<sub>i</sub>)t</sup>c
&ndash; <img src="MSLDEimg/ddt.bmp" align=middle>
e<sup>J(&lambda;<sub>i</sub>,n<sub>i</sub>)t</sup> ] c.
</td></tr></table>

<p>The bracketed quantity is identically zero by virtue of (3.7)
and therefore (3.11) is a solution of (3.10) for an arbitrary vector c.
This is the general solution sought since it contains n arbitrary
constants c<sub>1</sub>,c<sub>2</sub>,&#133;,c<sub>n</sub>.</p>

<p>The expanded form of (3.11) is exhibited below for the particular
matrix<sup><a href="MSLDE1.htm#note3b">[b]</a></sup> 
<nobr>J = J<sub>6</sub>(&lambda;<sub>1</sub>,3;&lambda;<sub>2</sub>,2;&lambda;<sub>3</sub>,1)</nobr>
appearing in section 3A11:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=6>y =</td>
<td rowspan=6><img src="MSLDEimg/matrixl6.bmp"></td>
<td rowspan=3>e<sup>&lambda;<sub>1</sub>t</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td align=center>1&nbsp;<sup>&nbsp;</sup></td>
<td align=center>t&nbsp;<sup>&nbsp;</sup></td>
<td align=center>t<sup>2</sup>/2!</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=6><img src="MSLDEimg/matrixr6.bmp"></td>
<td rowspan=6><img src="MSLDEimg/matrixl6.bmp"></td>
<td>c<sub>1</sub></td>
<td rowspan=6><img src="MSLDEimg/matrixr6.bmp"></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td align=center>1&nbsp;<sup>&nbsp;</sup></td>
 <td align=center>t&nbsp;<sup>&nbsp;</sup></td>
 <td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td>c<sub>2</sub></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td>&nbsp;</td>
 <td align=center>1&nbsp;<sup>&nbsp;</sup></td>
 <td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td>c<sub>3</sub></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td rowspan=2>e<sup>&lambda;<sub>2</sub>t</sup></td>
 <td rowspan=2><img src="MSLDEimg/matrixl2.bmp"></td>
 <td>1&nbsp;</td>
 <td align=center><sub>&nbsp;</sub>t</td>
 <td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
 <td>&nbsp;</td>
 <td>c<sub>4</sub></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td align=center><sub>&nbsp;</sub>1</td><td>&nbsp;</td>
 <td>c<sub>5</sub></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td align=center>e<sup>&lambda;<sub>3</sub>t</sup></td>
 <td>c<sub>6</sub></td></tr>
</table>

<p>In terms of components:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y<sub>1</sub> = (c<sub>1</sub> + c<sub>2</sub>t + c<sub>3</sub></td>
 <td><table cellspacing=0 cellpadding=0>
  <tr><td align=center>t<sup>2</sup></td></tr>
  <tr><td align=center><img src="MSLDEimg/14x1.bmp"></td></tr>
  <tr><td align=center>2!</td></tr>
 </table></td>
<td>)e<sup>&lambda;<sub>1</sub>t</sup></td>
 </tr>
<tr><td>&nbsp;</td><td colspan=3>
y<sub>2</sub> = (c<sub>2</sub> + c<sub>3</sub>t)
e<sup>&lambda;<sub>1</sub>t</sup>
 </td></tr>
<tr><td>&nbsp;</td><td colspan=3>
y<sub>3</sub> = c<sub>3</sub>e<sup>&lambda;<sub>1</sub>t</sup>
 </td></tr>
<tr><td>&nbsp;</td><td colspan=3>
y<sub>4</sub> = (c<sub>4</sub> + c<sub>5</sub>t)
e<sup>&lambda;<sub>2</sub>t</sup>
 </td></tr>
<tr><td>&nbsp;</td><td colspan=3>
y<sub>5</sub> = c<sub>5</sub>e<sup>&lambda;<sub>2</sub>t</sup>
 </td></tr>
<tr><td>&nbsp;</td><td colspan=3>
y<sub>6</sub> = c<sub>6</sub>e<sup>&lambda;<sub>3</sub>t</sup> .
 </td></tr>
</table>

<p>The general solution of Eq. (3.3) is now given by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x = Sy = Se<sup>J(&lambda;<sub>i</sub>,n<sub>i</sub>)t</sup>c.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
</tt>
(3.12)
</td></tr></table>

<p>Since S is non-singular x also contains n arbitrary constants.
Let <img src="MSLDEimg/xbar.bmp"> be the vector of initial conditions;
i.e. at</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
t = 0, &nbsp; x = <img src="MSLDEimg/xbar.bmp">.<br>&nbsp;
</td></tr></table>

Then

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/xbar.bmp"> = Sc.
</td></tr></table>

Hence

<table><tr><td width=28 nowrap>&nbsp;</td><td>
c = S<sup>-1</sup><img src="MSLDEimg/xbar.bmp">.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;
</tt>
(3.13)
</td></tr></table>


<a name="3C"></a>
<p><b>C. Reduction to Canonical Form</b></p>

<p>The theorem cited at the end of section A assures 
the existence of a non-singular matrix S 
such that the similarity transformation S<sup>-1</sup>DS
reduces D to canonical form.
A process for obtaining a suitable matrix S
for any given matrix D will now be derived.</p>

<a name="3C1"></a>
<p><b>(1) Principal Vectors and Latent Roots</b></p>

<p>A principal vector of order q is defined as non-trivial
vector x satisfying the conditions</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td nowrap>(A &ndash; &lambda;I)<sup>q-1</sup>x</td>
<td nowrap>&ne; 0, <sup>&nbsp;</sup> &nbsp;</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
<td rowspan=2>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>
(3.14)
</td></tr>
<tr><td>&nbsp;</td><td>
(A &ndash; &lambda;I)<sup>q</sup>x</td><td>= 0. <sup>&nbsp;</sup></td></tr>
</table>

<p>The corresponding value of &lambda; is called the latent root,
or more briefly the root, associated with the principal vector x.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<b>Theorem</b>: The unit vectors 
e<sub>1</sub>,e<sub>2</sub>,&#133;,e<sub>n</sub>
form a set of principal vectors 
for any canonical matrix J.
</td></tr><table>

<p><b>Proof</b>: In order to relate the unit vectors
to the structure of J<sub>n</sub>,
the following notation is adopted:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>x<sub>kj</sub> = e<sub>r</sub>,</td></tr>
<tr><td colspan=2>where</td></tr>
<tr><td>&nbsp;</td><td>r = j + 
<img src="MSLDEimg/sigmapk1.bmp" align=middle> n<sub>p</sub> ;
 </td></tr>
<tr><td>&nbsp;</td><td>k = 1,2,&#133;,m;</td></tr>
<tr><td>&nbsp;</td><td>j = 1,2,&#133;,n<sub>k</sub>.</td></tr>
</table>

<p>Thus x<sub>kj</sub> is a column vector with zero components
except for a unit element in the jth position of the group
of rows occupied by the kth Jordan box J<sub>n<sub>k</sub></sub>(&lambda;).
As k and j range over the integers 1 to m 
and 1 to n<sub>k</sub> respectively,
the vectors x<sub>kj</sub> range over the set
of unit vectors e<sub>1</sub>,e<sub>2</sub>,&#133;,e<sub>n</sub>.
Clearly then,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>[J<sub>n</sub>(&lambda;<sub>i</sub>,n<sub>i</sub>) &ndash; &lambda;I]
x<sub>kj</sub></td>
<td>= (&lambda;<sub>k</sub> &ndash; &lambda;)x<sub>kj</sub>
+ x<sub>k,j-1</sub> &nbsp;</td>
<td>if j &ne; 1,</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= (&lambda;<sub>k</sub> &ndash; &lambda;)x<sub>kj</sub>
<td>if j = 1.</td></tr>
</table>

<p>For example, if k = j = 2, and J is the matrix of section 3A11,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=6>[J<sub>n</sub>(&lambda;<sub>i</sub>,n<sub>i</sub>) &ndash; &lambda;I]
x<sub>22</sub> =</td>
<td rowspan=6><img src="MSLDEimg/matrixl6.bmp"></td>
<td>&lambda;<sub>1</sub>&ndash;&lambda;</td>
<td align=center>1<sub>&nbsp;</sub></td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=6><img src="MSLDEimg/matrixr6.bmp"></td>
<td rowspan=6><img src="MSLDEimg/matrixl6.bmp"></td>
<td align=center>0</td>
<td rowspan=6><img src="MSLDEimg/matrixr6.bmp"></td>
<td rowspan=6>=</td>
<td rowspan=6><img src="MSLDEimg/matrixl6.bmp"></td>
<td align=center>0</td>
<td rowspan=6><img src="MSLDEimg/matrixr6.bmp"></td>
<td rowspan=6>.</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>&lambda;<sub>1</sub>&ndash;&lambda;</td>
<td align=center>1<sub>&nbsp;</sub></td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>0</td><td align=center>0</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&lambda;<sub>1</sub>&ndash;&lambda;</td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>0</td><td align=center>0</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&lambda;<sub>2</sub>&ndash;&lambda;</td>
<td align=center>1<sub>&nbsp;</sub></td>
<td>&nbsp;</td>
<td align=center>0</td><td align=center>1</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&lambda;<sub>2</sub>&ndash;&lambda;</td>
<td>&nbsp;</td>
<td align=center>1<sub>&nbsp;</sub></td>
<td>&lambda;<sub>2</sub>&ndash;&lambda;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&nbsp;</td><td>&nbsp;</td>
<td>&lambda;<sub>3</sub>&ndash;&lambda;</td>
<td align=center>0<sub>&nbsp;</sub></td>
<td align=center>0<sub>&nbsp;</sub></td>
 </tr>
</table>

<p>If &lambda; = &lambda;<sub>k</sub> then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>(J<sub>n</sub> &ndash; &lambda;<sub>k</sub>I)x<sub>kj</sub></td>
<td>= x<sub>k,j-1, &nbsp; &nbsp; &nbsp;</td>
<td>j &ne; 1;&nbsp;</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
<td rowspan=2>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.15)</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= 0,</td>
<td>j = 1.</td>
 </tr>
</table>

<p>Clearly then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>(J<sub>n</sub> &ndash; &lambda;<sub>k</sub>I)<sup>j-1</sup>x<sub>kj</sub></td>
<td nowrap>= x<sub>k,1</sub> &ne; 0 &nbsp; &nbsp; &nbsp; <sup>&nbsp;</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.16)</td>
 </tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td>
<td>(J<sub>n</sub> &ndash; &lambda;<sub>k</sub>I)<sup>j</sup>x<sub>kj</sub></td>
<td>= 0. <sup>&nbsp;</sup></td>
 </tr>
</table>

<p>Thus x<sub>kj</sub> is a principal vector of order j
associated with the latent root &lambda;<sub>k</sub>
and the theorem is proved.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<b>Theorem</b>: Every matrix D possesses a set
of n linearly independent principal vectors.
</td></tr></table>

<p><b>Proof</b>: Let J<sub>n</sub> be the canonical form of D.
Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
S<sup>-1</sup>DS = J</td></tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td><td>
S<sup>-1</sup>(D &ndash; &lambda;I)S = J &ndash; &lambda;I.
</td></tr></table>

<p>Therefore</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td nowrap>
S<sup>-1</sup>(D &ndash; &lambda;<sub>k</sub>I)<sup>j-1</sup>Sx<sub>kj</sub>
= (J &ndash; &lambda;<sub>k</sub>I)<sup>j-1</sup>x<sub>kj</sub> 
&ne; 0 &nbsp; &nbsp;</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3><tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; 
</tt>(3.17)</td></tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td><td>
S<sup>-1</sup>(D &ndash; &lambda;<sub>k</sub>I)<sup>j</sup>Sx<sub>kj</sub>
= (J &ndash; &lambda;<sub>k</sub>I)<sup>j-1</sup>x<sub>kj</sub> = 0. 
</td></tr></table>

<p>Therefore Sx<sub>kj</sub> is a principal vector of order j
of the matrix D and is associated
with the root &lambda;<sub>k</sub>.</p>

<p>The principal vectors of A are therefore
the columns of S and since S is non-singular,
they are linearly independent.
In other words, the matrix S required
for the canonical reduction of D and hence
for the solution of (3.3) is made up of 
the principal vectors of D arranged in descending order
for each latent root as indicated by the relations (3.15).
These relations carry over to the matrix D in the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(D &ndash; &lambda;<sub>k</sub>I)S<sub>kj</sub> = S<sub>k,j-1</sub>,
&nbsp; &nbsp; j &ne; 1</td></tr>
<tr><td>&nbsp;</td><td>
(D &ndash; &lambda;<sub>k</sub>I)S<sub>k1</sub> = 0
</td></tr>
</table>

<p>where S<sub>kj</sub> is the r-th column of S and
r = j + <img src="MSLDEimg/sigmaik1.bmp" align=middle> n<sub>k</sub>.</p>

<p><b>Eigenvectors</b>. A principal vector of order one
is called an eigenvector.
If in the canonical form of D, 
n<sub>k</sub> = 1, k = 1,2,&#133;,m,
the principal vectors are all eigenvectors
and they form a linearly independent set.
The n<sub>k</sub> are necessarily equal to unity if</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</td><td>
the latent roots are distinct, or</td></tr>
<tr><td>2.</td><td>A is symmetric.</td></tr>
</table>

<p>The proof of (2) rests on the fact that if D is symmetric
so is J.
Therefore J has no off-diagonal elements.</p>

<p>The significance of a linearly independent set of eigenvectors
is that the solution of (3.3) then involves
only exponentials and no terms of the form
t<sup>k</sup>e<sup>&lambda;t</sup>.</p>

<a name="3C2"></a>
<p><b>(2) The Adjoint Matrix</b></p>

<p>It is easily verified that the pth power of a canonical matrix
J may be expressed as</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=4>J<sub>n</sub><sup>p</sup>(&lambda;<sub>i</sub>,n<sub>i</sub>) =</td>
<td rowspan=4><img src="MSLDEimg/matrixl4.bmp"></td>
<td>J<sub>n<sub>1</sub></sub><sup>p</sup>(&lambda;<sub>1</sub>)</td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=4><img src="MSLDEimg/matrixr4.bmp"></td>
<td rowspan=4>.</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>J<sub>n<sub>2</sub></sub><sup>p</sup>(&lambda;<sub>2</sub>)</td>
<td>&nbsp;</td><td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&#133;</td><td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>J<sub>n<sub>m</sub></sub><sup>p</sup>(&lambda;<sub>m</sub>)</td>
 </tr>
</table>

<p>Hence for any polynomial function of J</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=4 nowrap>F(J<sub>n</sub>) =</td>
<td rowspan=4><img src="MSLDEimg/matrixl4.bmp"></td>
<td>F[J<sub>n<sub>1</sub></sub>(&lambda;<sub>1</sub>)]</td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=4><img src="MSLDEimg/matrixr4.bmp"></td>
<td rowspan=4>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.18)</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>F[J<sub>n<sub>2</sub></sub>(&lambda;<sub>2</sub>)]</td>
<td>&nbsp;</td><td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&#133;</td><td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>F[J<sub>n<sub>m</sub></sub>(&lambda;<sub>m</sub>)]</td>
 </tr>
</table>

<p>Further, if J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub>)
is a Jordan box then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
F[J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub>)] =
F(&lambda;<sub>k</sub>I<sub>0</sub> + I<sub>1</sub>) =
F(&lambda;<sub>k</sub>)I<sub>0</sub> + 
F'(&lambda;<sub>k</sub>)I<sub>1</sub> +
<img src="MSLDEimg/TaylorF2.bmp" align=middle>(&lambda;<sub>k</sub>)I<sub>2</sub> 
+ &#133; .
<tt>&nbsp; &nbsp; &nbsp;</tt>(3.19)
</td></tr></table>

<p>For example,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3>J<sub>3</sub><sup>4</sup> =</td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>&lambda;<sub>k</sub><sup>4</sup> &nbsp;&nbsp;</td>
<td align=right>4&lambda;<sub>k</sub><sup>3</sup> &nbsp;&nbsp;</td>
<td align=right>6&lambda;<sub>k</sub><sup>2</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>.</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td align=right>&lambda;<sub>k</sub><sup>4</sup> &nbsp;&nbsp;</td>
<td align=right>4&lambda;<sub>k</sub><sup>3</sup></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=right>&lambda;<sub>k</sub><sup>4</sup></td>
 </tr>
</table>

<p>Formally applying the above results to the function J<sub>n</sub><sup>-1</sup>
we have for each Jordan box</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>F[J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub>)]</td>
<td>= J<sub>n<sub>k</sub></sub><sup>-1</sup>(&lambda;<sub>k</sub>) =
&lambda;<sub>k</sub><sup>-1</sup>I<sub>0</sub> &ndash;
&lambda;<sub>k</sub><sup>-2</sup>I<sub>1</sub> +
&lambda;<sub>k</sub><sup>-3</sup>I<sub>2</sub> &ndash;
&lambda;<sub>k</sub><sup>-4</sup>I<sub>3</sub> + &#133;</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>
= <img src="MSLDEimg/sigmap0nk1.bmp" align=middle> 
(-1)<sup>p</sup>&lambda;<sub>k</sub><sup>-(p+1)</sup>I<sub>p</sub>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.20)
</table>

<p>Premultiplication by J<sub>n<sub>k</sub></sub> gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub>)
J<sub>n<sub>k</sub></sub><sup>-1</sup>(&lambda;<sub>k</sub>)<img src="MSLDEimg/1x67w.bmp" align=middle></td>
<td>= (&lambda;<sub>k</sub>I<sub>0</sub> + I<sub>1</sub>)
<img src="MSLDEimg/sigmap0nk1.bmp" align=middle>
(-1)<sup>p</sup>&lambda;<sub>k</sub><sup>-(p+1)</sup>I<sub>p</sub></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= <img src="MSLDEimg/sigmap0nk1.bmp" align=middle> 
(-1)<sup>p</sup>&lambda;<sub>k</sub><sup>-p</sup>I<sub>p</sub> +
<img src="MSLDEimg/sigmap0nk1.bmp" align=middle> 
(-1)<sup>p</sup>&lambda;<sub>k</sub><sup>-(p+1)</sup>I<sub>p+1</sub></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= <img src="MSLDEimg/sigmap0nk1.bmp" align=middle> 
(-1)<sup>p</sup>&lambda;<sub>k</sub><sup>-p</sup>I<sub>p</sub> &ndash;
<img src="MSLDEimg/sigmap1nk1.bmp" align=middle> 
(-1)<sup>p</sup>&lambda;<sub>k</sub><sup>-p</sup>I<sub>p</sub></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= I<sub>0</sub>.</td>
 </tr>
</table>

<p>Therefore J<sub>n</sub><sup>-1</sup> as formally
define by (3.20) is indeed the inverse of J<sub>n</sub>
and the preceding results may therefore
be extended to negative powers of J.</p>

<p>To avoid difficulties due to the possible 
singularity of the matrix J
we consider instead the adjoint of J,
to be denoted by &Delta;(J) and defined by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&Delta;(J) = |J| J<sup>-1</sup>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.21)
</td></tr></table>

<p>Since the determinant |J| is equal to</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/pikm.bmp" align=middle>(&lambda;<sub>k</sub>)<sup>n<sub>k</sub></sup>
</td></tr></table>

<p>and since the largest negative power of &lambda; occurring 
in J<sup>-1</sup> is &lambda;<sub>k</sub><sup>-n<sub>k</sub></sup>
it follows that the adjoint is finite even for singular matrices.
Since J and J<sup>-1</sup> commute</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
J &Delta;(J) = &Delta;(J) J = |J| I.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.22)
</td></tr></table>

<a name="3C3"></a>
<p><b>(3) Principal Vectors Derived from the Adjoint</b></p>

<p>Since J<sub>n</sub>(&lambda;<sub>i</sub>,n<sub>i</sub>) &ndash; &lambda;I
= J<sub>n</sub>(&lambda;<sub>i</sub>&ndash;&lambda;,n<sub>i</sub>)
is a canonical matrix, 
substitution in (3.22) gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
[J<sub>n</sub>(&lambda;<sub>i</sub>,n<sub>i</sub>) &ndash; &lambda;I]
&Delta;[J<sub>n</sub>(&lambda;<sub>i</sub>&ndash;&lambda;,n<sub>i</sub>)]
= |J<sub>n</sub>(&lambda;<sub>i</sub>&ndash;&lambda;,n<sub>i</sub>)| I.
</td></tr></table>

<p>If &lambda; is chosen equal to any latent root &lambda;<sub>k</sub>
the determinant on the right
vanishes and therefore any non-trivial column 
of the adjoint 
&Delta;[J<sub>n</sub>(&lambda;<sub>i</sub>&ndash;&lambda;,n<sub>i</sub>)]
is an eigenvector of J<sub>n</sub> corresponding 
to the root &lambda;<sub>k</sub>
(see Eqs. (3.14) and the definition of an eigenvector 
in section 3C1).
The existence of such non-trivial columns
will now be demonstrated.</p>

<p>The form of a single box of the adjoint of (J &ndash; &lambda;I)
is shown below for n<sub>k</sub> = 3.</p>

<table cellspacing=0 cellpadding=0><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3 nowrap>
&Delta;[J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub> &ndash; &lambda;)]
= <img src="MSLDEimg/pijm.bmp" align=middle> 
(&lambda;<sub>j</sub> &ndash; &lambda;)<sup>n<sub>j</sub></sup>&nbsp;</td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>-1</sup> &nbsp;</td>
<td>-(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>-2</sup> &nbsp;</td>
<td align=right>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>-3</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3><tt>&nbsp; &nbsp;</tt>(3.23)</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td align=right>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>-1</sup> &nbsp;</td>
<td>-(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>-2</sup></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=right>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>-1</sup></td>
 </tr>
</table>

<p>In the definition of J<sub>n</sub>(&lambda;<sub>i</sub>,n<sub>i</sub>)
in section 3A11
the symbol &lambda;<sub>k</sub>
is used to denote the diagonal element of the kth Jordan box.
This notation is not meant to imply that the elements
&lambda;<sub>1</sub>,&lambda;<sub>2</sub>,&#133;,&lambda;<sub>m</sub>
are necessarily distinct.
To this point the occurrence of two or more equal values of
&lambda;<sub>k</sub> has not entered the discussion
but a distinction must now be made.</p>

<p>Let m<sub>k</sub> be the multiplicity of the root
&lambda;<sub>k</sub> in J.
Thus if T is the set of latent roots equal to
&lambda;<sub>k</sub> the multiplicity of &lambda;<sub>k</sub> is</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
m<sub>k</sub> = <img src="MSLDEimg/sigmaT.bmp" align=middle> n<sub>k</sub>.
</td></tr></table>

<p>In particular, if the &lambda;<sub>k</sub> are distinct
m<sub>k</sub> = n<sub>k</sub>;
and if a root &lambda;<sub>k</sub> is repeated in more than one
Jordan box then m<sub>k</sub> &gt; n<sub>k</sub>.</p>

<p>The symbol <img src="MSLDEimg/piu.bmp" align=middle> will now
be adopted for the product of the &lambda;&rsquo;s.
The prime indicates that the factors are chosen
only from a set of distinct &lambda;<sub>k</sub>.
Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/pijm.bmp" align=middle> 
&lambda;<sub>k</sub><sup>n<sub>k</sub></sup> =
<img src="MSLDEimg/pijm.bmp" align=middle><img src="MSLDEimg/prime.bmp" align=middle> 
&lambda;<sub>k</sub><sup>m<sub>k</sub></sup>.
</td></tr></table>

<p>The product in (3.23) may now be written as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/pijm.bmp" align=middle><img src="MSLDEimg/prime.bmp" align=middle> 
(&lambda;<sub>j</sub> &ndash; &lambda;)<sup>m<sub>j</sub></sup>.
</td></tr></table>

<p>Factoring out 
(&lambda;<sub>j</sub> &ndash; &lambda;)<sup>m<sub>k</sub></sup>
and multiplying it into the matrix, 
Eq. (2.23) appears as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3>&Delta;[J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub> 
&ndash; &lambda;)] =
<img src="MSLDEimg/pijnek.bmp" align=middle><img src="MSLDEimg/prime.bmp" align=middle>
(&lambda;<sub>j</sub> &ndash; &lambda;)<sup>m<sub>j</sub></sup></td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-1</sup> &nbsp;</td>
<td>-(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-2</sup> &nbsp;</td>
<td align=right>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-3</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td align=right>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-1</sup> &nbsp;</td>
<td align=right>-(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-2</sup></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=right>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-1</sup></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td align=right colspan=2><sup>&nbsp;<sup>&nbsp;</sup></sup>(3.24)</td>
</table>

<p>The same relation for a general value of n<sub>k</sub> 
may be written concisely in terms of the matrices
I<sub>k</sub> as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&Delta;[(J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub> &ndash; &lambda;)] =
<img src="MSLDEimg/pijnek.bmp" align=middle><img src="MSLDEimg/prime.bmp" align=middle>
(&lambda;<sub>j</sub> &ndash; &lambda;)<sup>m<sub>j</sub></sup>
<img src="MSLDEimg/sigmap0nk1.bmp" align=middle>
(-1)<sup>p</sup>(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-(p+1)</sup> I<sub>p</sub> .
<tt>
&nbsp; &nbsp; &nbsp;
</tt>(3.25)
</td></tr></table>

<p>Suppose now that &lambda;<sub>k</sub> is a latent root
occurring in a single Jordan box so that
<nobr>m<sub>k</sub> = n<sub>k</sub>.</nobr>
If the adjoint is evaluated at 
<nobr>&lambda; = &lambda;<sub>k</sub></nobr>
all Jordan boxes other than the kth are null
due to the occurrence of the factors
<nobr>(&lambda;<sub>k</sub> &ndash; &lambda;<sub>k</sub>)<sup>m<sub>k</sub></sup></nobr>
in the product
<img src="MSLDEimg/piu.bmp" align=middle>.
For the kth box the product</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/pijnek.bmp" align=middle><img src="MSLDEimg/prime.bmp" align=middle> 
(&lambda;<sub>k</sub> &ndash; &lambda;<sub>j</sub>)<sup>m<sub>j</sub></sup>
</td></tr></table>

<p>is some non-zero constant c<sub>0</sub>.
The kth box is also null 
except for an element equal to</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
c<sub>0</sub>(-1)<sup>n<sub>k</sub>-1</sup>(&lambda;<sub>k</sub> &ndash;
&lambda;<sub>k</sub>)<sup>m<sub>k</sub>-n<sub>k</sub></sup> = 
(-1)<sup>n<sub>k</sub>-1</sup>c<sub>0</sub>
</td></tr></table>

<p>in the upper right corner.
This may be verified by referring either 
to the example of (3.24) 
or to the general expression (3.25).
Therefore the only non-zero column
occurring in the adjoint is the vector
(-1)<sup>n<sub>k</sub>-1</sup>c<sub>0</sub>x<sub>k1</sub>.</p>

<p>Eigenvectors (and principal vectors) are defined
by a homogeneous equation and therefore any scalar multiple
of an eigenvector is also an eigenvector.
For convenience, the non-zero scalar multiplier
(-1)<sup>n<sub>k</sub>-1</sup>c<sub>0</sub>
in the column of the adjoint may be dropped 
to give x<sub>k1</sub>
as the eigenvector corresponding to &lambda;<sub>k</sub>.
This is the same result previously obtained
from Eqs. (3.16).</p>

<p>The remaining principal vectors associated
with &lambda;<sub>k</sub> may be obtained
by differentiating the adjoint
with respect to &lambda;.
For convenience the scalar constants
c<sub>p</sub> are defined as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
c<sub>p</sub> = <img src="MSLDEimg/dpdlamp.bmp" align=middle>
<img src="MSLDEimg/pijnek.bmp" align=middle><img src="MSLDEimg/prime.bmp" align=middle>
(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>j</sub></sup>
<img src="MSLDEimg/atlamk.bmp" align=middle> .
</td></tr></table>

<p>Differentiating (3.24) or (3.25) gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3><img src="MSLDEimg/ddlam.bmp" align=middle> 
&Delta;[J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub> &ndash; &lambda;)]
<img src="MSLDEimg/atlamk.bmp" align=middle> = c<sub>1</sub></td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>&nbsp; &nbsp;</td><td>&nbsp; &nbsp;</td>
<td>(-1)<sup>n<sub>k</sub>-1</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3 valign=middle>&ndash; c<sub>0</sub></td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>&nbsp; &nbsp;</td>
<td>(-1)<sup>n<sub>k</sub>-2</sup> &nbsp;</td>
<td align=center>0<sup>&nbsp;</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&nbsp;</td>
<td>(-1)<sup>n<sub>k</sub>-2</sup></td>
 </tr>
<tr>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&nbsp;</td><td>&nbsp;</td>
 </tr>
</table>

<p>or</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/ddlam.bmp" align=middle> 
&Delta;[(J<sub>n<sub>k</sub></sub>(&lambda;<sub>k</sub> &ndash; &lambda;)]
<img src="MSLDEimg/atlamk.bmp" align=middle> = 
c<sub>1</sub>(-I<sub>1</sub>)<sup>n<sub>k</sub>-1</sup> &ndash;
c<sub>0</sub>(-I<sub>1</sub>)<sup>n<sub>k</sub>-2</sup> .
</td></tr></table>

<p>Since c<sub>0</sub> &ne; 0 the derived adjoint
contains a non-zero column proportional 
to the eigenvector.
If c<sub>1</sub> = 0 it contains in addition
a linearly independent column with a component
proportional [to] x<sub>k2</sub>,
which is the principal vector of order 2
associated with &lambda;<sub>k</sub>.
If c<sub>1</sub> &ne; 0 this column contains
also a component in the direction of x<sub>k1</sub>.
This brings out a point already implicit in (3.16);
namely that a principal vector of order greater
than 1 is non-unique to the extent that it may
have added to it any linear combination
of the associated principal vectors of lower order.</p>

<p>The pattern should now be clear.
The final column in the kth Jordan box of</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/dpdlam.bmp" align=middle> &Delta;
<img src="MSLDEimg/atlamk.bmp" align=middle>
</td></tr></table>

<p>is a principal vector of order p+1 associated
with the eigenvalue &lambda;<sub>k</sub>.
At most p other columns are non-zero and
they are principal vectors of lower order.
Note also that for p &ge; n<sub>k</sub>
the derived adjoint is null.</p>

<p>The case where &lambda;<sub>k</sub> is repeated 
in two or more boxes may now be disposed of briefly.
The element of lowest degree in (&lambda;<sub>k</sub> &ndash; &lambda;)
which occurs in the adjoint is 
(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>m<sub>k</sub>-n<sub>k</sub></sup>
as shown by (3.24) or (3.25).
In this case m<sub>k</sub> &gt; n<sub>k</sub>
so that the adjoint evaluated at 
<nobr>&lambda; = &lambda;<sub>k</sub></nobr>
is null, as are all derived adjoints up to but not including
the (m<sub>k</sub>&ndash;n<sub>k</sub>)th.
The (m<sub>k</sub>&ndash;n<sub>k</sub>)th derived adjoint
has a single non-zero column proportional
to the eigenvector associated with the kth box.
The successive derived adjoints up to the (m<sub>k</sub>&ndash;1)st
repeat the pattern established above
and yield in all the n<sub>k</sub> principal vectors
associated with the kth box.
Note, however, that this same process is going on 
simultaneously for all boxes with diagonal elements
equal to &lambda;<sub>k</sub>.
Thus more than one new independent column
may be introduced by each differentiation.
In particular, the (m<sub>k</sub>&ndash;1)st
derived adjoint introduces all the principal vectors
of highest order associated with each occurrence
of &lambda;<sub>k</sub>.</p>

<a name="3C4"></a>
<p><b>(4) Principal Vectors of a General Matrix</b></p>

<p>The results obtained above for a canonical matrix 
may now be extended to a general matrix A by the use
of the following train of relations: If</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>A</td><td>= SJS<sup>-1</sup>,</td></tr>
<tr><td>then</td></tr>
<tr><td>&nbsp;</td><td>A<sup>-1</sup></td><td>= SJ<sup>-1</sup>S<sup>-1</sup></td></tr>
<tr><td>&nbsp;</td></tr>
<tr><td>&nbsp;</td>
 <td>&Delta;(A &ndash; &lambda;I)&nbsp;</td><td>= | A &ndash; &lambda;I | A<sup>-1</sup></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td>= | J &ndash; &lambda;I | SJ<sup>-1</sup>S<sup>-1</sup></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
 <td>= S | J &ndash; &lambda;I | J<sup>-1</sup>S<sup>-1</sup></td></tr>
<tr><td>&nbsp;</td>
<tr><td>&there4;</td>
 <td>&Delta;(A &ndash; &lambda;I)&nbsp;</td><td>= S&Delta;(J &ndash; &lambda;I)S<sup>-1</sup>
 <tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 </tt>(3.26)</td></tr>
</table>

<p>and</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/dpdlamp.bmp" align=middle> &Delta;(A &ndash; &lambda;I)
= S [<img src="MSLDEimg/dpdlamp.bmp" align=middle> &Delta;(J &ndash; &lambda;I)] 
S<sup>-1</sup>, &nbsp; &nbsp; p = 0,1,2,&#133;
</td></tr></table>

<p>Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(A &ndash; &lambda;I)<sup>q</sup>
<img src="MSLDEimg/dpdlamp.bmp" align=middle> 
&Delta;(A &ndash; &lambda;I)
= S [(J &ndash; &lambda;I)<sup>q</sup>
<img src="MSLDEimg/dpdlamp.bmp" align=middle> &Delta;(J &ndash; &lambda;I)] 
S<sup>-1</sup>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(3.27)
</td></tr></table>

<p>Since S is non-singular Eq. (3.26) shows that
&Delta;(A &ndash; &lambda;I) possesses
the same number of independent columns as does
&Delta;(J &ndash; &lambda;I).
Also from (3.27) the functions</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(A &ndash; &lambda;I)<sup>q</sup>
<img src="MSLDEimg/dpdlamp.bmp" align=middle> 
&Delta;(A &ndash; &lambda;I) &nbsp; and &nbsp; (J &ndash; &lambda;I)<sup>q</sup>
<img src="MSLDEimg/dpdlamp.bmp" align=middle> &Delta;(J &ndash; &lambda;I)
</td></tr></table>

<p>are seen to have the same zeros.
From the definition of a principal vector
and the results obtained for the canonical matrix,
it follows that the derived adjoints of A
contain columns proportional 
to the principal vectors of A.</p>

<a name="3D"></a>
<p><b>D. The Singular Case</b></p>

<p>In passing from Eq. (3.2) to Eq. (3.3) it is necessary
to assume that the matrix B is non-singular.
The singular case will now be considered.</p>

<p>If the matrix B is of rank (n-r)
it is possible by rearranging and taking linear sums
of the component equations of (3.2)
to reduce them to an equivalent system
in which the coefficients of the derivatives
in the last r equations are zero.
The last r equations may therefore be solved
directly for r of the variables in terms
of the remaining variables.
These r variables may then be eliminated
from the first (n-r) equations.
We are then left with a reduced system of equations
in (n-r) variables of the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
A<sub>1</sub>x &ndash; B<sub>1</sub><img src="MSLDEimg/xdot.bmp"> = 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp;
</tt>(3.28)
</td></tr>
</table>

<p>where B<sub>1</sub> is non-singular.
Solutions of the reduced system may now be obtained
by the methods previously described
and the r eliminated variables
are expressible in terms of these solutions.</p>

<p>The general solution now contains only (n-r) arbitrary constants
and the system may be said to have lost r degrees of freedom.
The initial values of the variables are therefore
subject to r linear restraints 
as determined by those r equations which are
free of terms in derivatives.</p>

<p>If the matrix A is non-singular 
an alternative method is available.
Premultiplication of (3.2) by A<sup>-1</sup> gives:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Ix &ndash; C<img src="MSLDEimg/xdot.bmp"> = 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.29)
</td></tr></table>

<p>where</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
C = A<sup>-1</sup>B.
</td></tr></table>

<p>Let J = S<sup>-1</sup>CS be the canonical form of C.
Then Eq. (3.29) may be transformed to</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y &ndash; J<img src="MSLDEimg/ydot.bmp" align=middle> = 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(3.30)
</td></tr></table>

<p>where</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y = S<sup>-1</sup>x.
</td></tr></table>

<p>Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y = e<sup>J<sup>-1</sup>t</sup>c
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.31)
</td></tr></table>

<p>is the general solution of (3.30). [Compare Eq. (3.11).]</p>

<p>If B is non-singular so are C and J and hence (3.31)
is a valid result.
If B is singular J<sup>-1</sup> does not exist
but the solution may still be made meaningful
in the following way.
Because C is singular it has at least one latent root
equal to zero.
Suppose it has exactly one zero root
and let S be so chosen that this root occurs
as the last element of J.
Formally inverting J by the use of Eq. (3.20)
the last latent root of J<sup>-1</sup> becomes infinite.
The solution y = e<sup>J<sup>-1</sup>t</sup>c therefore
has a discontinuity at t = 0 
unless the vector c 
(i.e., the vector of initial conditions)
is so chosen that it has no component in the direction
of the eigenvalue of J 
corresponding to the zero latent root.
A single linear restraint is thus imposed
on the variables and at the same time
one of the exponential terms 
in the general solution is dropped.
This eigenvector of the canonical matrix J is of course
the vector e<sub>n</sub> (see Eq. (3.16)).
The restraint is therefore simply c<sub>n</sub> = 0
and hence y<sub>n</sub> = 0.
This is the same linear restriction arrived at
by the previous method as may be verified as follows:
The transformation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
S<sup>-1</sup>A<sup>-1</sup>[Ax &ndash; B<img src="MSLDEimg/xdot.bmp">] 
&nbsp; &nbsp;</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td></tr>
<tr><td>&nbsp;</td><td>x = Sy</td></tr>
</table>

<p>is one of the possible ways of reducing Eq. (3.2) 
to the form (3.28),
for we obtain (3.30) namely,</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y &ndash; J<img src="MSLDEimg/ydot.bmp" align=middle> = 0.
</td></tr></table>

<p>Since the last row of the matrix J is null
the system may now be reduced to the form (3.28)
by dropping the last equation and using it
as an auxiliary restraint.
This equation is simply y<sub>n</sub> = 0 as
was to be demonstrated.
The same reasoning may be used to extend this result
to the case of multiple zero roots of B.
We may therefore lay down the following procedure
for the case when B is singular.</p>

<table>
<tr><td valign=top nowrap>1. <sup>&nbsp;</sup></td><td>
Obtain the latent roots and principal vectors of the matrix A<sup>-1</sup>B.
 </td></tr>
<tr><td valign=top nowrap>2.</td><td>
Use the solution (3.31) but with terms corresponding
to the &ldquo;infinite&rdquo; roots of J<sup>-1</sup>
deleted and the components of c
in the direction of the corresponding eigenvectors made zero.
 </td></tr>
</table>

<p>This result is very useful in the present problem 
since the matrix A is known to be non-singular and 
the matrix B is in some cases singular.
Also the inverse of the matrix A happens to be
of interest in itself since A<sup>-1</sup>
is the solution of the static system (1.1).</p>

<a name="3E"></a>
<p><b>E. The Particular Integral</b></p>

<p>The particular integral of (3.1) is now easily obtained.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Ax &ndash; B<img src="MSLDEimg/xdot.bmp"> 
= z.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.1)
</td></tr></table>

<table>
<tr><td colspan=2>Again assuming B non-singular, <sup>&nbsp;</sup></td></tr>
<tr><td width=28 nowrap>&nbsp;</td>
 <td>B<sup>-1</sup>Ax &ndash; <img src="MSLDEimg/xdot.bmp"> = B<sup>-1</sup>z.</td></tr>
<tr><td colspan=2>Let</td></tr>
<tr><td>&nbsp;</td><td>B<sup>-1</sup>A = SJS<sup>-1</sup>.</td></tr>
<tr><td colspan=2>Then</td></tr>
<tr><td>&nbsp;</td><td>S<sup>-1</sup>JSx &ndash; 
S<sup>-1</sup>IS<img src="MSLDEimg/xdot.bmp"> = B<sup>-1</sup>z.</td></tr>
<tr><td colspan=2>Letting y = Sx yields <sup>&nbsp;</sup> <sub>&nbsp;</sup></td></tr>
<tr><td>&nbsp;</td><td>Jy &ndash; <img src="MSLDEimg/ydot.bmp" align=middle>
= SB<sup>-1</sup>z.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt> 
(3.32)</td></tr>
</table>

<p>A particular integral is then given by</p>

<table><td><td width=28 nowrap>&nbsp;</td><td>
y = -e<sup>Jt</sup> <img src="MSLDEimg/integral.bmp" align=middle> 
e<sup>-Jt</sup> SB<sup>-1</sup>z <i>d</i>t
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>
(3.33)
</td></tr></table>

<p>as may be verified by substitution in (3.32).  Finally,</p>

<table><td><td width=28 nowrap>&nbsp;</td><td>
x = S<sup>-1</sup>y = -S<sup>-1</sup>e<sup>Jt</sup> 
<img src="MSLDEimg/integral.bmp" align=middle> 
e<sup>-Jt</sup> SB<sup>-1</sup>z <i>d</i>t .
</td></tr></table>

<a name="3F"></a>
<p><b>F. The Economic Model</b></p>

<p>The equations of motion (1.2) of the dynamic economic model
proposed in Chapter 1 may be expressed in matrix notation as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(I &ndash; A)x &ndash; B<img src="MSLDEimg/xdot.bmp"> = z
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp;
</tt>(3.34)
</td></tr></table>

<p>where A = [a<sub>ij</sub>] is the matrix of flow coefficients
and B = [b<sub>ij</sub>] is the matrix of capital coefficients.
As indicated in section 3D the associated homogeneous equation
may be put in the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x &ndash; D<img src="MSLDEimg/xdot.bmp"> = 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;
</tt>(3.35)
</td></tr></table>

<p>where D = (I &ndash; A)<sup>-1</sup>B.
The general solution is then expressible 
in terms of the latent roots and principal vectors of D.</p>

<p>As indicated in Chapter 1 the final demand functions 
of interest in the present problem are of the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
z = ge<sup>&mu;t</sup>
</td></tr></table>

<p>where g is an arbitrary vector and &mu; is a constant.
A particular integral of (3.34) then takes the simple form</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x = (I &ndash; A &ndash; &mu;B)<sup>-1</sup>ge<sup>&mu;t</sup>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(3.36)
</td></tr></table>

<p>This may be verified by substitution in (3.34) as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
{(I &ndash; A)[I &ndash; A &ndash; &mu;B]<sup>-1</sup> &ndash;
&mu;B[I &ndash; A &ndash; &mu;B]<sup>-1</sup>}ge<sup>&mu;t</sup> =
Ige<sup>&mu;t</sup> = z.
</td></tr></table>

<a name="notes3"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top><a name="note3a"></a>a. &nbsp;</td>
 <td>A system of linear differential equations of general order
may be reduced to an equivalent first order 
system.<sup><acronym title="9. Frazer, R.A., Duncan, W.J., and Collar, R.A., Elementary Matrices, Cambridge University Press, 1950.">9</acronym>
 </td></tr>
<tr><td valign=top><a name="note3b"></a>b.</td>
 <td>The particular order of occurrence 
of the Jordan boxes along the main diagonal
is of no significance and may be changed 
by a different choice of the matrix S.
 </td></tr>
</table>
<br>&nbsp;<br>





<a name="4"></a>
<p><b>Chapter 4. &nbsp; Latent Roots and Principal Vectors</b></p>

<p>In Chapter 3 the solution of linear differential equations
with constant coefficients was reduced to the problem
of evaluating the latent roots and principal vectors
of a certain matrix.
A number of possible computational methods will now
be considered with a view to choosing those methods
best suited to the solution of the problem 
outlined in Chapter 1.
Methods such as that of Jacobi which apply only
to symmetric matrices will not be considered.</p>

<p>The complete solution of this problem requires 
the calculation of all the latent roots
and all the principal vectors of the matrix
(I &ndash; A)<sup>-1</sup>B.
Since the matrix is not symmetric,
complex roots may be expected to occur
and indeed these complex roots are of
particular interest in the theory 
of the business cycle.<sup><a href="MSLDE1.htm#note4a">[a]</a>&nbsp;</sup>
The method chosen must therefore be capable
of extracting all roots, real or complex,
and the associated principal vectors.</p>

<a name="4A"></a>
<p><b>A. Bounds on the Roots</b></p>

<p>The rate of accumulation of round-off error in some
of the methods to be described depends critically
on the magnitude of the latent roots of the matrix concerned.
The latent roots also determine the magnitude
of the largest number to be encountered 
in the course of the 
calculation.<sup><a href="MSLDE1.htm#note4b">[b]</a>&nbsp;</sup>
For two reasons then it is useful to have a preliminary estimate
of the latent roots.
The best such estimate available with a small amount
of computation is furnished by Gerg&scaron;gorin&rsquo;s
theorem<sup><acronym title="11. Geršgorin, S., “Über die Abgrenzung der Eigenwerte einer Matrix”, Izvestiia Akademie Nauk SSSR, VII, (Mat Klass.) (1931) pp. 749-754.">11</acronym></sup>
which follows.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<p><b>Theorem</b>: Let C<sub>i</sub> be the closed region
of the complex plane enclosed by a circle
with center at a<sub>ii</sub> and radius</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
r<sub>i</sub> = <img src="MSLDEimg/sigmajn.bmp" align=center>
|a<sub>ij</sub>| .
</td></tr></table>

<p>Then all the latent roots of the matrix 
A = [a<sub>ij</sub>] lie in the region
formed by the union of the circles C<sub>i</sub>,
i = 1,2,&#133;,n.</p>
</td></tr></table>

<p><b>Proof</b>: Let x be an eigenvector of A
corresponding to the latent root &lambda;.
Then Ax = &lambda;x and</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td><img src="MSLDEimg/sigmajn.bmp" align=center> 
a<sub>ij</sub>x<sub>j</sub> = &lambda;x<sub>i</sub> ,</td>
<td rowspan=5><img src="MSLDEimg/matrixr5.bmp"></td>
<td rowspan=5>&nbsp; i = 1,2,&#133;,n;</td>
 </tr>
<tr><td>&there4;</td>
<td><img src="MSLDEimg/sigmajnei.bmp" align=center>
a<sub>ij</sub>x<sub>j</sub> = 
(&lambda; &ndash; a<sub>ii</sub>)x<sub>i</sub></td>
 </tr>
<tr><td>&there4;</td>
<td><img src="MSLDEimg/sigmajnei.bmp" align=center>
|a<sub>ij</sub>||x<sub>j</sub>| &ge; 
|&lambda; &ndash; a<sub>ii</sub>||x<sub>i</sub>| 
. &nbsp; &nbsp; &nbsp;</td>
 </tr></table>

<p>Let x<sub>k</sub> = max |x<sub>j</sub>| &ne; 0
since x is non-trivial.  Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/sigmajnei.bmp" align=center>
|a<sub>ij</sub>||x<sub>k</sub>| &ge; 
|&lambda; &ndash; a<sub>ii</sub>||x<sub>k</sub>| .</td>
</td></tr></table>

<p>Dividing by |x<sub>k</sub>| we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
|&lambda; &ndash; a<sub>ii</sub>| &le;
<img src="MSLDEimg/sigmajnei.bmp" align=center>
|a<sub>ij</sub>|
</td></tr></table>

<p>Thus &lambda; lies in one of the circles C<sub>i</sub>
and the theorem is proved.</p>

<p>Since A<sup>T</sup> has the same latent roots
as A a similar theorem holds for circles
D<sub>i</sub> with centers a<sub>ii</sub>
and radii</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/sigmajnei.bmp" align=center>
|a<sub>ij</sub>| .
</td></tr></table>

<p>Clearly then the two bounds to the roots are given by</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td><td>|&lambda;<sub>max</sub>| &le; 
<img src="MSLDEimg/maxi.bmp" align=middle>
<img src="MSLDEimg/sigmajn.bmp" align=middle>
|a<sub>ij</sub>|</td></tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td><td><td>|&lambda;<sub>max</sub>| &le;
<img src="MSLDEimg/maxj.bmp" align=middle>
<img src="MSLDEimg/sigmain.bmp" align=center>
|a<sub>ij</sub>| .</td></tr>
</table>

<a name="4B"></a>
<p><b>B. The Power Method</b></p>

<a name="4B1"></a>
<p><b>(1) Single Dominant Root</b></p>

<p>The power method in its simplest form can be used
only to obtain the dominant root of a matrix 
and owes its popularity to the large number
of problems in which a single dominant root is of interest.
The method is based on the fact that for any vector x
with a non-zero component in the direction
of the largest eigenvector<sup><a href="MSLDE1.htm#note4c">[c]</a>&nbsp;</sup>
the sequence of vectors Ax,A<sup>2</sup>x,&#133;,A<sup>p</sup>x
approaches the largest eigenvector of the matrix A.
The ratios of corresponding components 
of two successive vectors of the sequence
approach the largest latent root of the matrix A.
The proof of these statements is 
easily established with the use 
of the Jordan canonical form of A. For if</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>A = SJS<sup>-1</sup>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
</tt>(4.1)</td>
 </tr>
<tr><td>then</td></tr>
<tr><td>&nbsp;</td><td>
A<sup>p</sup>x = SJ<sup>p</sup>(S<sup>-1</sup>x).</td>
</tr>
</table>

<p>If &lambda;<sub>1</sub> is the dominant root
of A then J<sup>p</sup> has the form<sup><a href="MSLDE1.htm#note4d">[d]</a></p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=4>J<sup>p</sup> =</td>
<td rowspan=4><img src="MSLDEimg/matrixl4.bmp"></td>
<td>&lambda;<sub>1</sub><sup>p</sup></td>
<td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=4><img src="MSLDEimg/matrixr4.bmp"></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>J<sub>n<sub>2</sub></sub><sup>p</sup>(&lambda;<sub>2</sub>)</td>
<td>&nbsp;</td><td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&#133;</td>
<td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>J<sub>n<sub>m</sub></sub><sup>p</sup>(&lambda;<sub>m</sub>)</td>
 </tr>
</table>

<p>where each submatrix is of the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=5>J<sub>k</sub><sup>p</sup>(&lambda;<sub>k</sub>) =</td>
<td rowspan=5><img src="MSLDEimg/matrixl5.bmp"></td>
<td>&lambda;<sub>k</sub><sup>p</sup> &nbsp;</td>
<td>n&lambda;<sub>k</sub><sup>p-1</sup> &nbsp;</td>
<td><img src="MSLDEimg/nC2.bmp" align=middle> &lambda;<sub>k</sub><sup>p-2</sup></td>
<td>&nbsp;</td><td>&#133;</td>
<td rowspan=5><img src="MSLDEimg/matrixr5.bmp"></td>
<td rowspan=5>.</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>&lambda;<sub>k</sub><sup>p</sup></td>
<td align=center>n&lambda;<sub>k</sub><sup>p-1</sup></td>
<td>&nbsp;</td><td>&#133;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>&lambda;<sub>k</sub><sup>p</sup></td>
<td>&nbsp;</td><td>&#133;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td>&#133;</td><td>&nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
 <td>&lambda;<sub>k</sub><sup>p</sup></td>
 </tr>
</table>

<p>Since &lambda;<sub>1</sub> is dominant</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=4><img src="MSLDEimg/limitp.bmp" align=middle> J<sup>p</sup> 
= &lambda;<sub>1</sub><sup>p</sup></td>
<td rowspan=4><img src="MSLDEimg/matrixl3.bmp"></td>
<td>1</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td rowspan=4><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=4>.</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>0</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&#133;</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>0</td></tr>
</table>

<p>Hence</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/limitp.bmp" align=middle> 
J<sup>p</sup>(S<sup>-1</sup>x)
= &lambda;<sub>1</sub><sup>p</sup>&alpha;e<sub>1</sub> ,
</td></tr></table>

<p>where &alpha; is the first component of
(S<sup>-1</sup>x) and e<sub>1</sub> is a unit vector.
Since the eigenvector of A corresponding to &lambda;<sub>1</sub>
is Se<sub>1</sub> by Eq. (3.17)
and since x has a component in the direction 
of this eigenvector
it follows that &alpha; is not zero. Thus</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/limitp.bmp" align=middle> 
A<sup>p</sup>x =
<img src="MSLDEimg/limitp.bmp" align=middle>
SJ<sup>p</sup>(S<sup>-1</sup>x) =
&lambda;<sub>1</sub><sup>p</sup>&alpha;Se<sub>1</sub> .
</td></tr></table>

<p>The expression on the right is an eigenvector of A.
Furthermore,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3><img src="MSLDEimg/limitp.bmp"></td>
<td align=center>(A<sup>p</sup>x)<sub>j</sub></td>
<td rowspan=3>=</td>
<td align=center>&lambda;<sub>1</sub><sup>p</sup>&alpha;(Se<sub>1</sub>)<sub>j</sub></td>
<td rowspan=3>= &lambda;<sub>1</sub> .</td>
 </tr>
<tr><td></td><td><img src="MSLDEimg/50x1.bmp"></td>
 <td><img src="MSLDEimg/80x1.bmp"></td></tr>
<tr><td></td>
 <td align=center>(A<sup>p-1</sup>x)<sub>j</sub></td>
 <td align=center>&lambda;<sub>1</sub><sup>p-1</sup>&alpha;(Se<sub>1</sub>)<sub>j</sub></td></tr>
</table>

<p>The convergence is linear and depends on the ratios</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<td>|</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&lambda;<sub>1</sub></td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>&lambda;<sub>k</sub></td></tr>
 </table></td>
<td>|.</td>
</tr></table>

<p>For this reason troubles arise when two or more
roots of equal 
modulus<sup><a href="MSLDE1.htm#note4e">[e]</a></sup> occur.
This problem will be deferred and for the present
the roots are assumed to occur singly.</p>

<a name="4B2"></a>
<p><b>(2) Smaller Roots</b></p>

<p>The simple power method may be extended to obtain
roots other than the dominant root in either of two ways,
which will be referred to 
as the modified power method and
the method of deflation.</p>

<p><b>The Modified Power Method</b>.
Let the roots of matrix A be denoted by
&lambda;<sub>1</sub>,&lambda;<sub>2</sub>,&#133;,&lambda;<sub>n</sub>
where &lambda;<sub>1</sub> &gt; &lambda;<sub>2</sub> &gt; &#133; &gt; &lambda;<sub>n</sub>.
Equations (3.18) and (3.19) show that if F
is any polynomial function then the roots &mu;<sub>i</sub>
of the matrix F(A) are given by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&mu;<sub>i</sub> = F(&lambda;<sub>i</sub>).
</td></tr></table>

<p>In particular if <nobr>B = A &ndash; &alpha;I</nobr> 
then the roots of B
are <nobr>&lambda;<sub>i</sub> &ndash; &alpha;.</nobr>
Thus if <nobr>&alpha; &ge; &lambda;<sub>1</sub></nobr>
the dominant root of B becomes 
<nobr>&mu;<sub>n</sub> = &lambda;<sub>n</sub> &ndash; &alpha;.</nobr>
It is then possible to determine &mu;<sub>n</sub>
by applying the power method to B and so obtain &lambda;<sub>n</sub>.
Furthermore, Eq. (3.20) shows that the roots of A<sup>-1</sup>
are the reciprocals of the roots of A.
Thus the roots of the matrix 
<nobr>B = (A &ndash; &alpha;I)<sup>-1</sup></nobr>
are the quantities</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&mu;<sub>i</sub> = 1 / (&lambda;<sub>i</sub> &ndash; &alpha;) .
</td></tr></table>

<p>Clearly any one of the &mu;<sub>i</sub> may be made dominant
by choosing &alpha; close enough to the corresponding
root &alpha;<sub>i</sub>.
Thus each value of &mu;<sub>i</sub>
(and from it &lambda;<sub>i</sub>)
may be obtained from the matrix B by a suitable
choice of &alpha;.
In the absence of any knowledge 
of the location of &lambda;<sub>i</sub>,
the required values of &alpha; must be obtained 
by repeated trials.</p>

<p><b>The Method of Deflation</b>.
This method, like the preceding,
modifies the matrix A so as to make a secondary
root dominant, but differs in that the dominant root
only is changed and is replaced by zero.
If the roots of A are distinct and if c<sub>i</sub>
and r<sub>i</sub> are respectively
the eigenvectors of A and A<sup>T</sup>
corresponding to the root &lambda;<sub>i</sub>
and with length so chosen that
r<sub>i</sub><sup>T</sup>c<sub>i</sub> = 1, then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
r<sub>i</sub><sup>T</sup>c<sub>i</sub> =  &delta;<sub>ij</sub> .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;
</tt>(4.2)
</td></tr></table>

<p>For</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>r<sub>i</sub><sup>T</sup>(Ac<sub>j</sub>) =  
(r<sub>i</sub><sup>T</sup>A)c<sub>j</sub> .
 </td></tr>
<tr><td><font size=-1>&nbsp;</font></td></tr>
<tr><td>&there4;</td><td>
&lambda;<sub>j</sub>r<sub>i</sub><sup>T</sup>c<sub>j</sub> =  
&lambda;<sub>i</sub>r<sub>i</sub><sup>T</sup>c<sub>j</sub>
 </td></tr>
<tr><td><font size=-1>&nbsp;</font></td></tr>
<tr><td>and</td><td>
(&lambda;<sub>j</sub> &ndash; &lambda;<sub>i</sub>)r<sub>i</sub><sup>T</sup>c<sub>j</sub> 
= 0.
 </td></tr></table>

<p>If i &ne; j then &lambda;<sub>i</sub> &ne; &lambda;<sub>j</sub>
and hence r<sub>i</sub><sup>T</sup>c<sub>j</sub> = 0.
Equation (4.2) is therefore established.
If will now be shown that</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
A = <img src="MSLDEimg/sigmain.bmp" align=middle>
&lambda;<sub>i</sub>c<sub>i</sub>r<sub>i</sub><sup>T</sup> .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp;
</tt>(4.3)
</td></tr></table>

<p>Note that each product c<sub>i</sub>r<sub>i</sub><sup>T</sup>
is a square matrix of order n.
To establish the relation (4.3) consider the matrix product</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>(A &ndash; <img src="MSLDEimg/sigmain.bmp" align=middle>
&lambda;<sub>i</sub>c<sub>i</sub>r<sub>i</sub><sup>T</sup>)c<sub>j</sub></td>
<td>= Ac<sub>j</sub> &ndash; 
<img src="MSLDEimg/sigmain.bmp" align=middle>
&lambda;<sub>i</sub>c<sub>i</sub>r<sub>i</sub><sup>T</sup>c<sub>j</sub>
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= &lambda;<sub>j</sub>c<sub>j</sub> &ndash; 
<img src="MSLDEimg/sigmain.bmp" align=middle>
&lambda;<sub>i</sub>c<sub>i</sub>&delta;<sub>ij</sub>
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= (&lambda;<sub>j</sub> &ndash; &lambda;<sub>j</sub>)c<sub>j</sub> = 0.
 </td></tr>
</table>

<p>Since this is true for each of the n linearly independent
vectors c<sub>j</sub>,
it follows that the matrix in brackets above is null
and (4.3) is established.</p>

<p>Suppose the dominant root and corresponding eigenvector
of A have been obtained.
Then the corresponding eigenvector of A<sup>T</sup>
may be obtained by re-applying the power method,
or more simply (since &lambda;<sub>1</sub> is now known),
by solving the singular homogeneous system</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(A<sup>T</sup> &ndash; &lambda;<sub>1</sub>I)x = 0
</td></tr></table>

<p>to obtain a non-trivial vector x.
The &ldquo;deflated&rdquo; matrix</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
B = A &ndash; &lambda;<sub>1</sub>c<sub>1</sub>r<sub>1</sub><sup>T</sup>
= 0&middot;c<sub>1</sub>r<sub>1</sub><sup>T</sup> +
<img src="MSLDEimg/sigmai2n.bmp" align=middle>
&lambda;<sub>i</sub>c<sub>i</sub>r<sub>i</sub><sup>T</sup>
</td></tr></table>

<p>is now formed.
Clearly B has the same roots as A except that
the dominant root &lambda;<sub>1</sub> has been
replaced by zero.
The root &lambda;<sub>2</sub> of B may now be obtained
by a further application of the power method.
The process may be repeated for successive roots
but the error accumulation is rather rapid.
Other methods of carrying out the deflation process
are discussed by Feller and 
Forsythe.<sup><acronym title="14. Feller, W., and Forsythe, G.E., “Matrix Transformations for Obtaining Characteristic Vectors”, Quarterly Journal of Applied Mathematics, 8 (1950).">14</acronym></sup>
If A is symmetric r<sub>i</sub> = c<sub>i</sub>
and a separate calculation of r<sub>1</sub>
is not required.</p>

<a name="4B3"></a>
<p><b>(3) Multiple Roots</b></p>

<p>If multiple roots occur two cases must be distinguished:</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
The canonical form of J is diagonal and hence

<table><tr><td width=28 nowrap>&nbsp;</td><td>
n<sub>k</sub> = 1, &nbsp; &nbsp; k = 1,2,&#133;,m.
</td></tr></table>

In this case the principal vectors are all of order 1
and therefore the eigenvectors are n in number
and form a complete set.
The matrix A is said to be non-defective.
 </td></tr>
<tr><td valign=top>2.</td><td>
The order of one or more of the Jordan boxes exceeds one and 
the eigenvectors do not form a complete set.
The matrix A is said to be defective.
 </td></tr>
</table>

<p>If the matrix A is non-defective there are two or more
independent eigenvectors corresponding to the same root.
Furthermore any linear combination of these
independent eigenvectors is itself an eigenvector.
Thus the power method will converge to the dominant latent root
but the eigenvector obtained will not be unique.
By starting from different initial vectors the process
will in general converge to different possible eigenvectors
associated with the dominant root
so that finally all of the independent eigenvectors are obtained.
</p>

<p>If the matrix A is defective the form of a power 
of the Jordan box given in (3.19) shows that
a single limit of the sequence
Ax,A<sup>2</sup>x,&#133;,A<sup>p</sup>x exists.
For example, if &lambda;<sub>1</sub> is dominant 
and n<sub>1</sub> = 3 then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3>J<sub>3</sub><sup>p</sup>(&lambda;<sub>1</sub>)</td>
<td rowspan=3>=</td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>&lambda;<sub>1</sub><sup>p</sup> &nbsp;</td>
<td>p&lambda;<sub>1</sub><sup>p-1</sup> &nbsp;</td>
<td>&frac12; p(p&ndash;1)&lambda;<sub>1</sub><sup>p-2</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>&lambda;<sub>1</sub><sup>p</sup> &nbsp;</td>
<td align=center>p&lambda;<sub>1</sub><sup>p-1</sup> &nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>&lambda;<sub>1</sub><sup>p</sup> &nbsp;</td>
 </tr>
</table>

&nbsp;

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=11>= &frac12; p(p&ndash;1)&lambda;<sub>1</sub><sup>p-2</sup></td>
<td rowspan=11><img src="MSLDEimg/matrixl6.bmp"></td>
<td align=center>2&lambda;<sub>1</sub><sup>2</sup></td>
<td rowspan=11>&nbsp;</td>
<td align=center>2&lambda;<sub>1</sub></td>
<td rowspan=11>&nbsp;</td>
<td align=center rowspan=3>1</td>
<td rowspan=11><img src="MSLDEimg/matrixr6.bmp"></td>
<td rowspan=11>.</td>
 </tr>
<tr><td></td>
<td><img src="MSLDEimg/42x1.bmp"></td>
<td align=center><img src="MSLDEimg/35x1.bmp"></td>
 </tr>
<tr><td></td>
<td align=center>p(p&ndash;1)</td>
<td align=center>(p&ndash;1)</td>
 </tr>
<tr><td><img src="MSLDEimg/1x4w.bmp"></td></tr>

<tr><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>2&lambda;<sub>1</sub><sup>2</sup></td>
<td align=center>2&lambda;<sub>1</sub></td>
 </tr>
<tr><td></td><td></td>
<td><img src="MSLDEimg/42x1.bmp"></td>
<td align=center><img src="MSLDEimg/35x1.bmp"></td>
 </tr>
<tr><td></td><td></td>
<td align=center>p(p&ndash;1)</td>
<td align=center>(p&ndash;1)</td>
 </tr>
<tr><td><img src="MSLDEimg/1x4w.bmp"></td></tr>

<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td>
<td align=center>2&lambda;<sub>1</sub><sup>2</sup></td>
 </tr>
<tr><td></td><td></td><td></td>
<td><img src="MSLDEimg/42x1.bmp"></td>
 </tr>
<tr><td></td><td></td><td></td>
<td align=center>p(p&ndash;1)</td>
 </tr>
</table>

<p>Therefore</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=3><img src="MSLDEimg/limitp.bmp" align=middle> 
J<sub>3</sub><sup>p</sup>(&lambda;<sub>1</sub>) = 
&frac12; p(p&ndash;1)&lambda;<sub>1</sub><sup>p-2</sup></td>
<td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
<td>0</td>
<td rowspan=3>&nbsp;</td>
<td>0</td>
<td rowspan=3>&nbsp;</td>
<td>1</td>
<td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=3>.</td>
 </tr>
<tr><td></td><td></td><td>0</td><td>0</td></tr>
<tr><td></td><td></td><td></td><td>0</td></tr>
</table>

<p>Thus the sequence Ax,A<sup>2</sup>x,&#133;,A<sup>p</sup>x
converges always to the <i>eigenvector</i>
associated with the largest root.
The power method therefore fails to
obtain the principal vectors of order greater than one.</p>

<a name="4C"></a>
<p><b>C. Methods Employing the Characteristic Polynomial</b></p>

<p>The characteristic polynomial of a matrix A
is defined as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P(&lambda;) = | A &ndash; &lambda;I |.
</td></tr></table>

<p>The equation P(&lambda;) = 0 is called the characteristic
equation of the matrix A.
P(&lambda;) is a polynomial of degree n and
for a canonical matrix the expansion 
of the determinant clearly yields</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P(&lambda;) = | J &ndash; &lambda;I | = 
<img src="MSLDEimg/pikm.bmp" align=middle>
(&lambda;<sub>k</sub> &ndash; &lambda;)<sup>n<sub>k</sub></sup> .
</td></tr></table>

<p>Thus the zeros of P(&lambda;) are the latent roots
of J and each root occurs with the same multiplicity
in P(&lambda;) as in J.
This also holds true for a general matrix A
since similar matrices have the same characteristic polynomial,
as will now be demonstrated.</p>

<p>Suppose A and B are similar matrices, then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
A = S<sup>-1</sup>BS
</td></tr></table>

<p>and since</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>|<sup>&nbsp;</sup>S<sup>-1</sup><sup>&nbsp;</sup>| |<sup>&nbsp;</sup>S<sup>&nbsp;</sup>| 
= |<sup>&nbsp;</sup>S<sup>-1</sup>S<sup>&nbsp;</sup>| 
= |<sup>&nbsp;</sup>I<sup>&nbsp;</sup>| = 1,</td></tr>
<tr><td>then</td></tr>
<tr><td>&nbsp;</td><td>
|<sup>&nbsp;</sup>B &ndash; &lambda;I<sup>&nbsp;</sup>| 
= |<sup>&nbsp;</sup>S<sup>-1</sup><sup>&nbsp;</sup>| 
|<sup>&nbsp;</sup>B &ndash; &lambda;I<sup>&nbsp;</sup>| |<sup>&nbsp;</sup>S<sup>&nbsp;</sup>|
= |<sup>&nbsp;</sup>S<sup>-1</sup>BS &ndash; &lambda;S<sup>-1</sup>IS<sup>&nbsp;</sup>|
= |<sup>&nbsp;</sup>A &ndash; &lambda;I<sup>&nbsp;</sup>|.
</td></tr>
</table>

<p>Clearly then the latent roots of a matrix may be obtained
in the following two steps:</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
Compute the coefficients of the characteristics polynomial
P(&lambda;); and
 </td></tr>
<tr><td valign=top>2.</td><td>
Solve the characteristic equation P(&lambda;) = 0.
 </td></tr>
</table>

<p>Since the degree of P(&lambda;) is the same 
as the order of the matrix
the numerical solution of the characteristic equation
may be very difficult.
The problem of solving polynomial equations is taken
up in Chapter 5.
We now proceed to consider two closely related models
for generating the characteristic polynomial 
of a matrix.</p>

<a name="4C1"></a>
<p><b>(1) The Method of Bingham</b>
<sup><acronym title="15. Hotelling, H., “Some new methods in matrix calculation”, Annals of Mathematical Statistics, 14 (1943).">15</acronym></p>

<p>Let</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P(&lambda;) = &lambda;<sup>n</sup> + c<sub>1</sub>&lambda;<sup>n-1</sup>
+ &#133; + c<sub>n</sub>
</td></tr></table>

<p>be a polynomial with zeros 
&lambda;<sub>1</sub>,&lambda;<sub>2</sub>,&#133;,&lambda;<sub>n</sub>.
Defining</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
s<sub>k</sub> = <img src="MSLDEimg/sigmain.bmp" align=middle>
&lambda;<sub>i</sub><sup>k</sup> ,
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp;
</tt>(4.4)
</td></tr></table>

<p>it is possible with the help of Newton&rsquo;s identities
(<acronym title="Dickson, L.E., Theory of Equations, John Wiley and Sons, Inc., New York.">reference 
16</acronym>, p. 147)
to express the coefficients of P(&lambda;) in terms
of the quantities s<sub>k</sub>,
k = 1,2,&#133;,n as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>c<sub>1</sub> = -s<sub>1</td></tr>
<tr><td>&nbsp;</td><td>c<sub>2</sub> = -<sup>1</sup>/<sub>2</sub>
 (c<sub>1</sub>s<sub>1</sub> + s<sub>2</sub>)</td></tr>
<tr><td>&nbsp;</td><td>c<sub>2</sub> = -<sup>1</sup>/<sub>3</sub>
 (c<sub>2</sub>s<sub>1</sub> + c<sub>1</sub>s<sub>2</sub> + s<sub>3</sub>)</td></tr>
<tr><td colspan=2>and in general <sub>&nbsp;</sub><sup>&nbsp;</sup></td></tr>
<tr><td>&nbsp;</td><td>c<sub>k</sub> = -<sup>1</sup>/<sub>k</sub>
 (c<sub>k-1</sub>s<sub>1</sub> + c<sub>k-2</sub>s<sub>2</sub> + &#133; + s<sub>k</sub>) .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(4.5)
</td></tr></table>

<p>The quantities s<sub>k</sub> corresponding 
to the characteristic polynomial
of a canonical matrix J are easily obtained,
for clearly the sum of the diagonal elements
of J is equal to s<sub>1</sub>.
Furthermore, Eqs. (3.18) and (3.19) show that
the sum of the diagonal elements of J<sup>k</sup>
is equal to s<sub>k</sub>.</p>

<p>The sum of the diagonal elements of a matrix A
is usually called the trace of A or more briefly
Tr&nbsp;A.  Using this term we may write</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Tr J<sup>k</sup> = s<sub>k</sub> .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(4.6)
</td></tr></table>

<p>Moreover, the trace is invariant 
under a similar transformation for if</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
B = S<sup>-1</sup>AS
</td></tr></table>

<p>then in terms of components</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>ij</sub> &nbsp;&nbsp; =
<img src="MSLDEimg/sigmak.bmp" align=middle>
<img src="MSLDEimg/sigmam.bmp" align=middle>
s<sup>-1</sup><sub>ik</sub>a<sub>km</sub>s<sub>mj</sub></td></tr>
<tr><td>and<sup>&nbsp;</sup><sub>&nbsp;</sub></td></tr>
<tr><td>&nbsp;</td><td>
Tr B = <img src="MSLDEimg/sigmai.bmp" align=middle> b<sub>ii</sub> = 
<img src="MSLDEimg/sigmai.bmp" align=middle>
<img src="MSLDEimg/sigmak.bmp" align=middle>
<img src="MSLDEimg/sigmam.bmp" align=middle>
s<sup>-1</sup><sub>ik</sub>a<sub>km</sub>s<sub>mi</sub></td></tr>
<tr><td><img src="MSLDEimg/1x4w.bmp"></td></tr>
<tr><td>&nbsp;</td><td>
&nbsp; &nbsp; &nbsp; &nbsp; = 
<img src="MSLDEimg/sigmak.bmp" align=middle>
<img src="MSLDEimg/sigmam.bmp" align=middle>
a<sub>km</sub>
(<img src="MSLDEimg/sigmai.bmp" align=middle>s<sub>mi</sub>s<sup>-1</sup><sub>ik</sub>)
</td></tr>
<tr><td><img src="MSLDEimg/1x4w.bmp"></td></tr>
<tr><td>&nbsp;</td><td>
&nbsp; &nbsp; &nbsp; &nbsp; = 
<img src="MSLDEimg/sigmak.bmp" align=middle>
<img src="MSLDEimg/sigmam.bmp" align=middle>
a<sub>km</sub>&delta;<sub>mk</sub> =
<img src="MSLDEimg/sigmak.bmp" align=middle>
a<sub>kk</sub> = Tr A.
</td></tr></table>

<p>Consequently if (4.6) is true for the canonical form
of a matrix A it is also true for A. Hence</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Tr A<sup>k</sup> = s<sub>k</sub>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(4.7)
</td></tr></table>

<p>To summarize, the coefficients of the characteristic polynomial
of the matrix A may be obtained in the following steps:</p>

<table>
<tr><td valign=top>1. <sup>&nbsp;</sup></td><td>
Compute s<sub>k</sub> = Tr A<sup>k</sup>, k = 1,2,&#133;,n;
 </td></tr>
<tr><td valign=top>2.</td><td>
Compute the coefficients c<sub>i</sub> from (4.5).
 </td></tr>
</table>

<a name="4C2"></a>
<p><b>(2) The Method of Frame</b>
<sup><acronym title="17. Fettis, D.H.E., “A method for obtaining the characteristic equation of a matrix and computing the associated modal columns”, Quart. Jour. Appl. Math. 8, 206-212 (1950).">17</acronym></p>

<p><b>The Latent Roots</b>.
Let c<sub>0</sub>,c<sub>1</sub>,&#133;,c<sub>n</sub>
be the coefficients of the characteristic polynomial 
of the matrix A and let F<sub>j</sub>(A) be a sequence
of polynomial functions of A defined by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
F<sub>0</sub>(A) = I,</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
<td rowspan=2>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
</tt>(4.8)
 </td></tr>
<tr><td>&nbsp;</td>
<td nowrap>F<sub>k</sub>(A) = AF<sub>k-1</sub>(A) + c<sub>k</sub>I,
&nbsp; &nbsp; k = 1,2,&#133;,n. &nbsp;
 </td></tr>
</table>

<p>Now</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>Tr[AF<sub>0</sub>(A)] = s<sub>1</sub></td></tr>
<tr><td>&nbsp;</td>
 <td>Tr[AF<sub>1</sub>(A)] = s<sub>2</sub> + c<sub>1</sub>s<sub>1</sub></td></tr>
<tr><td>&nbsp;</td>
 <td>Tr[AF<sub>2</sub>(A)] = s<sub>3</sub> + c<sub>1</sub>s<sub>2</sub> +
 c<sub>2</sub>s<sub>1</sub>
 </td></tr>
</table>

<p>and in general</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Tr[AF<sub>k-1</sub>(A)]</td><td>= s<sub>k</sub> + c<sub>1</sub>s<sub>k-1</sub> + ... +
 c<sub>k-1</sub>s<sub>1</sub>
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>= -kc<sub>k</sub></td></tr>
</table>

<p>by virtue of (4.5).</p>

<table><tr><td width=28 nowrap>&there4;</td><td>
c<sub>k</sub> = -<sup>1</sup>/<sub>k</sub> Tr[AF<sub>k-1</sub>(A)].
</td></tr></table>

<p>Using this expression for c<sub>k</sub>, Eqs. (4.8) may be written</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
F<sub>0</sub>(A) = I,</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
<td rowspan=2>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(4.9)
 </td></tr>
<tr><td>&nbsp;</td>
<td nowrap>F<sub>k</sub>(A) = AF<sub>k-1</sub>(A) &ndash; 
<sup>1</sup>/<sub>k</sub> Tr[AF<sub>k-1</sub>(A)] I. &nbsp;
 </td></tr>
</table>

<p>Equations (4.9) define the sequence of operations
in the Frame method.</p>

<p>The Bingham process computes the sequence of matrices
A,A<sup>2</sup>,A<sup>3</sup>,&#133;,A<sup>n</sup>
and from the traces
s<sub>1</sub>,s<sub>2</sub>,&#133;,s<sub>n</sub>
of these matrices computes the coefficients by the use
of Newton&rsquo;s identities.
The Frame method also uses repeated multiplication
by A but at each step modifies the matrix so as
to derive the coefficients directly without explicit use
of Newton&rsquo;s identities.
The Frame method has the further advantage that</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
F<sub>n</sub>(A) = P(A) = 0.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp;&nbsp; 
</tt>(4.10)
</td></tr></table>

<p>This relation serves as a useful overall check 
on the accuracy of the computation.</p>

<p><b>The Principal Vectors</b>.
The adjoint of (A &ndash; &lambda;I) may be expressed
as a polynomial in &lambda; with coefficients formed
from the sequence of matrices (4.8) as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&Delta;(A &ndash; &lambda;I) = &ndash; 
{&lambda;<sup>n-1</sup>F<sub>0</sub>(A) + 
&lambda;<sup>n-2</sup>F<sub>1</sub>(A) + &#133; +
F<sub>n-1</sub>(A)}.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(4.11)
</td></tr></table>

&nbsp;

<table><tr><td width=28 nowrap>For</td></tr>
<tr><td>&nbsp;</td><td>
(A &ndash; &lambda;I) &Delta;(A &ndash; &lambda;I) = 
&lambda;<sup>n</sup>I + 
&lambda;<sup>n-1</sup>(F<sub>1</sub>&ndash;AF<sub>0</sub>) + 
&#133; +
&lambda;(F<sub>n-1</sub>&ndash;AF<sub>n-2</sub>) &ndash;
AF<sub>n-1</sub>.
 </td></tr>
<tr><td>But</td></tr>
<tr><td>&nbsp;</td><td>
F<sub>k</sub> &ndash; AF<sub>k-1</sub> = c<sub>k</sub>I, &nbsp; &nbsp; 
k = 1,2,&#133;,n,
 </td></tr>
</table>

<p>by (4.8) and since F<sub>n</sub> = 0 then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(A &ndash; &lambda;I) &Delta;(A &ndash; &lambda;I)</td>
<td>= 
[&lambda;<sup>n</sup> + c<sub>1</sub>&lambda;<sup>n-1</sup> + &#133; + c<sub>n</sub>] I
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= |<sup>&nbsp;</sup>A &ndash; &lambda;I<sup>&nbsp;</sup>| I
 </td></tr>
</table>

<p>by the definition of the characteristic equation.
Thus &Delta;(A &ndash; &lambda;I) as given by (4.11)
is indeed the adjoint of <nobr>(A &ndash; &lambda;I).</nobr>
When evaluated at a value of &lambda; equal to a latent root
the columns are proportional to the eigenvectors
associated with that root.
Furthermore since &lambda; is general in the expression (4.11)
the derived adjoints may be obtained by differentiation.
The derived adjoints furnish all the principal vectors
as demonstrated in section 3C.</p>

<a name="4D"></a>
<p><b>D. Error Analysis</b></p>

<p>The power method is an iterative process and 
error accumulation is therefore not a problem.
In the use of the Frame method however the
error accumulation may be expected to be rather rapid
and a bound on the error will now be established.</p>

<p>Assume that A is non-defective and consider the series
of matrices F<sub>k</sub>(A) defined by (4.8).
Because of round-off error the computed value
<img src="MSLDEimg/Fobar.bmp"><sub>k</sub>(A) 
will differ from F<sub>k</sub>(A)
and we may write</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/Fobar.bmp"><sub>k</sub>(A) &ndash;
F<sub>k</sub>(A) = E<sub>k</sub>.
</td></tr></table>

<p>The propagation of the error introduced at the first step
will be investigated first.
Let the length of the longest column vector
of E<sub>k</sub> be denoted by &epsilon;<sub>k</sub>. Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td><img src="MSLDEimg/Fobar.bmp"><sub>k+1</sub>(A) &ndash;
F<sub>k+1</sub>(A)</td>
<td>= A[<img src="MSLDEimg/Fobar.bmp"><sub>k</sub>(A) &ndash; F<sub>k</sub>(A)]
&ndash; <sup>1</sup>/<sub>k</sub> 
Tr{A[<img src="MSLDEimg/Fobar.bmp"><sub>k</sub>(A) &ndash; F<sub>k</sub>(A)]}I
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>
= AE<sub>k</sub> &ndash; <sup>1</sup>/<sub>k</sub>
Tr(AE<sub>k</sub>)I.
 </td></tr>
</table>

<p>Since for any vector x,</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
|<sup>&nbsp;</sup>Ax<sup>&nbsp;</sup>| &le; 
|<sup>&nbsp;</sup>&lambda;<sup>&nbsp;</sup>|
|<sup>&nbsp;</sup>x<sup>&nbsp;</sup>|
</td></tr></table>

<p>where &lambda; is the dominant root of A, then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&epsilon;<sub>k+1</sub></td>
<td><table><tr><td>&le; |<sup> </sup>&lambda;<sup> </sup>|&epsilon;<sub>k</sub>
+ <sup>n</sup>/<sub>k</sub> |<sup> </sup>&lambda;<sup> </sup>|&epsilon;<sub>k</sub>
 </td></tr></table></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td><table><tr><td>= |<sup> </sup>&lambda;<sub> </sub>|</td>
 <td><table cellspacing=0 cellpadding=0>
   <tr><td align=center><font size=-1>n+k</font></td></tr>
   <tr><td><img src="MSLDEimg/22x1.bmp"></td></tr>
   <tr><td align=center><font size=-1>k</font></td></tr>
 </table></td>
 <td>&epsilon;<sub>k</sub></td>
 </tr></table></td>
 </tr>
<tr><td>&there4;</td>
<td>&epsilon;<sub>n-1</sub></td>
<td><table><tr>
  <td>&le; |<sup> </sup>&lambda;<sup> </sup>|<sup>n-1</sup></td>
  <td><table cellspacing=0 cellpadding=0>
   <tr><td align=center><font size=-1>(2n-1)!<sub>&nbsp;</sub></font></td></tr>
   <tr><td><img src="MSLDEimg/42x1.bmp"></td></tr>
   <tr><td align=center><font size=-1>[(n-1)!]<sup>2</sup></font></td></tr>
  </table></td>
 <td>&epsilon;<sub>0</sub><sup> </sup>.</td>
</tr></table></td></tr>
</table>

<p>The error computed above is due to the round-off
committed at the first step.
Since a similar round-off occurs at each step the total
error bound &epsilon; will be a sum of such terms as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&epsilon; &lt; &epsilon;<sub>0</sub>
<img src="MSLDEimg/sigmakn.bmp" align=middle></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=middle><font size=-1>[2(n&ndash;k)+1]!<sub>&nbsp;</sub></font></td></tr>
 <tr><td><img src="MSLDEimg/60x1.bmp"></td></tr>
 <tr><td align=middle><font size=-1>[(n&ndash;k)!]<sup>2</sup></font></td></tr>
</table></td>
<td>|<sub> </sub>&lambda;<sub> </sub>|<sup>n-k</sup> .<sub>&nbsp;</sub></td>
</tr></table>

<p>For large values of n the error accumulation
may therefore be very severe but in work with matrices
the maximum error bounds are usually found to be
too pessimistic.</p>

<a name="4E"></a>
<p><b>E. Improving an Approximate Solution</b></p>

<p>Let A be a non-defective matrix and let S
be the matrix which diagonalizes A.
If S<sub>0</sub> is an approximation to S
then the method of 
Jahn<sup><acronym title="18. Jahn, H.A., “Improvement of an approximate set of latent roots and modal columns by methods akin to those of classical perturbation theory”, Quarterly Journal of Mechanics and Applied Mathematics, Vol. 1 (1948).">18</acronym></sup>
may be used to improve this approximation.
The object is to compute S where
AS = S&Lambda; and &Lambda; is a diagonal matrix.
Since S<sub>0</sub> &#x2250; S we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
S<sub>0</sub><sup>-1</sup>AS<sub>0</sub> = &Lambda;<sub>1</sub> + B
</td></tr></table>

<p>where &Lambda;<sub>1</sub> is diagonal and B is a matrix
whose elements are small;
in particular the diagonal elements are zero.
Let the diagonal elements of &Lambda;<sub>1</sub> be
&lambda;<sub>1</sub>,&lambda;<sub>2</sub>,&#133;,&lambda;<sub>n</sub>
and let <nobr>C = [c<sub>rs</sub>]</nobr> where</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<td>c<sub>rs</sub> =</td>
<td><table>
 <tr><td align=center>b<sub>rs</sub></td></tr>
 <tr><td><img src="MSLDEimg/50x1.bmp"></td></tr>
 <tr><td align=center>&lambda;<sub>r</sub> &ndash; &lambda;<sub>s</sub></td></tr>
</table></td>
<td>.</td>
</tr></table>

<p>Then S<sub>1</sub> = S<sub>0</sub> (I + C) is an improved
approximation to S correct to second order.
For clearly <nobr>B = C&Lambda;<sub>1</sub> &ndash; &Lambda;<sub>1</sub>C</nobr><p>

<table><tr><td width=28 nowrap>&there4;</td><td>
S<sub>0</sub><sup>-1</sup>AS<sub>0</sub> = 
&Lambda;<sub>1</sub> + C&Lambda;<sub>1</sub> &ndash; &Lambda;<sub>1</sub>C
&nbsp; &nbsp; and &nbsp; &nbsp; AS<sub>1</sub> = AS<sub>0</sub> (I + C).
</td></tr></table>

<p>Hence</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>AS<sub>0</sub></td>
<td>= S<sub>0</sub> (&Lambda;<sub>1</sub> + 
C&Lambda;<sub>1</sub> &ndash; &Lambda;<sub>1</sub>C)</td>
 </tr>
<tr><td>&nbsp;</td></tr>
<tr><td>&there4;</td>
<td>AS<sub>1</sub></td>
<td>= S<sub>0</sub> (&Lambda;<sub>1</sub> + 
C&Lambda;<sub>1</sub> &ndash; &Lambda;<sub>1</sub>C) (I + C)</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= S<sub>0</sub> (&Lambda;<sub>1</sub> + 
C&Lambda;<sub>1</sub> &ndash; &Lambda;<sub>1</sub>C +
&Lambda;<sub>1</sub>C + 
C&Lambda;<sub>1</sub>C &ndash; &Lambda;<sub>1</sub>C<sup>2</sup>)</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= S<sub>0</sub> (I + C) &Lambda;<sub>1</sub> + 
S<sub>0</sub> (C&Lambda;<sub>1</sub> &ndash; &Lambda;<sub>1</sub>C) C</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= S<sub>1</sub>&Lambda;<sub>1</sub>
&nbsp; plus terms of second order in C.</td>
 </tr>
</table>

<a name="4F"></a>
<p><b>F. Comparison of Methods</b></p>

<p>The amount of computation required to generate
the characteristic equation by either the Frame
or Bingham method is of the order n<sup>4</sup> multiplications.
In addition the solution of the equation so obtained
will require considerable labor.
Consequently if only a few of the roots are wanted,
use of the power method is indicated.
If a complete solution is desired,
the power method has serious shortcomings.
Firstly, the process is not uniform 
since special modifications are required to care
for special cases such as complex or multiple roots.
Furthermore in the case of a defective matrix
it fails to obtain the principal vectors
of order &gt; 1.</p>

<p>The attractive features of the Frame method
are its complete generality and uniformity.
Unlike the power method the process proceeds
in a purely routine manner independent of the type
of roots encountered.
The completion of the method of course assumes
the solution of a polynomial equation of degree
equal to the order of the matrix
and some of the essential difficulties
may re-appear in this phase of the problem.
In any case the solution of polynomial equations
is an important problem in its own right
and the development of suitable methods
is worth considerable effort.
It therefore seems reasonable to base
the solution of the problem of latent roots
on the assumption of the solvability 
of the characteristic equation.</p>

<p>If the round-off error in the Frame method
should be so severe that the desired accuracy
is not attained,
the method of Jahn may be used 
to improve the accuracy.
Each application of the Jahn method requires
the equivalent of about four matrix multiplications.</p>

<a name="notes4"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top><a name="note4a"></a>a. &nbsp;</td><td>
The term of the general solution corresponding to a pair
of complex roots is periodic.
 </td></tr>
<tr><td valign=top><a name="note4b"></a>b.</td><td>
In an automatic computer the numbers must be kept 
within machine capacity.
 </td></tr>
<tr><td valign=top><a name="note4c"></a>c.</td><td>
Since any scalar multiple of an eigenvector
is also an eigenvector,
the term &ldquo;largest eigenvector&rdquo;
makes no sense except as an elliptical expression
for &ldquo;the eigenvector corresponding to the largest root&rdquo;.
 </td></tr>
<tr><td valign=top><a name="note4d"></a>d.</td><td>
The assumption that &lambda;<sub>1</sub> occupies
the first position is not necessary to the argument.
If &lambda;<sub>1</sub> is in the rth position
the associated eigenvector of J becomes e<sub>r</sub>.
 </td></tr>
<tr><td valign=top><a name="note4e"></a>e.</td><td>
The case of a pair of complex roots may be handled
by a slight modification of the method discussed above.
See <acronym title="12. Aitken, A.C., “Studies in Practical Mathematics II. The evaluation of the latent roots and latent vectors of a matrix”, Proc. Roy. Soc., Edinburgh 57, 269-304 (1937).">reference 
12</acronym>;
or <acronym title="13. Zurmuhl, R., Matrizen, Springer-Verlag, Berlin, 1950.">reference 
13</acronym>, page 299.
 </td></tr></table>
<br>&nbsp;<br>





<a name="5"></a>
<p><b>5. &nbsp; Zeros of a Polynomial</b></p>

<a name="5A"></a>
<p><b>A. Methods Depending on an Initial Approximation</b></p>

<p>The problem of locating the zeros of a polynomial of high degree
is greatly simplified when good initial approximations to the zeros
are known.
This situation is often met in practice because 
of the frequent occurrence of families of polynomial equations.
If, for example, the solution of a polynomial equation
is required within some larger iterative process,
the coefficients may be expected to vary gradually
from one step to the next and the set of roots obtained
at one step may be expected to provide good approximations
to the roots of the next equation.
Under these conditions, the Birge-Vieta<sup><acronym 
title="19. Marchant Methods, MM-225, Marchant Calculating Machine Co., Oakland, California.">19</acronym></sup>
method and the quadratic factor method are probably the best,
since they converge quadratically near a root.
Before proceeding to a discussion of these methods
certain useful processes will be developed.</p>

<a name="5A1"></a>
<p><b>1. Synthetic Division</b><sup><acronym title="16. Dickson, L.E., Theory of Equations, John Wiley and Sons, Inc., New York.">16
</acronym></sup></p>

<p>Let P<sub>n</sub>(x) = a<sub>0</sub>x<sup>n</sup> 
+ a<sub>1</sub>x<sup>n-1</sup> + &#133; +
a<sub>n-1</sub>x + a<sub>n</sub> be a polynomial of degree n
with real coefficients. Let</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Q(x) = b<sub>0</sub>x<sup>n-1</sup> 
+ b<sub>1</sub>x<sup>n-2</sup> +
&#133; + b<sub>n-1</sub>
</td></tr></table>

<p>where the coefficients b<sub>i</sub> are defined by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>0</sub> = a<sub>0</sub> ,</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
<td rowspan=2><tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.1)</td>
 </tr>
<tr><td>&nbsp;</td><td nowrap>
b<sub>i</sub> = a<sub>i</sub> + pb<sub>i-1</sub> , &nbsp; &nbsp; &nbsp;
i = 1,2,&#133;,n. &nbsp;</td>
 </tr>
</table>

<p>Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(x&ndash;p) Q(x) + b<sub>n</sub><sup>&nbsp;</sup></td>
<td>= (x&ndash;p) [b<sub>0</sub>x<sup>n-1</sup> + 
(a<sub>1</sub>+pb<sub>0</sub>)x<sup>n-2</sup>
+ &#133; + (a<sub>n-1</sub>+pb<sub>n-2</sub>)] + </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
a<sub>n</sub>+pb<sub>n-1</sub>
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>
= a<sub>0</sub>x<sup>n</sup> + a<sub>1</sub>x<sup>n-1</sup> 
+ &#133; + a<sub>n</sub> .
 </td></tr>
</table>

<p>Therefore</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P<sub>n</sub>(x) = (x&ndash;p) Q(x) + b<sub>n</sub>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.2)
 </td></tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td><td>
P<sub>n</sub>(p) = b<sub>n</sub> .
 </td></tr>
</table>

<p>Hence b<sub>n</sub> is the value of the polynomial
evaluated at p and when b<sub>n</sub> is zero,
Q(x) is the so-called reduced equation.
The effect of the algorithm (5.1) is to divide
the polynomial P(x) by the linear factor (x&ndash;p)
to obtain the quotient polynomial Q(x) and the remainder
b<sub>n</sub>.
The algorithm is therefore called synthetic division.
Adopting the convention that all coefficients 
with negative subscripts are zero, Eq. (5.1) may be
written more concisely as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>i</sub> = a<sub>i</sub> + pb<sub>i-1</sub>, 
&nbsp; &nbsp; i = 0,1,2,&#133;,n.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(5.3)
</td></tr></table>

<p>Synthetic division by factors of any degree is easily
defined but the case of a quadratic factor is of particular interest.
In this case let</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>i</sub> = a<sub>i</sub> + pb<sub>i-1</sub> + qb<sub>i-2</sub>, 
&nbsp; &nbsp; i = 0,1,2,&#133;,n&ndash;1, &nbsp; &nbsp;</td>
<td rowspan=2><img src="MSLDEimg/matrixr2.bmp"></td>
<td rowspan=2>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.4)</td>
 </tr>
<tr><td>&nbsp;</td><td>
b<sub>n</sub> = a<sub>n</sub> + qb<sub>n-2</sub>.</td>
 </tr>
</table>

<p>Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(x<sup>2</sup> &ndash; px &ndash; q) Q(x) + b<sub>n-1</sub>x +  b<sub>n</sub> =
P<sub>n</sub>(x)
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp;
</tt>(5.5)
</td></tr></table>

<p>as may be verified by expanding the left-hand side.</p>

<a name="5A2"></a>
<p><b>2. Change of Origin</b></p>

<p>A translation or change of origin to the point
x = p is defined by the substitution <nobr>y = x &ndash; p.</nobr>
The change in the coefficients of P<sub>n</sub>(x)
induced by such a translation may be derived from the relation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P<sub>n</sub>(x)</td><td>= a<sub>0</sub>(y+p)<sup>n</sup> + 
a<sub>1</sub>(y+p)<sup>n-1</sup> + &#133; + a<sub>n</sub>
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>
= b<sub>0</sub>y<sup>n</sup> + 
b<sub>1</sub>y<sup>n-1</sup> + &#133; + b<sub>n</sub>.
 </td></tr>
</table>

<p>The coefficients b<sub>k</sub> are most conveniently computed
by the process known as Horner&rsquo;s method.<sup><acronym 
title="16. Dickson, L.E., Theory of Equations, John Wiley and Sons, Inc., New York.">16</acronym></sup>
This method is nothing more than a repeated application
of synthetic division to the successive quotient polynomials.
For n = 4 the scheme of computation is as follows:</p>

<table cellspacing=0 cellpadding=0><tr><td width=28 nowrap>&nbsp;</td>
<td align=right>&nbsp; a<sub>0</sub> &nbsp;</td><td></td>
<td align=right>a<sub>1</sub> &nbsp;</td><td></td>
<td align=right>a<sub>2</sub> &nbsp;</td><td></td>
<td align=right>a<sub>3</sub> &nbsp;</td><td></td>
<td align=right>a<sub>4</sub> &nbsp;</td>
<td align=right>│</td>
<td>&nbsp; &nbsp;</td>
<td>│<sub>&nbsp;</sub> p</td></tr>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td></td>
<td align=right>&nbsp; pb<sub>0</sub> &nbsp;</td><td></td>
<td align=right>&nbsp; pb<sub>1</sub> &nbsp;</td><td></td>
<td align=right>&nbsp; pb<sub>2</sub> &nbsp;</td><td></td>
<td align=right>&nbsp; pb<sub>3</sub> &nbsp;</td>
<td align=right>│</td>
<td>&nbsp;</td>
<td>└──</td>
 </tr>
<tr><td></td><td colspan=10 align=right>──────────────────────┘</td></tr>

<tr><td>&nbsp;</td>
<td align=right>&nbsp; b<sub>0</sub> &nbsp;</td><td></td>
<td align=right>b<sub>1</sub> &nbsp;</td><td></td>
<td align=right>b<sub>2</sub> &nbsp;</td><td></td>
<td align=right>b<sub>3</sub> &nbsp;</td>
<td align=right>│</td>
<td align=right>b<sub>4</sub> &nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td></td>
<td align=right>&nbsp; pc<sub>0</sub> &nbsp;</td><td></td>
<td align=right>&nbsp; pc<sub>1</sub> &nbsp;</td><td></td>
<td align=right>&nbsp; pc<sub>2</sub> &nbsp;</td>
<td align=right>│</td>
 </tr>
<tr><td></td><td colspan=8 align=right>─────────────────┘</td></tr>

<tr><td>&nbsp;</td>
<td align=right>&nbsp; c<sub>0</sub> &nbsp;</td><td></td>
<td align=right>c<sub>1</sub> &nbsp;</td><td></td>
<td align=right>c<sub>2</sub> &nbsp;</td>
<td align=right>│</td>
<td align=right>c<sub>3</sub> &nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td></td>
<td align=right>&nbsp; pd<sub>0</sub> &nbsp;</td><td></td>
<td align=right>&nbsp; pd<sub>1</sub> &nbsp;</td>
<td align=right>│</td>
 </tr>
<tr><td></td><td colspan=6 align=right>────────────┘</td></tr>

<tr><td>&nbsp;</td>
<td align=right>&nbsp; d<sub>0</sub> &nbsp;</td><td></td>
<td align=right>d<sub>1</sub> &nbsp;</td>
<td align=right>│</td>
<td align=right>d<sub>2</sub> &nbsp;</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td></td>
<td align=right>&nbsp; pe<sub>0</sub> &nbsp;</td>
<td align=right>│</td>
 </tr>
<tr><td></td><td colspan=4 align=right>───────┘</td></tr>

<tr><td>&nbsp;</td>
<td align=right>&nbsp; e<sub>0</sub> &nbsp;</td>
<td align=right>│</td>
<td align=right>e<sub>1</sub> &nbsp;</td>
 </tr>
<tr><td></td><td colspan=2 align=right>───┘</td></tr>
<tr><td>&nbsp;</td>
<td align=right>&nbsp; f<sub>0</sub> &nbsp;</td>
 </tr>
</table>

<p>The polynomial in y is then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
f<sub>0</sub>y<sup>4</sup> + e<sub>1</sub>y<sup>3</sup> 
+ d<sub>2</sub>y<sup>2</sup> + c<sub>3</sub>y + b<sub>4</sub>.
</td></tr></table>

<p>To prove that the polynomial in y may be so obtained
let the successive quotient polynomials Q<sub>k</sub>(x)
be defined by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P<sub>n</sub>(x)</td>
<td>= (x&ndash;p)Q<sub>n-1</sub>(x) + b<sub>n</sub>,</td>
 </tr>
<tr><td>&nbsp;</td><td>
Q<sub>n-k</sub>(x)</td>
<td>= (x&ndash;p)Q<sub>n-k-1</sub>(x) + b<sub>n-k</sub>,
&nbsp; &nbsp; k = 1,2,&#133;,n.</td>
 </tr>
</table>

<p>Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td nowrap>
P<sub>n</sub>(x)</td>
<td>= (x&ndash;p)[(x&ndash;p)Q<sub>n-2</sub> + b<sub>n-1</sub>] + b<sub>n</sub></td>
<td rowspan=4>&nbsp; <img src="MSLDEimg/matrixr4.bmp"></td>
<td rowspan=4><tt>
&nbsp; &nbsp; &nbsp;
</tt>(5.6)</td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= (x&ndash;p){(x&ndash;p)[((x&ndash;p)Q<sub>n-3</sub> + b<sub>n-2</sub>] 
+ b<sub>n-1</sub>} + b<sub>n</sub></td>
 </tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= &#133; = (x&ndash;p)<sup>n</sup>b<sub>0</sub> + 
(x&ndash;p)<sup>n-1</sup>b<sub>1</sub> + &#133; +
(x&ndash;p)b<sub>n-1</sub> + b<sub>n</sub>
 </td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td>
<td>= b<sub>0</sub>y<sup>n</sup> + b<sub>1</sub>y<sup>n-1</sup> + &#133; + b<sub>n.</sub>
 </td></tr>
</table>

<p>Repeating the differentiation of the relation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P<sub>n</sub>(x) = b<sub>0</sub>(x&ndash;p)<sup>n</sup> + 
b<sub>1</sub>(x&ndash;p)<sup>n-1</sup> + &#133; +
b<sub>n-1</sub>(x&ndash;p) + b<sub>n</sub>
</td></tr></table>

<p>yields the values of the successive derivatives of
P<sub>n</sub>(x) evaluated at the point p. Thus</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td><table cellspacing=0 cellpadding=0><tr>
 <td align=center><i>d</i><sup>k</sup>P<sub>n</sub>(x)<i></td>
 <td rowspan=3>&nbsp;<img src="MSLDEimg/1x53.bmp"></td>
 <td rowspan=3 valign=bottom>&nbsp;p</td></tr>
<tr><td><img src="MSLDEimg/50x1.bmp"></td></tr>
<tr><td align=center><i>d</i>x<sup>k</sup></td></tr>
</tr></table></td>
<td>= k!b<sub>n-k</sub>, &nbsp; &nbsp; k = 0,1,2,&#133;,n.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.7)
</td></tr></table>

<a name="5A3"></a>
<p><b>3. The Birge-Vieta Process</b></p>

<p>The so-called Birge-Vieta process for locating a real root
of a polynomial was first proposed by Vieta in 1600
and was discarded by later mathematicians as requiring
too much computation.
It was not until the advent of the modern desk calculator
that the method was re-established by Birge
and came into wide use.
The method is perhaps the most satisfactory known
for obtaining real roots with a desk calculator.</p>

<p>The Birge-Vieta method is based on the well-known 
Newton-Raphson process (<acronym 
title="20. Whittaker, E., and Robinson, G., The Calculation of Observations, 4th Ed., Blackie and Sons, Ltd., 1946.">reference 20</acronym>,
page 84)
which is defined as follows: 
Let <img src="MSLDEimg/xbar.bmp">
be a real roots of the equation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
f(x) = 0.
</td></tr></table>

<p>Then for any value x<sub>0</sub> sufficiently close to
<img src="MSLDEimg/xbar.bmp"> the quantities x<sub>i</sub>
defined by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>i+1</sub> = x<sub>i</sub> &ndash;</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>f(x<sub>i</sub>)</td></tr>
 <tr><td><img src="MSLDEimg/35x1.bmp"></td></tr>
 <tr><td align=center>f '(x<sub>i</sub>)</td></tr>
 </table></td>
<td>, &nbsp; &nbsp; i = 0,1,2,&#133; ,
</td></tr></table>

<p>will converge to the root <img src="MSLDEimg/xbar.bmp">.
The convergence is quadratic; i.e., the difference 
<nobr>x<sub>i</sub> &ndash; <img src="MSLDEimg/xbar.bmp"></nobr>
is approximately squared at each step.
When the function f(x) is a polynomial 
the discussion of Horner&rsquo;s method and Eq. (5.7)
make it clear that two synthetic divisions
with p = x<sub>0</sub> will furnish the quantities</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>n</sub> = P<sub>n</sub>(x<sub>0</sub>) &nbsp; and &nbsp;
b<sub>n-1</sub> = P<sub>n</sub>'(x<sub>0</sub>) .
</td></tr></table>

<p>The process is then repeated using the quantity</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>1</sub> = x<sub>0</sub> &ndash;</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>b<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>b<sub>n-1</sub></td></tr>
 </table></td>
<td>,</td>
</tr></table>

<p>and in general</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>i+1</sub> = x<sub>i</sub> &ndash;</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>b<sub>n</sub>(x<sub>i</sub>)</td></tr>
 <tr><td><img src="MSLDEimg/50x1.bmp"></td></tr>
 <tr><td align=center>b<sub>n-1</sub>(x<sub>i</sub>)</td></tr>
 </table></td>
<td>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.8)
</td></tr></table>

<p>Equation (5.8) defines the Birge-Vieta process.
This process is usually employed only for real roots,
but may be extended to complex roots by simply allowing
the variables x<sub>0</sub>,x<sub>1</sub>,&#133;
to be complex.
Since a polynomial is an analytic function<sup><acronym 
title="21. Churchill, Ruel, V., Introduction to Complex Variables and Applications, McGraw-Hill, 1948.">21</acronym></sup>
the derivative P<sub>n</sub>'(x) exists for x complex,
and the process carries over without change 
to complex numbers.
(Note, however, that if the initial approximation 
x<sub>0</sub> is real,
the successive x<sub>i</sub> remain real.)
Every operation is now complex and hence 
the amount of computation is increased 
by a factor of about four.</p>

<p>The complex Birge-Vieta method can be applied
equally well to the solution of polynomial equations
with complex coefficients,
but for real polynomials the method to be described
below is generally considered to be superior.</p>

<a name="5A4"></a>
<p><b>4. The Quadratic Factor Method</b></p>

<p>Since the complex zeros of a real polynomial occur
in conjugate pairs it is possible to factor any real polynomial
into a set of real linear and quadratic factors.
Using a method similar to the Birge-Vieta process
it is possible to improve an approximate quadratic factor
by repeated synthetic divisions of the type 
described by Eq. (5.4).
The goal is to so adjust the coefficients p and q
as to make both remainders b<sub>n-1</sub>
and b<sub>n</sub> zero.
Treating b<sub>n-1</sub> and b<sub>n</sub>
as functions of p and q and expanding in 
Taylor&rsquo;s series we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>n</sub>(p+&delta;p, q+&delta;q)</td>
<td>= b<sub>n</sub></td><td>+</td>
<td align=center><table cellspacing=0 cellpadding=0>
 <tr><td>&part;b<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
</table></td>
<td>&delta;p + </td>
<td align=center><table cellspacing=0 cellpadding=0>
 <tr><td>&part;b<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
</table></td>
<td>&delta;q + &#133;</td>
 </tr>
<tr><td>&nbsp;</td><td>
b<sub>n-1</sub>(p+&delta;p, q+&delta;q)</td>
<td>= b<sub>n-1</sub></td><td>+</td>
<td align=center><table cellspacing=0 cellpadding=0>
 <tr><td>&part;b<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/35x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
</table></td>
<td>&delta;p + </td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td>&part;b<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/35x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
</table></td>
<td>&delta;q + &#133; .</td>
 </tr></table>

<p>A generalization of the Newton-Raphson process
to two variables then gives the desired
corrections &delta;p and &delta;q as solutions
of the pair of linear equations</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>b<sub>n</sub></td><td>+</td>
 <td align=center><table cellspacing=0 cellpadding=0>
  <tr><td align=center>&part;b<sub>n</sub></td></tr>
  <tr><td align=center><img src="MSLDEimg/28x1.bmp"></td></tr>
  <tr><td align=center>&part;p</td></tr>
  </table></td>
<td>&delta;p +</td>
 <td align=center><table cellspacing=0 cellpadding=0>
  <tr><td align=center>&part;b<sub>n</sub></td></tr>
  <tr><td align=center><img src="MSLDEimg/28x1.bmp"></td></tr>
  <tr><td align=center>&part;q</td></tr>
  </table></td>
<td>&delta;q = 0</td>
<td rowspan=2><img src="MSLDEimg/matrixr3.bmp"></td>
<td rowspan=2><tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp;
</tt>(5.9)</td>
 </tr>
<tr><td>&nbsp;</td>
<td>b<sub>n-1</sub></td><td>+</td>
 <td><table cellspacing=0 cellpadding=0>
  <tr><td align=center>&part;b<sub>n-1</sub></td></tr>
  <tr><td align=center><img src="MSLDEimg/35x1.bmp"></td></tr>
  <tr><td align=center>&part;p</td></tr>
  </table></td>
<td>&delta;p +</td>
 <td><table cellspacing=0 cellpadding=0>
  <tr><td align=center>&part;b<sub>n-1</sub></td></tr>
  <tr><td align=center><img src="MSLDEimg/35x1.bmp"></td></tr>
  <tr><td align=center>&part;q</td></tr>
  </table></td>
<td>&delta;q = 0. &nbsp; &nbsp;</td>
 </tr>
</table>

<p>The quadratic factor method as first proposed by
Bairstow<sup><acronym 
title="22. Bairstow, L., Applied Aerodynamics, Longman’s Green & Co., London, 1920.">22</acronym></sup>
and as presented in standard 
works<sup><acronym
title="23. Milne, W.E., Numerical Calculus, Princeton University Press, 1949.">23</acronym>,
<acronym
title="24. Hartree, D.R., Numerical Analysis, Oxford at the Clarendon Press, 1952.">24</acronym></sup>
on numerical analysis requires the use
of a second synthetic division to evaluate
the derivatives in (5.9) and
the application of the corrections
&delta;p and &delta;q so derived.
A modification of the method,
which provides a more uniform computational
scheme<sup><a href="MSLDE1.htm#note5a">[a]</a></sup>
and simplifies the extension to a higher order process,
will now be described.
The proof is also simplified.</p>

<p>We defined a modified algorithm for synthetic division,
similar to that given by (5.4), namely,</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
c<sub>i</sub> = a<sub>i</sub> + pc<sub>i-1</sub>
+ qc<sub>i-2</sub>, &nbsp; &nbsp;
i = 0,1,2,&#133;,n.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp;&nbsp;
</tt>(5.10)
</td></tr></table>

<p>This process is completely uniform, without exception
for the computation of c<sub>n</sub>. 
Comparing with (5.4)</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>c<sub>i</sub> = b<sub>i</sub>, 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
i = 0,1,2,&#133;,n-1.
 </td></tr>
<tr><td>&nbsp;</td>
<td>c<sub>n</sub> = b<sub>n</sub> + pc<sub>n-1</sub>, 
 </td></tr>
</table>

<p>letting</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
Q(x) = c<sub>0</sub>x<sup>n-2</sup> + &#133; + c<sub>n-2</sub>,
</td></tr></table>

<p>Eq (5.5) becomes</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
P<sub>n</sub>(x) = (x<sup>2</sup> &ndash; px &ndash; q)Q(x)
+ c<sub>n-1</sub>x + (c<sub>n</sub> &ndash; pc<sub>n-1</sub>).
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
</tt>(5.11)
</td></tr></table>

<p>The modification of the quadratic factor method
now consists in choosing &delta;p and &delta;q
so as to reduce to zero the quantities 
c<sub>n-1</sub> and c<sub>n</sub> rather than 
b<sub>n-1</sub> and b<sub>n</sub>.
Note that the vanishing of
c<sub>n-1</sub> and c<sub>n</sub>
is the necessary and sufficient condition
for the vanishing of
b<sub>n-1</sub> and b<sub>n</sub>.</p>

<p>Treating x, p and q as independent variables
and differentiating (5.11) partially with respect to
p we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
0 = (x<sup>2</sup> &ndash; px &ndash; q)</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;Q(x)</td></tr>
 <tr><td><img src="MSLDEimg/38x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>&ndash; xQ(x) + x</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>+</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>&ndash; p</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>&ndash; c<sub>n-1</sub></td>
</tr></table>

<table><tr><td width=28 nowrap>&there4;</td><td>
xQ(x) + c<sub>n-1</sub> = (x<sup>2</sup> &ndash; px &ndash; q)</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;Q(x)</td></tr>
 <tr><td><img src="MSLDEimg/38x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>+ x</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>+ (</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>&ndash; p</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>).</td>
</tr></table>

<p>Thus &part;c<sub>n-1</sub>/&part;p and &part;c<sub>n</sub>/&part;p 
are respectively the first and second
remainders on applying the modified algorithm
to the polynomial <nobr>xQ(x) + c<sub>n-1</sub>,</nobr>
i.e. to the coefficients c<sub>0</sub>&#133;c<sub>n-1</sub>.
Let the coefficients so obtained be
d<sub>0</sub>&#133;d<sub>n-1</sub>, then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>d<sub>n-2</sub> =</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>&nbsp; &nbsp; and &nbsp; &nbsp; d<sub>n-1</sub> =</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
<td>.<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(5.12)
</td></tr></table>

<p>Differentiating (5.11) with respect to q we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
0 = (x<sup>2</sup> &ndash; px &ndash; q)</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;Q(x)</td></tr>
 <tr><td><img src="MSLDEimg/38x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
 </table></td>
<td>&ndash; Q(x) + x</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
 </table></td>
<td>+ (</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
 </table></td>
<td>&ndash; p</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
 </table></td>
<td>)</td>
</tr></table>

<p>and the first and second remainders on
applying the modified algorithm to Q(x) are
&part;c<sub>n-1</sub>/&part;q and 
&part;c<sub>n</sub>/&part;q respectively. Hence</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>d<sub>n-3</sub> =</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n-1</sub></td></tr>
 <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
 </table></td>
<td>&nbsp; &nbsp; and &nbsp; &nbsp; d<sub>n-2</sub> =</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;c<sub>n</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>&part;q</td></tr>
 </table></td>
<td>.<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(5.13)
</td></tr></table>

<p>Applying Eq. (5.9) to the quantities
c<sub>n</sub> and c<sub>n-1</sub> and
substituting the expressions obtained above
for the derivatives we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>c<sub>n-1</sub><td><td>+ d<sub>n-2</sub>&delta;p + d<sub>n-3</sub>&delta;q = 0.
 </td></tr>
<tr><td>&nbsp;</td><td>
c<sub>n</sub><td><td>+ d<sub>n-1</sub>&delta;p + d<sub>n-2</sub>&delta;q = 0.
</td></tr></table>

<p>Thus:</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>&delta;p</td><td>=</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center nowrap>-(c<sub>n-1</sub>d<sub>n-2</sub> &ndash; c<sub>n</sub>d<sub>n-3</sub>)</td></tr>
 <tr><td><img src="MSLDEimg/120x1.bmp"></td></tr>
 <tr><td align=center>D</td></tr>
 </table></td>
<td>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</td>
<td rowspan=3><img src="MSLDEimg/matrixr5.bmp"></td>
<td rowspan=3><tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(5.14)</td>
 </tr>
<tr><td>&nbsp;</td>
<td>&delta;q</td><td>=</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>(c<sub>n-1</sub>d<sub>n-1</sub> &ndash; c<sub>n</sub>d<sub>n-2</sub>)</td></tr>
 <tr><td><img src="MSLDEimg/120x1.bmp"></td></tr>
 <tr><td align=center>D</td></tr>
 </table></td>
 </tr>
<tr><td>&nbsp;</td>
<td>D</td><td>=</td>
<td colspan=3>d<sub>n-2</sub><sup>2</sup> &ndash; 
 d<sub>n-1</sub>d<sub>n-3</sub>.</td></tr>
</table>

<p>The complete uniformity of the method is clearly
show in the following example for <nobr>n = 5.</nobr>
The initial approximations are p<sub>0</sub> and q<sub>0</sub>.
Coefficients with negative subscripts are 
by convention assumed to be zero.</p>

<table cellspacing=0 cellpadding=0><tr><td width=28 nowrap>&nbsp;</td>
<td align=center>a<sub>0</sub></td><td></td>
<td align=center>a<sub>1</sub></td><td></td>
<td align=center>a<sub>2</sub></td><td></td>
<td align=center>a<sub>3</sub></td><td></td>
<td align=center>a<sub>4</sub></td><td></td>
<td align=center>a<sub>5</sub></td>
<td align=right>│</td>
<td>&nbsp;&nbsp;</td>
<td>│</td>
<td>&nbsp;p<sub>0</sub>,q<sub>0</sub></td></tr>
 </tr>
<tr><td>&nbsp;</td>
<td align=center>p<sub>0</sub>c<sub>-1</sub></td><td></td>
<td align=center>p<sub>0</sub>c<sub>0</sub></td><td></td>
<td align=center>p<sub>0</sub>c<sub>1</sub></td><td></td>
<td align=center>p<sub>0</sub>c<sub>2</sub></td><td></td>
<td align=center>p<sub>0</sub>c<sub>3</sub></td><td></td>
<td align=center>p<sub>0</sub>c<sub>4</sub></td>
<td align=right>│</td>
<td>&nbsp;</td>
<td>└</td><td>───</td>
 </tr>
<tr><td>&nbsp;</td>
<td align=center>q<sub>0</sub>c<sub>-2</sub></td><td></td>
<td align=center>q<sub>0</sub>c<sub>-1</sub></td><td></td>
<td align=center>q<sub>0</sub>c<sub>0</sub></td><td></td>
<td align=center>q<sub>0</sub>c<sub>1</sub></td><td></td>
<td align=center>q<sub>0</sub>c<sub>2</sub></td><td></td>
<td align=center>q<sub>0</sub>c<sub>3</sub></td>
<td align=right>│</td>
 </tr>
<tr><td>&nbsp;</td><td colspan=12 align=right>───────────────────┘</td></tr>
<tr><td>&nbsp;</td>
<td align=center>c<sub>0</sub></td><td></td>
<td align=center>c<sub>1</sub></td><td></td>
<td align=center>c<sub>2</sub></td><td></td>
<td align=center>c<sub>3</sub></td><td></td>
<td align=center>c<sub>4</sub></td>
<td align=right>│</td>
<td align=center>c<sub>5</sub></td>
 </tr>
<tr><td>&nbsp;</td>
<td align=center>p<sub>0</sub>d<sub>-1</sub></td><td></td>
<td align=center>p<sub>0</sub>d<sub>0</sub></td><td></td>
<td align=center>p<sub>0</sub>d<sub>1</sub></td><td></td>
<td align=center>p<sub>0</sub>d<sub>2</sub></td><td></td>
<td align=center>p<sub>0</sub>d<sub>3</sub></td>
<td align=right>│</td>
 </tr>
<tr><td>&nbsp;</td>
<td align=center>q<sub>0</sub>d<sub>-2</sub></td><td></td>
<td align=center>q<sub>0</sub>d<sub>-1</sub></td><td></td>
<td align=center>q<sub>0</sub>d<sub>0</sub></td><td></td>
<td align=center>q<sub>0</sub>d<sub>1</sub></td><td></td>
<td align=center>q<sub>0</sub>d<sub>2</sub></td>
<td align=right>│</td>
 </tr>
<tr><td>&nbsp;</td><td colspan=10 align=right>───────────────┘</td></tr>
<tr><td>&nbsp;</td>
<td align=center>d<sub>0</sub></td><td></td>
<td align=center>d<sub>1</sub></td><td></td>
<td align=center>d<sub>2</sub></td><td></td>
<td align=center>d<sub>3</sub></td><td></td>
<td align=center>d<sub>4</sub></td>
 </tr>
</table>

&nbsp;

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>c<sub>i</sub> = a<sub>i</sub>  + p<sub>0</sub>c<sub>i-1</sub></td>
<td>+ q<sub>0</sub>c<sub>i-2</sub>, &nbsp; &nbsp;
i = 0,1,2,&#133;,n;
 </td></tr>
<tr><td>&nbsp;</td>
<td>d<sub>i</sub> = c<sub>i</sub>  + p<sub>0</sub>d<sub>i-1</sub></td>
<td>+ q<sub>0</sub>d<sub>i-2</sub>, &nbsp; &nbsp;
i = 0,1,2,&#133;,(n&ndash;1);
 </td></tr>
</table>

<p>&delta;p and &delta;q are then given by (5.14).</p>
 
<p>In comparison with the complex Birge-Vieta method
the quadratic factor method is seen to require only
half as many multiplications per iteration 
since it requires only two real multiplications per step
while the complex Birge-Vieta method requires
one complex<sup><a href="MSLDE1.htm#note5b">[b]</a></sup> multiplication
per step.
Because the coefficients computed in the complex Birge-Vieta
process are in general complex,
two registers are required to store the real and complex parts.
The storage required is therefore double that required
for the quadratic factor method.
On the other hand, some additional calculation
is required to resolve the quadratic factors obtained
to yield the actual roots.
The time required for this additional calculation is,
however, rather insignificant.</p>

<a name="5A5"></a>
<p><b>5. Higher Order Processes</b></p>

<p>The Newton-Raphson process for obtaining 
the solution <img src="MSLDEimg/xbar.bmp"> of the
equation <nobr>f(x) = 0</nobr> may be derived
by truncating the Taylor&rsquo;s series expansion</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
f(x) = f(x<sub>0</sub>) +
(<img src="MSLDEimg/xbar.bmp"> &nbsp; x<sub>0</sub>)f '(x<sub>0</sub>)
+ (x &ndash; x<sub>0</sub>)<sup>2</sup></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>f "(x<sub>0</sub>)</td></tr>
 <tr><td align=center><img src="MSLDEimg/38x1.bmp"></td></tr>
 <tr><td align=center>2!</td></tr>
 </table></td>
<td>+ &#133; <sup>&nbsp;</sup></td></td></tr>
</table>

<p>after the second term.
In a similar way higher order processes may be derived
by considering further terms of the expansion.
Thus a third order process is obtained by solving</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
0 = f(<img src="MSLDEimg/xbar.bmp">) &cong; 
f(x<sub>0</sub>) + 
(<img src="MSLDEimg/xbar.bmp"> &ndash; x<sub>0</sub>)f '(x<sub>0</sub>)
+ (<img src="MSLDEimg/xbar.bmp"> &ndash; x<sub>0</sub>)<sup>2</sup></td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>f "(x<sub>0</sub>)</td></tr>
 <tr><td align=center><img src="MSLDEimg/38x1.bmp"></td></tr>
 <tr><td align=center>2!</td></tr>
 </table></td>
</tr></table>

<p>for <img src="MSLDEimg/xbar.bmp">. Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/xbar.bmp"> &cong; x<sub>0</sub> &ndash;</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>f '(x<sub>0</sub>) &plusmn; &radic; 
  {[f '(x<sub>0</sub>)]<sup>2</sup> &ndash; 2f(x<sub>0</sub>)f "(x<sub>0</sub>)}
  </td></tr>
 <tr><td align=center><img src="MSLDEimg/228x1.bmp"></td></tr>
 <tr><td align=center>f "(x<sub>0</sub>)</td></tr>
 </table></td>
<td>
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp;
</tt>(5.15)
</td></tr></table>

<p>where the smaller of the two possible correction terms 
is chosen.</p>

<p>In the Birge-Vieta method the convergence
is already quadratic near a single root 
and little is gained by a higher order process.
In the case of a double root the convergence
of the Birge-Vieta method is linear and
may be made quadratic by the use of (5.15).
Use of the higher order process may therefore
be advisable if multiple roots are encountered.
Similar remarks apply to the application of a higher
order process based on the quadratic factor method.</p>

<p>The value of f "(x<sub>0</sub>) is given by (5.7)
and may be obtained by a third synthetic division.</p>

<p>To extend the quadratic factor method to a higher
order process we adopt a new notation for the remainders
as indicated in the scheme below.</p>

<table cellspacing=0 cellpadding=0><tr><td width=28 nowrap>&nbsp;</td>
<td align=center>a<sub>0</sub> &nbsp;</td><td></td>
<td align=center>a<sub>1</sub> &nbsp;</td><td></td>
<td align=center>a<sub>2</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>a<sub>n</sub> &nbsp;</td><td></td>
<td rowspan=7><tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp;&nbsp;
</tt>(5.16)</td>
 </tr>
<tr><td>&nbsp;</td><td colspan=17><hr></td></tr>
<tr><td>&nbsp;</td>
<td align=center>c<sub>0</sub> &nbsp;</td><td></td>
<td align=center>c<sub>1</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>c<sub>n-2</sub> &nbsp;</td><td></td>
<td align=center>&nbsp; B &nbsp;</td><td>│</td>
<td align=center>A &nbsp;</td><td></td>
 </tr>
<tr><td>&nbsp;</td><td colspan=15><hr></td><td>┘</td></tr>
<tr><td>&nbsp;</td>
<td align=center>d<sub>0</sub> &nbsp;</td><td></td>
<td align=center>d<sub>1</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>d<sub>n-4</sub> &nbsp;</td><td></td>
<td align=center>&nbsp; E &nbsp;</td><td></td>
<td align=center>D &nbsp;</td><td>│</td>
<td align=center>&nbsp; C &nbsp;</td><td></td>
 </tr>
<tr><td>&nbsp;</td><td colspan=13><hr></td><td>┘</td></tr>
<tr><td>&nbsp;</td>
<td align=center>e<sub>0</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>e<sub>n-6</sub> &nbsp;</td><td></td>
<td align=center>&nbsp; I &nbsp;</td><td></td>
<td align=center>&nbsp; H &nbsp;</td><td></td>
<td align=center>&nbsp; G &nbsp;</td><td></td>
<td align=center>F &nbsp;</td><td></td>
 </tr>
</table>

<p>Successive lines of the scheme correspond
to successive applications of the modified algorithm.
Using the notation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
A<sub>p</sub> =</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>&part;A</td></tr>
 <tr><td><img src="MSLDEimg/20x1.bmp"></td></tr>
 <tr><td align=center>&part;p</td></tr>
 </table></td>
</tr></table>

<p>Eqs. (5.12) and (5.13) become</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>A<sub>p</sub> = C</td>
<td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>A<sub>q</sub> = D</td>
 </tr>
<tr><td>&nbsp;</td>
<td>B<sub>p</sub> = D</td>
<td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>B<sub>q</sub> = E.</td>
 </tr>
</table>

<p>Because of the complete uniformity of the algorithm
the relation between any two quantities in the scheme above
is determined by their relative position.
For example, since G and C stand in the same relation
as do D and A and since <nobr>D = A<sub>q</sub>,</nobr>
then <nobr>G = C<sub>q</sub> = A<sub>pq</sub>.</nobr>
Clearly then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>A<sub>pp</sub> = F</td>
<td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>B<sub>pp</sub> = G</td>
 </tr>
<tr><td>&nbsp;</td>
<td>A<sub>pq</sub> = G</td>
<td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>B<sub>pq</sub> = H</td>
 </tr>
<tr><td>&nbsp;</td>
<td>A<sub>qq</sub> = H</td>
<td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>B<sub>qq</sub> = I.</td>
 </tr>
</table>

<p>Extending the Eqs. (5.9) to include second order terms
of the Taylor&rsquo;s series
and using the present notation we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>0 = A + A<sub>p</sub>&delta;p + A<sub>q</sub>&delta;q +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>A<sub>pp</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;p<sup>2</sup> + A<sub>pq</sub>&delta;p&delta;q +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>A<sub>qq</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;q<sup>2</sup></td>
 </tr>
<tr><td>&nbsp;</td>
<td>0 = B + B<sub>p</sub>&delta;p + B<sub>q</sub>&delta;q +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>B<sub>pp</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;p<sup>2</sup> + B<sub>pq</sub>&delta;p&delta;q +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>B<sub>qq</sub></td></tr>
 <tr><td><img src="MSLDEimg/28x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;q<sup>2</sup>.</td>
 </tr>
</table>

<p>Hence</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>C&delta;p + D&delta;q</td>
<td>= &ndash; (A +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>F</td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;p<sup>2</sup> + G&delta;p&delta;q +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>H</td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;p&delta;q<sup>2</sup>)</td>
 </tr>
<tr><td>&nbsp;</td>
<td>D&delta;p + E&delta;q</td>
<td>= &ndash; (B +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>G</td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;p<sup>2</sup> + H&delta;p&delta;q +</td>
<td><table cellspacing=0 cellpadding=0>
 <tr><td align=center>I</td></tr>
 <tr><td><img src="MSLDEimg/14x1.bmp"></td></tr>
 <tr><td align=center>2</td></tr>
 </table></td>
<td>&delta;p&delta;q<sup>2</sup>).</td>
 </tr>
</table>

<p>This pair of non-linear equations may be solved by iteration
providing that &delta;p and &delta;q are not too large.
Each approximation to the values of &delta;p and &delta;q
are substituted in the right-hand side 
and the resulting linear equations are solved
for a new approximation.  
The initial values of &delta;p and &delta;q
are taken as zero.</p>

<a name="5A6"></a>
<p><b>6. Factors of Higher Degree</b></p>

<p>Aitken<sup><acronym 
title="25. Aitken, A.C., Studies in Practical Mathematics, VII., “On the Theory of Methods of Factorizing Polynomials by Iterated Division”, Proc. Roy. Soc., Edinburgh, Vol. 63, p. 326 (1951).">25</acronym></sup>
has studied the extraction of real factors of higher degree than second
by means of methods called &ldquo;penultimate remaindering&rdquo;.
The modified quadratic factor method may also be extended to
extract factors of higher degree.
Unless all roots are known to be real the real factor must be of even degree.
The case of a quartic factor will be developed briefly.</p>

<p>As for the quadratic factor the synthetic division algorithm</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>b<sub>i</sub></td> <td>=</td>
 <td>a<sub>i</sub> + pb<sub>i-1</sub> + qb<sub>i-2</sub> + rb<sub>i-3</sub> + sb<sub>i-4</sub>, &nbsp; &nbsp; &nbsp;
  i = 0,1,&#133;,n-3,</td></tr>
<tr><td></td><td>b<sub>n-2</sub></td> <td>=</td>
  <td>a<sub>n-2</sub> + qb<sub>n-4</sub> + rb<sub>n-5</sub> + sb<sub>n-6</sub>,</td></tr>
<tr><td></td><td>b<sub>n-1</sub></td> <td>=</td>
  <td>a<sub>n-1</sub> + rb<sub>n-4</sub> + sb<sub>n-5</sub>,</td></tr>
<tr><td></td><td>b<sub>n</sub></td> <td>=</td>
  <td>a<sub>n</sub> + sb<sub>n-4</sub>,</td></tr>
</table>

<p>yields the quotient polynomial</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>Q(x) = b<sub>0</sub>x<sup>n-4</sup> + b<sub>1</sub>x<sup>n-5</sup> + &#133; + b<sub>n-4</sub>
</td></tr></table>

<p>such that</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>P<sub>n</sub>(x) = (x<sup>4</sup> - px<sup>3</sup> - qx<sup>2</sup> - rx - s)Q(x) + 
b<sub>n-3</sub>x<sup>3</sup> + b<sub>n-2</sub>x<sup>2</sup> + b<sub>n-1</sub>x + b<sub>n</sub>.
</td></tr></table>

<p>Again the modified algorithm</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>c<sub>i</sub> = a<sub>i</sub> + pc<sub>i-1</sub> + qc<sub>i-2</sub> + rc<sub>i-3</sub>
+ sc<sub>i-4</sub>,  &nbsp; &nbsp; &nbsp;  i = 0,1,&#133;,n,
</td></tr></table>

<p>yields the same quotient polynomial Q(x). Adopting the notation</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>D<sub>&nbsp;</sub></td> <td>=<sub>&nbsp;</sub></td> <td>c<sub>n-3</sub></td> <td>=</td> <td>b<sub>n-3</sub>,</td></tr>
<tr><td></td>
 <td>C<sub>&nbsp;</sub></td> <td>=<sub>&nbsp;</sub></td> <td>c<sub>n-2</sub></td> <td>=</td> <td>b<sub>n-2</sub> + pc<sub>n-3</sub> = b<sub>n-2</sub> + pD</td></tr>
<tr><td></td>
 <td>B<sub>&nbsp;</sub></td> <td>=<sub>&nbsp;</sub></td> <td>c<sub>n-1</sub></td> <td>=</td> <td>b<sub>n-1</sub> + pc<sub>n-2</sub> + qc<sub>n-3</sub> = b<sub>n-1</sub> + pC + qD</td></tr>
<tr><td></td>
 <td>A<sub>&nbsp;</sub></td> <td>=<sub>&nbsp;</sub></td> <td>c<sub>n</sub></td>   <td>=</td> <td>b<sub>n</sub>   + pc<sub>n-1</sub> + qc<sub>n-2</sub> + rc<sub>n-3</sub> 
= b<sub>n</sub> + pB + qC + rD</td></tr>

<tr><td>and</td></tr>

<tr><td></td>
 <td>&alpha;<sup>&nbsp;</sup></td> <td>=<sup>&nbsp;</sup></td> <td colspan=3>(x<sup>4</sup> - px<sup>3</sup> - qx<sup>2</sup> - rx - s)</td></tr>
</table>

<p>we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>P<sub>n</sub>(x)<sup>&nbsp;</sup></td> <td>=<sup>&nbsp;</sup><sub>&nbsp;</sub></td> 
 <td>&alpha;Q(x) + Dx<sup>3</sup> + (C-pD)x<sup>2</sup> + (B-qD-pC)x + (A-rD-qC-pB). &nbsp;(5.17)</td></tr>
</table>

<p>Note that the vanishing of the quantities A, B, C, and D is the necessary 
and sufficient condition for the vanishing of the remainders
b<sub>n</sub>, b<sub>n-1</sub>, b<sub>n-2</sub> and b<sub>n-3</sub>.</p>

<p>Differentiating Eq. (5.17) we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>0 = &alpha;Q<sub>p</sub>(x) - x<sup>3</sup>Q(x) + x<sup>3</sup>D<sub>p</sub> + 
 x<sup>2</sup>(C<sub>p</sub>-pD<sub>p</sub>-D) + x(B<sub>p</sub>-qD<sub>p</sub>-pC<sub>p</sub>-C) +<br>
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> A<sub>p</sub> - rD<sub>p</sub> - qC<sub>p</sub> - pB<sub>p</sub> - B,
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> (5.18)
 </td></tr>
<tr><td><sup>&nbsp;</sup></td></tr>
<tr><td></td><td>0 = &alpha;Q<sub>q</sub>(x) - x<sup>2</sup>Q(x) + x<sup>3</sup>D<sub>q</sub> + 
 x<sup>2</sup>(C<sub>q</sub>-pD<sub>q</sub>) + x(B<sub>q</sub>-qD<sub>q</sub>-pC<sub>q</sub>-D) +<br>
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> A<sub>q</sub> - rD<sub>q</sub> - qC<sub>q</sub> - pB<sub>q</sub> - C,
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> (5.19)
 </td></tr>
<tr><td><sup>&nbsp;</sup></td></tr>
<tr><td></td><td>0 = &alpha;Q<sub>r</sub>(x) - xQ(x) + x<sup>3</sup>D<sub>r</sub> + 
 x<sup>2</sup>(C<sub>r</sub>-pD<sub>r</sub>) + x(B<sub>r</sub>-qD<sub>r</sub>-pC<sub>r</sub>) +<br>
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> A<sub>r</sub> - rD<sub>r</sub> - qC<sub>r</sub> - pB<sub>r</sub> - D,
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</tt>&nbsp; (5.20)
 </td></tr>
<tr><td><sup>&nbsp;</sup></td></tr>
<tr><td></td><td>0 = &alpha;Q<sub>s</sub>(x) - Q(x) + x<sup>3</sup>D<sub>s</sub> + 
 x<sup>2</sup>(C<sub>s</sub>-pD<sub>s</sub>) + x(B<sub>s</sub>-qD<sub>s</sub>-pC<sub>s</sub>) +<br>
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> A<sub>s</sub> - rD<sub>s</sub> - qC<sub>s</sub> - pB<sub>s</sub>.
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</tt>&nbsp; (5.21)
 </td></tr>
</table>

<p>Re-arranging the last equation to the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>Q(x) = &alpha;Q<sub>s</sub>(x) + x<sup>3</sup>D<sub>s</sub> + 
 x<sup>2</sup>(C<sub>s</sub>-pD<sub>s</sub>) + x(B<sub>s</sub>-qD<sub>s</sub>-pC<sub>s</sub>) +<br>
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> A<sub>s</sub> - rD<sub>s</sub> - qC<sub>s</sub> - pB<sub>s</sub>
</td></tr></table>

<p>shows that the application of the modified algorithm to Q(x) 
leaves D<sub>s</sub> as the first remainder.
Thus if the scheme of two successive applications 
of the algorithm is shown as</p>

<table cellspacing=0 cellpadding=0><tr><td width=28 nowrap>&nbsp;</td>
<td align=center>a<sub>0</sub> &nbsp;</td><td></td>
<td align=center>a<sub>1</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>a<sub>n-4</sub> &nbsp;</td><td></td>
<td align=center>a<sub>n-3</sub> &nbsp;</td><td></td>
<td align=center>a<sub>n-2</sub> &nbsp;</td><td></td>
<td align=center>a<sub>n-1</sub> &nbsp;</td><td></td>
<td align=center>a<sub>n</sub> &nbsp;</td><td></td>
 </tr>
<tr><td>&nbsp;</td><td colspan=19><hr></td></tr>
<tr><td>&nbsp;</td>
<td align=center>c<sub>0</sub> &nbsp;</td><td></td>
<td align=center>c<sub>1</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>c<sub>n-5</sub> &nbsp;</td><td></td>
<td align=center>c<sub>n-4</sub> &nbsp;</td><td></td>
<td align=center>&nbsp; D &nbsp;</td><td></td>
<td align=center>&nbsp; C &nbsp;</td><td></td>
<td align=center>B &nbsp;</td><td>│</td>
<td align=center>A &nbsp;</td><td></td>
 </tr>
<tr><td>&nbsp;</td><td colspan=17><hr></td><td>┘</td></tr>
<tr><td>&nbsp;</td>
<td align=center>d<sub>0</sub> &nbsp;</td><td></td>
<td align=center>&#133;</td><td></td>
<td align=center>&nbsp; K &nbsp;</td><td></td>
<td align=center>&nbsp; J &nbsp;</td><td></td>
<td align=center>&nbsp; I &nbsp;</td><td></td>
<td align=center>&nbsp; H &nbsp;</td><td></td>
<td align=center>&nbsp; G &nbsp;</td><td></td>
<td align=center>&nbsp; F &nbsp;</td><td></td>
<td align=center>E &nbsp;</td><td></td>
 </tr>
</table>

<p>we have D<sub>s</sub> = K. The coefficient of x<sub>2</sub>
in (5.21) is</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>C<sub>s</sub> - pD<sub>s</sub> = C<sub>s</sub> - pK.
</td></tr></table>

<p>But from the form of the remainders in (5.17) it is clear
that the coefficient of x<sup>2</sup> is <nobr>J - pK.</nobr></p>

<table><tr><td width=28 nowrap>&therefore; </td>
 <td>J = C<sub>s</sub>.
</td></tr></table>

<p>Similarly B<sub>s</sub> = I and A<sub>s</sub> = H.
Rearranging (5.18) to the form</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>x<sup>3</sup>Q(x) + Dx<sup>2</sup> + Cx + B = &alpha;Q<sub>p</sub>(x) + x<sup>3</sup>D<sub>p</sub> +
 x<sup>2</sup>(C<sub>p</sub>-pD<sub>p</sub>) + x(B<sub>p</sub>-qD<sub>p</sub>-pC<sub>p</sub>)
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>+ A<sub>p</sub> - rD<sub>p</sub> - qC<sub>p</sub> - pB<sub>p</sub>
</td></tr></table>

<p>shows the left-hand side to be the polynomial formed
of the coefficients c<sub>0</sub>,c<sub>1</sub>,&#133;,c<sub>n-4</sub>,
D, C, B, and therefore the derivatives may be obtained as before
but with K, J, I, and H replaced respectively by
H, G, F, and E.
Thus D<sub>p</sub> = H, C<sub>p</sub> = G, B<sub>p</sub> = F,
A<sub>p</sub> = E.
In a similar way derivatives with respect to q and r may be
obtained from Eqs. (5.19) and (5.20) and
in matrix notation we have finally</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=4><img src="MSLDEimg/matrixl4.bmp"></td>
 <td>A<sub>p</sub></td> <td rowspan=4></td> <td>A<sub>q</sub></td> <td rowspan=4></td>
 <td>A<sub>r</sub></td> <td rowspan=4></td> <td>A<sub>s</sub></td>
 <td rowspan=4><img src="MSLDEimg/matrixr4.bmp"></td> <td rowspan=4></td>
 <td rowspan=4>=</td> <td rowspan=4></td>
 <td rowspan=4><img src="MSLDEimg/matrixl4.bmp"></td>
 <td>E</td> <td rowspan=4></td> <td>F</td> <td rowspan=4></td> <td>G</td> <td rowspan=4></td> <td>H</td>
 <td rowspan=4><img src="MSLDEimg/matrixr4.bmp"></td> <td rowspan=4></td>
 <td rowspan=4>.</td></tr>
<tr><td></td>
 <td>B<sub>p</sub></td> <td>B<sub>q</sub></td> 
 <td>B<sub>r</sub></td> <td>B<sub>s</sub></td>
 <td>F</td> <td>G</td> <td>H</td> <td align=center>I</td>
 </tr>
<tr><td></td>
 <td>C<sub>p</sub></td> <td>C<sub>q</sub></td>
 <td>C<sub>r</sub></td> <td>C<sub>s</sub></td>
 <td>G</td> <td>H</td> <td align=center>I</td> <td align=center>J</td> 
 </tr>
<tr><td></td>
 <td>D<sub>p</sub></td> <td>D<sub>q</sub></td>
 <td>D<sub>r</sub></td> <td>D<sub>s</sub></td>
 <td>H</td> <td align=center>I</td> <td align=center>J</td> <td>K</td> 
 </tr>
</table>

<p>Expanding A, B, C, and D in Taylor&rsquo;s series
and retaining only linear terms we have (compare Eq. (5.9))</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=4><img src="MSLDEimg/matrixl3.bmp"></td>
 <td>E</td> <td rowspan=4></td> <td>F</td> <td rowspan=4></td> <td>G</td> <td rowspan=4></td> <td>H</td>
 <td rowspan=4><img src="MSLDEimg/matrixr3.bmp"></td> <td rowspan=4></td>
 <td rowspan=4><img src="MSLDEimg/matrixl3.bmp"></td> <td rowspan=4></td>
 <td>&delta;p</td>
 <td rowspan=4><img src="MSLDEimg/matrixr3.bmp"></td> <td rowspan=4></td>
 <td rowspan=4>=</td> <td rowspan=4></td>
 <td rowspan=4><img src="MSLDEimg/matrixl3.bmp"></td>
 <td>-A</td>
 <td rowspan=4><img src="MSLDEimg/matrixr3.bmp"></td> <td rowspan=4></td>
 <td rowspan=4>.</td>
 <td rowspan=4><tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>(5.22)</td>
 </tr>
<tr><td></td>
 <td>F</td> <td>G</td> <td>H</td> <td align=center>I</td>
 <td>&delta;q</td> <td>-B</td>
 </tr>
<tr><td></td>
 <td>G</td> <td>H</td> <td align=center>I</td> <td align=center>J</td>
 <td>&delta;r</td> <td>-C</td>
 </tr>
<tr><td></td>
 <td>H</td> <td align=center>I</td> <td align=center>J</td> <td>K</td>
 <td>&delta;s</td> <td>-D</td>
 </tr>
</table>

<p>Solutions of the fourth order linear system (5.22) yield the
necessary corrections.
This method removed four roots at a time but because 
of the greater complexity the quadratic factor method is
probably to be preferred.
When finally obtained, the quartic factor must itself be resolved.  
A method of solution is considered below.</p>

<a name="5A7"></a>
<p><b>7. Solution of the Quartic</b><sup><a href="MSLDE1.htm#note5c">[c]</a></sup></p>

<p>The following method is easily programmed for an automatic computer
and could be used in conjunction with the method discussed above.
However, it is probably of greatest value as a method for desk calculators
and is believed to be superior for this purpose to existing methods.
The method is capable of extracting either real or complex roots
but works essentially well if all four roots are complex.</p>

<p>Given a real quartic</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>x<sup>4</sup> + Ax<sup>3</sup> + Bx<sup>2</sup> + Cx + D,
</td></tr></table>

<p>two real quadratic factors are assumed to be
<nobr>x<sup>2</sup> + ax + b</nobr> and
<nobr>x<sup>2</sup> + &alpha;x + &beta;</nobr>.</p>

<p>Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=4><tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt></td>
 <td align=right>a + &alpha;</td> <td>=</td> <td>A</td> 
 <td rowspan=4><tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> &nbsp;</td>
 <td>(5.23)</td></tr>
<tr><td></td> <td align=right>b + &alpha;a + &beta;</td> <td>=</td> <td>B</td>
 <td>(5.24)</td></tr>
<tr><td></td> <td align=right>&alpha;b + &beta;a</td> <td>=</td> <td>C</td>
 <td>(5.25)</td></tr>
<tr><td></td> <td align=right>&beta;b</td> <td>=</td> <td>D.</td>
 <td>(5.26)</td></tr>
</table>

<p>Setting</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=3>b =</td> <td align=center>D</td> <td rowspan=3>, &nbsp;</td>
 <td rowspan=3>&alpha; =</td> <td align=center>C-A&beta;</td> <td rowspan=3>;</td>
 <td rowspan=3>&nbsp; &nbsp; &nbsp;</td>
 <td rowspan=3>a = A - &alpha;</td>
 <td rowspan=3><tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> (5.27)</td></tr>
<tr><td></td> <td><img src="MSLDEimg/14x1.bmp"></td>
 <td><img src="MSLDEimg/38x1.bmp"></td></tr>
<tr><td></td> <td align=center>&beta;</td> <td align=center>b-&beta;</td></tr>
</table>

<p>satisfies all but the second of the equations above.
Imposing the further condition</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>&oslash;(&beta;) = B - b - &beta; - &alpha;a = 0
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>
 (5.28)
</td></tr></table>

<p>satisfies (5.24) and the values of a, b, &alpha;, &beta; 
so obtained furnish the desired solution of the quartic
in terms of its quadratic factors.</p>

<p>Equation (5.28) may be solved for &beta;
by the method of false position,
using the fact that &beta; is real and less than &radic;D
in absolute value.
If all the roots are complex than &beta; is also positive
and (5.28) has a single real root in (0,&radic;D).
The computation of &oslash;(&beta;) is carried out
by evaluating b, &alpha; and a according to (5.27)
and substituting in (5.28).</p>

<p>The method fails only if b - &beta; = 0.
This case can arise only if <nobr>C - AD<sup>&frac12;</sup> = 0</nobr>
and a solution may then be obtained directly from 
the relations</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>b = &beta; = D<sup>&frac12;</sup>, &nbsp; &nbsp; &nbsp;
 a + &alpha; = A, &nbsp; &nbsp; &nbsp; 
 a&alpha; = B - 2D<sup>&frac12;</sup>.
</td></tr></table>

<p><b>Example</b>.</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td colspan=8>x<sup>4</sup> + 2.5504x<sup>3</sup> + 37.1185x<sup>2</sup> 
- 38.4650x + 520.3597 = 0
</td></tr>
<tr><td></td></tr>
<tr><td></td>
 <td>b =</td>
 <td><table>
  <tr><td align=center>520.3597</td></tr>
  <tr><td><img src="MSLDEimg/60x1.bmp"></td></tr>
  <tr><td align=center>&beta;</td></tr>
 </table></td>
 <td>;</td>
<td nowrap>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</td>
 <td>&alpha; = </td>
 <td><table>
  <tr><td align=center>-38.4650 - 2.5504&beta;</td></tr>
  <tr><td><img src="MSLDEimg/140x1.bmp"></td></tr>
  <tr><td align=center>b - &beta;</td></tr>
 </table></td>
 <td>;</td>
<td nowrap>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</td>
</tr>
<tr><td></td></tr>
<tr><td></td><td colspan=3>a = 2.5504 - &alpha;;</td>
 <td></td>
 <td colspan=3>&oslash; = 37.1185 - b - &beta; - a&alpha;.</td>
 <td></td></tr>
</table>

&nbsp;

<table>
<tr><td>Choose &beta;<sub>0</sub> &cong;</td>
 <td><table>
  <tr><td align=center>D<sup>&frac12;</sup></td></tr>
  <tr><td><img src="MSLDEimg/32x1.bmp"></td></tr>
  <tr><td align=center>2</td></tr>
 </table></td>
<td>.</td></tr></table>


<table>
<tr><td align=center>&beta;</td> <td rowspan=8>&nbsp; &nbsp;</td> 
 <td align=center>b</td> <td rowspan=8>&nbsp; &nbsp;</td> 
 <td align=center>&alpha;</td> <td rowspan=8>&nbsp; &nbsp;</td>
<td align=center>a</td> <td rowspan=8>&nbsp; &nbsp;</td>
<td align=center>&oslash;<br></td></tr>
<tr><td></td></tr>

<tr><td>12.</td>     <td>43.36</td>   <td>-2.20</td>   <td>4.75</td>   <td>-7.80</td></tr>
<tr><td>14.</td>     <td>37.17</td>   <td>-3.20</td>   <td>5.75</td>   <td>&nbsp;4.35</td></tr>
<tr><td>13.2839</td> <td>39.1722</td> <td>-2.7945</td> <td>5.3449</td> <td>-0.4011</td></tr>
<tr><td>13.3443</td> <td>38.9949</td> <td>-2.8264</td> <td>5.3768</td> <td>-0.0237</td></tr>
<tr><td>13.3481</td> <td>38.9838</td> <td>-2.8284</td> <td>5.3788</td> <td>&nbsp;0.0000</td></tr>
</table>

<p>A useful modification of the method consists in first
making the substitution x= y + h with h = -A/4 so that
the resulting equation in y contains on term in y<sup>3</sup>.
Considering the equation in y we now have A = 0 and
Eqs. (5.27) and (5.28) simplify to</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>b = D/&beta;,</td> <td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>&alpha; =</td>
<td><table>
 <tr><td align=center>C</td></tr>
 <tr><td><img src="MSLDEimg/38x1.bmp"></td></tr>
 <tr><td align=center>b - &beta;</td></tr>
 </table></td>
<td>,</td> <td>&nbsp; &nbsp; &nbsp; &nbsp;</td>
<td>a = - &alpha;,</td>
<td><tt>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> (5.27a)</td>
 </tr>
<tr><td></td></tr>
<tr><td></td>
 <td colspan=7>&oslash;(&beta;) = B - b + &alpha;<sup>2</sup> - &beta; = 0.</td>
<td align=right>(5.28a)</td>
 </tr>
</table>

<p>The special case mentioned above now exhibits itself
immediately by the fact that <nobr>C = 0.</nobr></p>

<p>An attempt to use a Newton-Raphson process to solve
Eq. (5.28a) leads to a difficult expression for
d&oslash;(&beta;)/d&beta;.
If, however, &oslash; is expressed as a function
of the new variable</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td align=right>&gamma;</td> <td>=</td> <td colspan=3>&beta; + b</td></tr>
<tr><td></td></tr>
<tr><td>then</td></tr>
<tr><td></td>
 <td align=right>&oslash;(&gamma;)</td> <td>=</td> <td colspan=3>B - &gamma; + d</td></tr>
<tr><td></td></tr>
<tr><td></td>
 <td>&oslash;'(&gamma;)</td> <td>=</td>
 <td><table>
  <tr><td rowspan=3>-</td> 
   <td rowspan=3><img src="MSLDEimg/matrixl2.bmp"></td>
   <td rowspan=3>1 + </td>
   <td align=center>&gamma;d<sup>2</sup></td>
   <td rowspan=3><img src="MSLDEimg/matrixr2.bmp"></td></tr>
  <tr><td><img src="MSLDEimg/38x1.bmp"></td></tr>
  <tr><td>C<sup>2</sup>/2</td></tr>
 </table></td> </tr>  
<tr><td></td></tr>
<tr><td>where</td></tr>
<tr><td></td></tr>
<tr><td></td>
 <td align=right>d</td> <td>=</td> 
  <td><table>
   <tr><td align=center>C<sup>2</sup></td><td rowspan=3>.</td></tr>
   <tr><td><img src="MSLDEimg/50x1.bmp"></td></tr>
   <tr><td>&gamma;<sup>2</sup> - 4D</td></tr>
   </table></td>
  </tr>
</table>

<p>Although the method works for either real or complex roots
the behavior of &oslash;(&gamma;) is greatly simplified
by the assumption that all roots are complex,
for then &oslash;(&gamma;) has exactly one real zero
and since b and &beta; are both positive &gamma; &gt; 2D<sup>&frac12;</sup>.
Also &oslash;'(&gamma;) &lt; 0 and &oslash;"(&gamma;) &gt; 0
for &gamma; &gt; 2D<sup>&frac12;</sup>.
Thus &oslash;(&gamma;) has one real zero and no minimum
or inflection point in the interval (2&radic;D,&infin;).
Because of these properties the zero of &oslash;(&gamma;)
is very easy to locate.</p>

<p>Using a machine which will store d as a constant multiplier
the only quantities necessary to record are &gamma; and &oslash;.
However since the final value of <nobr>&gamma;<sup>2</sup> - 4D</nobr>
is required to compute <nobr>b - &beta; = &radic;&gamma;<sup>2</sup> - 4D</nobr>
and since &oslash;' need not be recomputed in the final stages it is convenient
to record both of these quantities as well.</p>

<p><b>Example</b>. The substitution x = y - 0.6376 in the quartic of the
previous example gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>y<sup>4</sup> + 34.6793y<sup>2</sup> - 83.7248y + 559.4971 = 0.</td></tr></table>

<p>Computing C<sup>2</sup>, C<sup>2</sup>/2 and 4D we have</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>d =</td> 
 <td><table>
  <tr><td align=center>7009.8421</td></tr>
  <tr><td><img src="MSLDEimg/80x1.bmp"></td></tr>
  <tr><td align=center>&gamma;<sup>2</sup> - 2237.9164</td></tr>
  </table></td>
 <td>, &nbsp; &nbsp; &nbsp;</td>
 <td>&oslash; = 34.6783 - &gamma; + d</td></tr>
<tr><td></td></tr>
<tr><td></td>
 <td>&oslash;' = -</td>
 <td colspan=2><table>
  <tr>
   <td rowspan=3><img src="MSLDEimg/matrixl2.bmp"></td>
   <td rowspan=3>1 +</td>
   <td align=center>&gamma;d<sup>2</sup></td>
   <td rowspan=3><img src="MSLDEimg/matrixr2.bmp"></td>
   <td rowspan=3>.</td>
  <tr><td><img src="MSLDEimg/75x1.bmp"></td></tr>
  <tr><td align=center>3504.9210</td></tr>
 </table></td>
</tr></table>

<p>A few rough mental calculations show that
&oslash;(50) &gt; 0 and &oslash;(52) &lt; 0.</p>

<table>
<tr><td align=center>&gamma;</td> <td rowspan=6>&nbsp; &nbsp;</td>
<td align=center>&gamma;<sup>2</sup> - 4D</td> <td rowspan=6>&nbsp; &nbsp;</td>
<td align=center>&oslash;</td> <td rowspan=6>&nbsp; &nbsp;</td>
<td align=center>&oslash;'</td></tr>
<tr><td></td></tr>
<tr><td>51.0000</td> <td>363.0836</td> <td>2.9857</td> <td>-6.4237</td></tr>
<tr><td>51.4648</td> <td>410.7092</td> <td>0.2821</td> <td>-5.2774</td></tr>
<tr><td>51.5182</td> <td>416.2085</td> <td>0.0032</td> <td>-5.1694</td></tr>
<tr><td>51.5188</td> <td>416.2704</td> <td>0.0001</td> <td></td></tr>
</table>

<p>Hence b - &beta; = &radic;416.2704 = 20.4027,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>
b = 35.9607, &nbsp; &nbsp; 
&beta; = 15.5581 &nbsp; &nbsp; 
&alpha; = - 4.1036 = -a.</td></tr></table>

<p>Thus</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td rowspan=2>y =</td> <td rowspan=2><img src="MSLDEimg/matrixl2.bmp"></td>
 <td align=right>-2.0518</td> <td>&plusmn; 5.6348i</td></tr>
<tr><td></td> <td align=right>2.0518</td> <td>&plusmn; 3.3687i</td></tr>
<tr><td></td></tr>
<tr><td></td> <td rowspan=2>x =</td> <td rowspan=2><img src="MSLDEimg/matrixl2.bmp"></td>
 <td align=right>-2.6894</td> <td>&plusmn; 5.6348i</td></tr>
<tr><td></td> <td align=right>1.4142</td> <td>&plusmn; 3.3687i.</td></tr>
</table>

<p>The quartic formed from these roots agrees with the given 
quartic to four decimal places in each coefficient.</p>

<p>Since</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=3>&oslash;"(&gamma;) = </td> 
  <td align=center>2C<sup>2</sup>(3&gamma;<sup>2</sup> + 4D)</td></tr>
<tr><td></td><td><img src="MSLDEimg/100x1.bmp"></td></tr>
<tr><td></td><td align=center>(&gamma;<sup>2</sup> - 4D)<sup>3</sup></td></tr>
</table>

<p>is easily computed it is possible to use the higher order
correction</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td align=center>- &oslash;' &plusmn; &radic; &oslash;'<sup>&nbsp;2</sup> - 2&oslash;&oslash;"</td></tr> 
<tr><td></td><td><img src="MSLDEimg/120x1.bmp"></td></tr>
<tr><td></td><td align=center>&oslash;"</td></tr>
</table>

<p>where the sign is chosen to give the smaller correction.
Application of this in the preceding example gives 
<nobr>&oslash;"(51) = 2.9410</nobr> and a correction of
0.5288 which is much better than that given by 
the Newton-Raphson method.
However the simpler method is probably to be preferred.</p>

<a name="5A8"></a>
<p><b>8. Solution of the Sextic</b></p>

<p>An extension of the method of real factors
to the sextic yields a practical computational method as follows:
Assume that the sextic</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td>P<sub>6</sub>(x) = x<sup>6</sup> + Ax<sup>5</sup> + Bx<sup>4</sup> + Cx<sup>3</sup> 
+ Dx<sup>2</sup> + Ex + F
</td></tr></table>

<p>has real factors x<sup>2</sup> + &alpha;x + &beta; and
x<sup>4</sup> + ax<sup>3</sup> + bx<sup>2</sup> + cx + d. 
Again assuming that A is made zero by translation we obtain 
the following relations</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td align=right>a</td> <td rowspan=6></td> <td>=</td> <td rowspan=6></td> <td>-&alpha;</td>
 <td rowspan=6><tt>&nbsp; &nbsp; &nbsp; &nbsp;</tt></td>
 <td rowspan=6><img src="MSLDEimg/matrixr5.bmp"></td>
 <td rowspan=6><tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> (5.29)</td>
 </td></tr>
<tr><td></td> <td align=right>b + &beta; + &alpha;<sup>2</sup></td> <td>=</td> <td>B</td></tr>
<tr><td></td> <td align=right>c + b&alpha; - &beta;&alpha;</td> <td>=</td> <td>C</td></tr>
<tr><td></td> <td align=right>d + c&alpha; - b&beta;</td> <td>=</td> <td>D</td></tr>
<tr><td></td> <td align=right>d&alpha; - c&beta;</td> <td>=</td> <td>E</td></tr>
<tr><td></td> <td align=right>d&beta;</td> <td>=</td> <td>F.</td></tr>
</table>

<p>Then if d = F/&beta;, &gamma; = d - &beta;<sup>2</sup>,</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=3>&alpha; =</td> <td rowspan=3></td> <td>E/2</td> <td rowspan=3>&plusmn;</td>
 <td rowspan=3>&radic; &nbsp;(</td> <td>E/2</td> <td rowspan=3>)<sup>2</sup> +</td>
 <td align=center>F + &beta; [-D + &beta;(B-&beta;)]</td>
 <td rowspan=3>, &nbsp; &nbsp; &alpha; real <tt>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</tt> (5.30)</td>
<tr><td></td><td><img src="MSLDEimg/20x1.bmp"></td> <td><img src="MSLDEimg/20x1.bmp"></td> <td><img src="MSLDEimg/140x1.bmp"></td></tr>
<tr><td></td><td align=center>&gamma;</td> <td align=center>&gamma;</td> <td align=center>&gamma;</td></tr>
</table>

<p>and &beta; is chosen as any real root of</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>&oslash;(&beta;) = E/&beta; - C + &alpha;(B - 2&beta; + &alpha;<sup>2</sup> - d/&beta;) = 0
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> &nbsp; (5.31)
</td></tr></table>

<p>the values of &alpha; and &beta; so obtained satisfy Eqs. (5.29).
The remaining coefficients are then furnished by the relations</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=3>b = B - &beta; + &alpha;<sup>2</sup>, &nbsp; &nbsp; &nbsp;
c =</td> <td>E - d&alpha;</td> <td rowspan=3>.</td></tr>
<tr><td></td> <td><img src="MSLDEimg/42x1.bmp"></td></tr>
<tr><td></td> <td align=center>&beta;</td></tr>
</table>

<p>At least one root of (5.31) lies in the interval (-F<sup>1/3</sup>,F<sup>1/3</sup>).
If all the zeros of P<sub>6</sub>(x) are complex the zeros of (5.31)
are all positive and exactly three in number.
Hence either the interval (0,F<sup>1/3</sup>)
or the interval (F<sup>1/3</sup>,&infin;) contains one real zero only.
The two values of &alpha; given by Eq. (5.30)
introduce a complication but it is usually easy to recognize
which value should be chosen.</p>

<p>The method fails only if d - &beta;<sup>2</sup> = 0, 
in which case &beta; = F<sup>1/3</sup> and
(since &alpha; is finite) <nobr>FB<sup>3</sup> - D<sup>3</sup> = 0.</nobr>
The solution is then given immediately by</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td rowspan=3>&beta; = F<sup>3</sup>, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&alpha; =</td> <td align=center>1</td> <td rowspan=3>(3&beta; - 2B +</td> <td align=center>D</td>
 <td rowspan=3>),</td> </tr>
<tr><td></td> <td><img src="MSLDEimg/20x1.bmp"></td> <td><img src="MSLDEimg/14x1.bmp"></td></tr>
<tr><td></td> <td align=center>3E</td> <td align=center>&beta;</td></tr>
</table>

<p>the expression for &alpha; being obtained as the limiting form of (5.30).</p>

<p><b>Example:</b></p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td>A</td> <td>=</td> <td>0</td> <td>&nbsp; &nbsp; &nbsp;</td>
 <td>B</td> <td>=</td> <td align=right>46.5813</td>  <td>&nbsp; &nbsp; &nbsp;</td>
 <td>C</td> <td>=</td> <td align=right>-89.2555</td></tr>
<tr><td></td>
 <td>D</td> <td>=</td> <td> 1355.5763</td> <td></td>
 <td>E</td> <td>=</td> <td align=right>-2198.2332</td>  <td></td>
 <td>F</td> <td>=</td> <td align=right>10076.5517</td></tr>
<tr><td></td>
 <td>d</td> <td>=</td> <td><table><tr><td align=center>10076.5517</td></tr>
  <tr><td><img src="MSLDEimg/75x1.bmp"></td></tr>
  <tr><td align=center>&beta;</td></tr></table></td> <td></td>
  <td>&gamma;</td> <td>=</td> <td>d - &beta;<sup>2</sup></td> <td></td>
  <td>&delta;</td> <td>=</td> <td><table><tr><td align=center>-1099.1166</td></tr>
  <tr><td><img src="MSLDEimg/75x1.bmp"></td></tr>
  <tr><td align=center>&gamma;</td></tr></table></td> </tr>
<tr><td></td>
 <td>&alpha;</td> <td>=</td> 
 <td colspan=9><table>
  <tr><td rowspan=3>&delta; &plusmn; &radic; &delta;<sup>2</sup> +</td>
   <td align=center>100076.5517 + &beta; [&beta;(46.5813 - &beta;) - 1355.5763]</td></tr>
  <tr><td><img src="MSLDEimg/300x1.bmp"></td></tr>
  <tr><td align=center>d - &beta;<sup>2</sup></td></tr>
  </table></td></tr>
<tr><td></td>
 <td>&oslash;</td> <td>=</td> 
 <td colspan=9><table>
  <tr><td align=center>-2198.2332</td>
   <td rowspan=3>+ 89.2555 + &alpha; [46.5813 - 2&beta; + &alpha;<sup>2</sup> -</td>
   <td align=center>d</td>
   <td rowspan=3>].</td></tr>
  <tr><td><img src="MSLDEimg/80x1.bmp"></td> <td><img src="MSLDEimg/14x1.bmp"></td></tr>
  <tr><td align=center>&beta;</td><td align=center>&beta;</td></tr>
  </table></td></tr>
</table>

<p>Since 0 &le; &beta; &le; F<sup>1/3</sup> &cong; 22 choose &beta;<sub>0</sub> = 10.
Linear prediction is used even if two successive values of &oslash;
do not differ in sign.
R<sup>2</sup> represents the square of the radical.</p>

<table>
<tr><td align=center>&beta;</td> <td rowspan=8></td> 
 <td align=center>d</td> <td rowspan=8>&nbsp;</td> 
 <td align=center>&gamma;</td> <td rowspan=8>&nbsp;</td>
<td align=center>&delta;</td> <td rowspan=8>&nbsp;</td>
<td align=center>R<sup>2</sup></td> <td rowspan=8>&nbsp;</td>
<td align=center>&alpha;</td> <td rowspan=8>&nbsp;</td>
<td align=center>&oslash;<br></td></tr>
<tr><td></td></tr>
<tr><td>10.</td> <td>1000.</td>      <td>900.</td> <td>-1.22</td> <td>1.68</td> <td>-2.52</td> <td>37.</td></tr>
<tr><td>12.</td> <td>&nbsp; 840.</td> <td>700.</td> <td>-1.57</td> <td>0.72</td> <td>-2.42</td> <td>&nbsp; 8.</td></tr>
<tr><td>12.5</td> <td>&nbsp; 806.1241</td> <td>649.8741</td> <td>-1.6913</td> <td>0.4863</td> <td>-2.3886</td> <td>&nbsp; 2.2605</td></tr>
<tr><td>12.7</td> <td>&nbsp; 793.4292</td> <td>632.1392</td> <td>-1.7387</td> <td>0.3741</td> <td>-2.3503</td> <td>&nbsp; 0.2354</td></tr>
<tr><td>12.7226</td> <td>&nbsp; 792.0198</td> <td>630.1552</td> <td>-1.7442</td> <td>0.3613</td> <td>-2.3453</td> <td>&nbsp; 0.0049</td></tr>
<tr><td>12.7231</td> <td>&nbsp; 791.9887</td> <td>630.1114</td> <td>-1.7443</td> <td>0.3610</td> <td>-2.3452</td> <td>&nbsp; 0.0002</td></tr>
</table>

<a name="5B"></a>
<p><b>B. Initial Approximations</b></p>

<a name="5B1"></a>
<p><b>1. Method of Search</b></p>

<p>Any point in the complex plane from which
one of the methods of section A will converge
to a particular zero of given polynomial
will be considered a satisfactory initial
approximation to that zero.
If the complex plane is covered with a set of points
on a sufficiently fine mesh
then at least one point of the set will be
sufficiently close to each zero 
of a given polynomial so as to serve
as a suitable initial approximation.
If bounds on the roots are known,
the number of mesh points to be considered
is finite and a program which will choose
in succession the points of this finite set
may therefore be used to provide
initial approximations.
Since the fineness of the mesh required 
is not generally known in advance it is advisable
to cover the region of interest first on a coarse mesh
and repeat on successively finer meshes
until all zeros are located.</p>

<p>This approach was employed in locating the zeros
of the truncated series for the exponential<sup><acronym
title="26. Iverson, K.E., “The Zeros of the Partial Sums of ez ”, Mathematical Tables and Other Aids to Computation 7 (1953).">26</acronym></sup>
namely</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
S<sub>n</sub>(z) = <img src="MSLDEimg/sigmak0n.bmp" align=middle> z<sup>k</sup>/k!
</td></tr></table>

<p>for values of n &le; 23.
Approximations were chosen on a rectangular mesh
and each was introduced as an initial approximation to
(a) the Birge-Vieta process if the point lay on the real axis, or
(b) the quadratic factor method if the point was complex.</p>

<p>This method was found to be extremely slow for values
of n &gt; 10.
In order to study the behavior of the process,
an optional routine was included in the program
to print out each successive approximation if desired.
This record of successive approximation showed 
two reasons for the slowness of the process.
(1) In general a very close first approximation
was required for convergence.
This meant tat the search had to be conducted
on a very fine mesh.
(2) The process would often follow blind leads
with the remainders continuing to decrease
for many iterations before finally diverging.
Because of the computing time required
this method was discarded as impractical.</p>

<a name="5B2"></a>
<p><b>2. Graeffe&rsquo;s Method</b><sup><acronym title="27. Lovitt, W.V., Elementary Theory of Equations, Prentice-Hall, New York 1939.">27
</acronym></sup></p>

<p>Let r<sub>1</sub>,r<sub>2</sub>,&#133;,r<sub>n</sub>
be the zeros of the polynomial</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
a<sub>0</sub>x<sup>n</sup> + 
a<sub>1</sub>x<sup>n-1</sup> +  &#133; + a<sub>n</sub> .
</td></tr></table>

<p>The coefficients are expressible in terms of the zeros
as follows:<sup><acronym 
title="16. Dickson, L.E., Theory of Equations, John Wiley and Sons, Inc., New York.">16</acronym></sup></p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td align=right>- a<sub>1</sub>/a<sub>0</sub></td>
<td>= r<sub>1</sub> + r<sub>2</sub> + &#133; + r<sub>n</sub></td>
 </tr>
<tr><td>&nbsp;</td>
<td align=right>a<sub>2</sub>/a<sub>0</sub></td>
<td>= r<sub>1</sub>r<sub>2</sub> + r<sub>2</sub>r<sub>3</sub> 
+ &#133; + r<sub>n-1</sub>r<sub>n</sub></td>
 </tr>
<tr><td>&nbsp;</td>
<td align=right>- a<sub>3</sub>/a<sub>0</sub></td>
<td>= r<sub>1</sub>r<sub>2</sub>r<sub>3</sub> 
+ &#133;</td>
<tr><td>&nbsp;</td>
<td align=right>(-1)<sup>n</sup> a<sub>n</sub>/a<sub>0</sub></td>
<td>= r<sub>1</sub>r<sub>2</sub>&#133;r<sub>n</sub> .</td>
 </tr>
</table>

<p>If the zeros are real and widely separated
it is clear that</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
<td colspan=3>- a<sub>1</sub>/a<sub>0</sub> 
&#x2250; r<sub>1</sub> ;</td>
 </tr>
<tr><td>&nbsp;</td>
<td>- a<sub>2</sub>/a<sub>1</sub> =
(a<sub>2</sub>/a<sub>0</sub>)(- a<sub>0</sub>/a<sub>1</sub>) =</td>
<td><table>
 <tr><td align=center>r<sub>1</sub>r<sub>2</sub>
  (1 + r<sub>1</sub>r<sub>3</sub>/r<sub>1</sub>r<sub>2</sub> + &#133; +
  r<sub>n-1</sub>r<sub>n</sub>/r<sub>1</sub>r<sub>2</sub>)</td></tr>
 <tr><td><img src="MSLDEimg/228x1.bmp"></td></tr>
 <tr><td align=center>r<sub>1</sub>
  (1 + r<sub>2</sub>/r<sub>1</sub> + r<sub>3</sub>/r<sub>1</sub> + &#133; +
  r<sub>n</sub>/r<sub>1</sub>)</td></tr>
</table></td>
<td>&#x2250; r<sub>2</sub> ;</td>
 </tr>
</table>

<p>and in general</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
- a<sub>k</sub>/a<sub>k-1</sub> &#x2250; r<sub>k</sub> .
</td></tr></table>

<p>Thus a polynomial with widely separated roots
breaks into a set of linear equations whose zeros
are approximately the zeros of the original polynomial.
More generally if a set of s zeros of equal magnitude occur
and the rest are widely separated
the polynomial breaks into (n&ndash;s) linear equations
and a cluster of (s+1) successive coefficients
defining a polynomial of degree s whose zeros are
(approximately) the s zeros of equal modulus.</p>

<p>The Graeffe process consists in generating
a sequence of polynomials such that the zeros
of any one of the sequence are the negative squares
of the zeros of the preceding polynomial.
Applications of the process k times raises
the original zeros to the 2<sup>k</sup> power.
Hence if all zeros are distinct a few iterations
serve to separate them widely.
If b<sub>0</sub>,b<sub>1</sub>,&#133;,b<sub>n</sub>
are coefficients of the polynomials so generated,
we have </p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
|b<sub>1</sub>/b<sub>0</sub>| &#x2250; 
|r<sub>1</sub><sup>2<sup>k</sup></sup>|; &nbsp; &nbsp;
|b<sub>2</sub>/b<sub>1</sub>| &#x2250; 
|r<sub>2</sub><sup>2<sup>k</sup></sup>|; &nbsp; etc.
</td></tr></table>

<p>Hence |r<sub>1</sub>|,|r<sub>2</sub>|,&#133;,|r<sub>n</sub>|
may be obtained. More generally if
r<sub>p+1</sub>,r<sub>p+2</sub>,&#133;,r<sub>p+s</sub>
are a set of s roots with equal moduli then
the polynomial</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>p</sub>y<sup>s</sup> + b<sub>p+1</sub>y<sup>s-1</sup> 
+ &#133; + b<sub>p+s</sub>
</td></tr></table>

<p>has the s zeros, 
r<sub>p</sub><sup>2<sup>k</sup></sup>,&#133;,r<sub>p+s</sub><sup>2<sup>k</sup></sup>.
Therefore</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
|b<sub>p+s</sub>/b<sub>p</sub>| =
<img src="MSLDEimg/pij1s.bmp" align=middle>
|r<sub>p+j</sub><sup>2<sup>k</sup></sup>|
 </td></tr>
<tr><td>and</td></tr>
<tr><td>&nbsp;</td><td>
|r<sub>p</sub>| = |r<sub>p+1</sub>| = &#133; = |r<sub>p+s</sub>| 
&#x2250;
[|b<sub>p+s</sub>/b<sub>p</sub>|]<sup>1/(2<sup>k</sup>s)</sup>.
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; 
</tt>(5.32)
</td></tr></table>

<p>Thus the moduli of all the roots are obtainable.</p>

<p>Certain refinements of Graeffe&rsquo;s method
allow the zeros 
r<sub>1</sub>,r<sub>1</sub>,&#133;,r<sub>n</sub>
themselves to be evaluated.
If, however, some of the zeros are complex this involves
essentially the solution of the polynomial equation</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>p</sub>y<sup>s</sup> + &#133; + b<sub>p-s</sub> = 0
</td></tr></table>

<p>and so leads back to the original problem
of solving a polynomial equation of high degree
if s is large.</p>

<p>The root-squaring process is obtained by rearranging
the polynomial equation<p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
a<sub>0</sub>x<sup>n</sup> + a<sub>1</sub>x<sup>n-1</sup>+ &#133; 
+ a<sub>n</sub> = 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
</tt>(5.33)
</td></tr></table>

<p>so that odd powers of x appear on one side of the equation
and even powers on the other.
Squaring this relation gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
(a<sub>0</sub>x<sup>n</sup> + a<sub>2</sub>x<sup>n-2</sup> + &#133;)<sup>2</sup>
= (a<sub>1</sub>x<sup>n-1</sup> + a<sub>3</sub>x<sup>n-3</sup> + &#133;)<sup>2</sup> .
</td></tr></table>

<p>The substitution y = -x<sup>2</sup> then gives</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>0</sub>y<sup>n</sup> + b<sub>1</sub>y<sup>n-1</sup> + &#133; 
+ b<sub>n</sub> = 0
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.34)
</td></tr></table>

<p>where</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
b<sub>0</sub> = a<sub>0</sub><sup>2</sup>,
 </td></tr>
<tr><td>&nbsp;</td><td>
b<sub>1</sub> = a<sub>1</sub><sup>2</sup> &ndash; 2a<sub>0</sub>a<sub>2</sub>,
  </td></tr>
<tr><td>&nbsp;</td><td>
b<sub>1</sub> = a<sub>1</sub><sup>2</sup> &ndash; 2a<sub>1</sub>a<sub>3</sub>
+ 2a<sub>0</sub>a<sub>4</sub>,
  </td></tr>
<tr><td colspan=2>and in general</td></tr>
<tr><td>&nbsp;</td><td>
b<sub>k</sub> = a<sub>k</sub><sup>2</sup> + 2
<img src="MSLDEimg/sigmaj1r.bmp" align=middle> 
(-1)<sup>j</sup>a<sub>k-j</sub>a<sub>k+j</sub> , &nbsp; &nbsp;
k = 0,1,2,&#133;,n,
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(5.35)
 </td></tr>
<tr><td colspan=2>where</td></tr>
<tr><td>&nbsp;</td><td>
r = min[k,(n&ndash;k)].
 </td></tr>
</table>

<p>Equation (5.35) defines the Graeffe root-squaring process
and the roots of (5.34) are the negative squares 
of the roots of (5.33).</p>

<a name="5C"></a>
<p><b>C. A General Method of Solution</b></p>

<p>The problem of finding all the zeros of a polynomial
of high degree is of very frequent occurrence
and a satisfactory general method of solution
is greatly to be desired.
The standard methods are unsatisfactory however
because of their failure in certain special cases.
For example, the Birge-Vieta and quadratic factor methods
may fail to converge unless a very good first approximation
is available;
the method of false position and Sturm&rsquo;s functions
fail to locate complex roots;
and methods such as that of 
Lin<sup><acronym 
title="28. Lin, S., “A method for finding roots of algebraic equations”, Journal of Mathematics and Physics, Mass. Inst. Tech. 22, (1943).">28</acronym></sup>
which are included under the process of penultimate
remaindering converge only under certain conditions
as established by Aitken<sup><acronym 
title="25. Aitken, A.C., Studies in Practical Mathematics, VII., “On the Theory of Methods of Factorizing Polynomials by Iterated Division”, Proc. Roy. Soc., Edinburgh, Vol. 63, p. 326 (1951).">25</acronym></sup>.
The method of Graeffe also fails in certain cases 
as pointed out in the previous section.</p>

<p>A mathematician using a desk computer is not seriously troubled
by these special cases since they may be fairly easily
recognized and an intelligent choice of a different method
usually resolves the difficulty.
Conceivably a computer may also be programmed to recognize
the special cases and choose a new method accordingly.
However, decisions of this type which may appear simple
to a human are exceedingly difficult to program
and a single general method is to be preferred.</p>

<p>Moore has considered this situation in some
detail<sup><acronym 
title="29. Moore, E.F., “A new general method for finding roots of polynomial equation”, Math. Tables and Other Aids to Computation 3, (1949).">29</acronym></sup>
and has suggested a possible general method of solution.
Moore&rsquo;s method is considered impractical
because of the amount of computation required.
The number of multiplications required is of the
order n<sup>3</sup>
(where n is the degree of the polynomial)
and each involves complex numbers represented
with a floating decimal point.</p>

<p>A general method of solution is now proposed
consisting of the following five steps:</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</td><td>
Apply the Graeffe process several times to determine
<table><tr><td width=28 nowrap>&nbsp;</td><td>
|r<sub>1</sub>|,|r<sub>2</sub>|,&#133;,|r<sub>n</sub>|
</td></tr></table>
approximately.
 </td></tr>
<tr><td valign=top>2.</td><td>
Make the substitution y = x&ndash;d and determine
the coefficients of the corresponding polynomial in y.
 </td></tr>
<tr><td valign=top>3.</td><td>
Re-apply the Graeffe process to determine
<table><tr><td width=28 nowrap>&nbsp;</td><td>
|r<sub>1</sub>&ndash;d|,|r<sub>2</sub>&ndash;d|,&#133;,|r<sub>n</sub>&ndash;d|.
</td></tr></table>
 </td></tr>
<tr><td valign=top>4.</td><td>
Compute in succession the points of intersection of the
two families of concentric circles;
the one centered at the origin and having radii
|r<sub>1</sub>|,|r<sub>2</sub>|,&#133;,|r<sub>n</sub>|,
the other at x = d and having radii
|r<sub>1</sub>&ndash;d|,|r<sub>2</sub>&ndash;d|,&#133;,|r<sub>n</sub>&ndash;d|.
 </td></tr>
<tr><td valign=top>5.</td><td>
Use the points so computed as initial approximations for 
(a) the Birge-Vieta process if the point is real, or
(b) the quadratic factor method if the point is imaginary.
 </td></tr>
</table>

<p>Since the zeros are known to lie on each of the two
families of circles described in step 4
it is clear that the first four steps alone
could provide solutions.
Such a method would however suffer from the 
following disadvantages:</p>

<table>
<tr><td valign=top nowrap>1. <sup>&nbsp;</sup></td><td>
The root-squaring process (requiring n<sup>2</sup>/2 multiplications)
may have to be repeated a large number of times
to evaluate |r<sub>1</sub>|,|r<sub>2</sub>|,&#133;,|r<sub>n</sub>|
to the accuracy required.
 </td></tr>
<tr><td valign=top>2.</td><td>
Further errors may be introduced in locating the intersection
of circles which cut obliquely.
 </td></tr>
<tr><td valign=top>3.</td><td>
Some of the intersections do not correspond to zeros
and must be rejected by some means of selection.
 </td></tr>
</table>

<p>If, however, the method is used only to supply 
initial approximations for step 5
the above disadvantages cause no difficulty.
For clearly, the intersections not corresponding
to zeros will be rejected and the genuine approximations
will be improved to the accuracy desired,
by a method requiring only 2n or 4n multiplications
per iteration and possessing quadratic convergence.</p>

<p>In general the Graeffe process should be repeated
only enough times that the given approximations will converge
to the desired zero.
No general rule can be given for attaining 
this<sup><a href="MSLDE1.htm#note5d">[d]</a></sup>
but suppose the accuracy required is such that
two zeros differing by less than one part 
in hundred need not be separated, i.e.
they will appear as a pair of roots with moduli
equal to the geometric mean of their true moduli.</p>

<p>Since (0.99)<sup>2<sup>9</sup></sup> &#x2250;
0.005 it is clear that two roots with moduli
in the ratio 0.99 will after 9 iterations have the ratio 0.005.
In practice fewer than nine iterations may well be used,
the number being increased
if difficulties are encountered.</p>

<p>The choice of the parameter d in shifting the origin
is arbitrary but should be made a function
of the moduli of the roots.
The choice of <nobr>d = r<sub>max</sub>/5</nobr>
has been used with success.
A more complex dependence on the moduli
of the roots may be indicated 
for certain types of polynomials.</p>

<p>Since one iteration of the Graeffe process
and a change of origin each require n<sup>2</sup>/2 
multiplications,
the whole process described 
(assuming k applications of the Graeffe process
and neglecting the computation of the intersections
of the circles) requires in all
<nobr>(k + &frac12;)n<sup>2</sup></nobr> multiplications.
This figure compares favorably with the minimum of
n<sup>2</sup> multiplications required 
simply to evaluate the polynomials 
at each of the n zeros.</p>

<p>Because of the wide range in magnitude of the numbers
derived from Graeffe&rsquo;s process
it is clear that floating point operation
will be required.
Floating point operation is probably to be desired
in any case since careful scaling of the coefficients
to avoid over-capacity numbers is rendered unnecessary.</p>

<p>The advantages offered by the proposed method may now be summarized:</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</td><td>
The method applies to any polynomial with real coefficients;
 </td></tr>
<tr><td valign=top>2.</td><td>
The method is uniform and only simple decisions need be provided for
in the program;
 </td></tr>
<tr><td valign=top>3.</td><td>
Initial approximations are not required;
 </td></tr>
<tr><td valign=top>4.</td><td>
No preliminary estimate of bounds on the roots is necessary;
 </td></tr>
<tr><td valign=top>5.</td><td>
No preliminary scaling of the coefficients is required;
 </td></tr>
<tr><td valign=top>6.</td><td>
The use of complex numbers is avoided;
 </td></tr>
<tr><td valign=top>7.<sup>&nbsp;</sup></td><td>
The number of multiplications required is of the order of n<sup>2</sup>.
 </td></tr>
</table>

<p>Further discussion of the method will be found in Chapter 6
where suitable checks and other programming details are considered.</p>

<a name="notes5"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top><a name="note5a"></a>a. &nbsp;</td><td>
Uniformity is desirable in automatic computation
since exceptions may require as much programming
as the main process.
 </td></tr>
<tr><td valign=top><a name="note5b"></a>b.</td><td>
A complex multiplication requires four real multiplications
and two additions.
 </td></tr>
<tr><td valign=top><a name="note5c"></a>c.</td><td>
The solution of the quartic given here appeared
in substantially the same form in Progress Report No. 29
of the Harvard Computation Laboratory under the title
&ldquo;Practical Methods of the Quartic and Sextic&rdquo;.
 </td></tr>
<tr><td valign=top><a name="note5d"></a>d.</td><td>
A detailed study of the behavior of the Graeffe process
has been made by 
Ostrowski<sup><acronym title="30. Ostrowskis, “Recherches sur la methode de Graeffe et les zeros des polynomes et des series de Laurent”, Acta Math. 72, p. 99-155.">30</acronym></sup>
but for our purposes the following rough analysis will suffice.
 </td></tr>
</table>
<br>&nbsp;<br>





<a name="6"></a>
<p><b>Chapter 6 &nbsp; Machine Programs</b></p>

<a name="6A"></a>
<p><b>A. Summary of Calculations</b></p>

<p>The general solution of the model proposed in Chapter 1
has been shown in Chapters 3 and 4 to be expressible 
in terms of the latent roots and principal vectors
of the matrix <nobr>(I - A)<sup>-1</sup>B.</nobr>
The particular solution for functions of the form
<nobr>z = ge<sup>&mu;t</sup></nobr> was shown
in section 3F to be expressible as
<nobr>x = (I - A - &mu;B)<sup>-1</sup>g.</nobr></p>

<p>The solution was obtained by the following steps:</p>

<table>
<tr><td valign=top nowrap>1. &nbsp;</td><td>
The matrices (I - A - &mu;B)<sup>-1</sup> were computed
for the desired range of values of &mu;, namely &mu; =
0, 0.015, 0.020, 0.025, 0.030, 0.035.
 </td></tr>
<tr><td valign=top>2. &nbsp;</td><td>
The matrix <nobr>(I - A)<sup>-1</sup></nobr> (obtained in (1) for &mu; = 0)
was pre-multiplied into B to obtain <nobr>(I - A)<sup>-1</sup>B</nobr>.
 </td></tr>
<tr><td valign=top>3. &nbsp;</td><td>
The Frame method (section 4C) was applied to the matrix
<nobr>(I - A)<sup>-1</sup>B</nobr> to generate the
characteristic equation and the matrices
F<sub>0</sub>,F<sub>1</sub>,&#133;,F<sub>n</sub>,
required in the calculation of the principal vectors.
 </td></tr>
<tr><td valign=top>4. &nbsp;</td><td>
The method of section 5C was applied to the solution 
of the characteristic equation.
 </td></tr>
<tr><td valign=top>5. &nbsp;</td><td>
The principal vectors of <nobr>(I - A)<sup>-1</sup>B</nobr> were evaluated 
by means of Eq. (4.11).
 </td></tr>
<tr><td valign=top>6. &nbsp;</td><td>
The &ldquo;modal&rdquo; matrix S formed of the principal vectors
was inverted.
 </td></tr>
</table>

<p>All programs were made to handle a system of general order
and in general the calculations for the six systems of order
5, 6, 10, 11, 20, 21 were run concurrently.
This mode of operation had the advantage that the programs
could be checked out on the low order system.</p>

<p>The programs employed will be examined in some detail
in the following sections.
The desirability of uniformity in machine programs discussed
in section 2A will be stressed throughout.</p>

<a name="6B"></a>
<p><b>B. Matrix Subroutines</b></p>

<a name="6B1"></a>
<p><b>1. Introduction</b></p>

<p>Since it was not clear at the outset what matrix methods
might be required,
it was decided to organize the programming around a number of
basic subroutines which were designed to carry out all of the more
important matrix operations likely to be encountered.
In this way any method to be employed would consist of a relatively
simple main program piecing together the subroutines required.
So far as possible the use of the subroutines should reduce
the programming of matrix operations to the simplicity 
of the programming of the corresponding algebraic operations.</p>

<p>In designing the subroutines a number of requirements
were laid down. These requirements are listed below:</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
The routines are to be completely general with respect to 
the order n (n &lt; 60).
 </td></tr>
<tr><td valign=top nowrap>2. &nbsp;</td><td>
Every matrix is to be stored so as to include a sum check
for each row and column and must be stored in some standard way
so that the location of any row many be completely specified
by the row number and the location of the first element 
of the matrix.
 </td></tr>
<tr><td valign=top nowrap>3. &nbsp;</td><td>
The ten fast storage registers 190-199 are reserved
for certain control numbers (e.g., n is stored in 199)
and other uses within the routines.
 </td></tr>
<tr><td valign=top nowrap>4. &nbsp;</td><td>
The remaining fast storage registers 0-189 are not 
to be used except as directed by the main program.
 </td></tr>
<tr><td valign=top nowrap>5. &nbsp;</td><td>
The operations required in calling the routine should
be as simple as possible.
 </td></tr>
<tr><td valign=top nowrap>6. &nbsp;</td><td>
Each routine should be as flexible as possible.
For example, a single routine should be capable of
recording on tape either a single specified row of a matrix
or of recording the specified row 
followed by all the succeeding rows;
the choice of operation to be made when the routine is called.
 </td></tr>
<tr><td valign=top nowrap>7. &nbsp;</td><td>
All checks are, so far as possible, to be completed
in the main program and not in the subroutines.
 </td></tr>
<tr><td valign=top nowrap>8. &nbsp;</td><td>
Since they are expected to be used repeatedly
the subroutines are to be made as efficient as possible
in terms of computing time.
 </td></tr>
</table>

<p>Since the fast storage registers can accommodate
a matrix of order fifteen at most,
the first requirement makes the use 
of slow storage registers necessary.</p>

<p>A vector x = [x<sub>1</sub>,x<sub>2</sub>,&#133;,x<sub>n</sub>]
will be said to be stored in location b if the components
x<sub>1</sub>,x<sub>2</sub>,&#133;,x<sub>n</sub> are stored
respectively in the registers 
b+1,b+2,&#133;,b+n.
The negative sum of the components will be designated by
x<sub>0</sub>, and will be stored in register b.
In this way the location of the vector may be completely specified
by the order n and the number b.
Also each vector carries with it its own check since
the sum of the elements 
x<sub>0</sub>,x<sub>1</sub>,x<sub>2</sub>,&#133;,x<sub>n</sub>
must be zero.
The use of the negative sum makes the process of summing the check
more uniform.</p>

<p>Each row of a matrix A = [a<sub>ij</sub>] may be considered
as a vector and will be stored as indicated above.
The collection of negative row sums then forms a &ldquo;zeroth&rdquo;
column of the matrix.
In the same way negative column sums are included as a zero row
and finally the negative sum of this row completes the
(n+1)&times;(n+1) square array:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td rowspan=5><img src="MSLDEimg/matrixl5.bmp"></td>
 <td>a<sub>00</sub>&nbsp;</td> <td>a<sub>01</sub>&nbsp;</td> 
 <td>a<sub>02</sub>&nbsp;</td>
 <td>&#133; &nbsp;</td> <td>a<sub>0n</sub></td>
 <td rowspan=5><img src="MSLDEimg/matrixr5.bmp"></td></tr>
<tr><td>a<sub>10</sub></td> <td>a<sub>11</sub></td> <td>a<sub>12</sub></td>
 <td>&#133;</td> <td>a<sub>1n</sub></td></tr>
<tr><td>a<sub>20</sub></td> <td>a<sub>21</sub></td> <td>a<sub>22</sub></td>
 <td>&#133;</td> <td>a<sub>2n</sub></td></tr>
<tr><td>&#133;</td> <td>&#133;</td> <td>&#133;</td>
 <td>&#133;</td> <td>&#133;</td></tr>
<tr><td>a<sub>n0</sub></td> <td>a<sub>n1</sub></td> <td>a<sub>n2</sub></td>
 <td>&#133;</td> <td>a<sub>nn</sub></td></tr>
</table></td></tr></table>

<p>where</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>a<sub>0i</sub> = &ndash;</td>
 <td><img src="MSLDEimg/sigmaj0n.bmp"></td>
 <td>a<sub>ji</sub>, <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt></td>
 <td>i = 0,1,2,&#133;,n;</td></tr>
<tr><td></td></tr>
<tr><td>a<sub>i0</sub> = &ndash;</td>
 <td><img src="MSLDEimg/sigmaj0n.bmp"></td>
 <td>a<sub>ij</sub>,</td>
 <td>i = 0,1,2,&#133;,n.</td></tr>
</table></td></tr></table>

<p>Note that</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>a<sub>00</sub> = &nbsp;</td>
 <td><img src="MSLDEimg/sigmaij.bmp"></td>
 <td>a<sub>ij</sub></td></tr>
</table></td></tr></table>

<p>by either definition above.</p>

<p>Sum checks prove extremely useful in linear operations
and for this reason all matrices
(including and output on magnetic tape)
are stored as indicated above.</p>

<p>The simplest use of the sum-check is for checking
the components of a vector each time it enters into the computation.
As an example of other ways in which the sum-check may be used,
consider the matrix multiplication</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y = Ax
</td></tr></table>

<p>which is normally defined as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
y<sub>i</sub> =</td>
<td><img src="MSLDEimg/sigmajn.bmp"></td>
<td>a<sub>ij</sub>x<sub>j</sub>,
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>
i = 1,2,&#133;,n.
</td></tr></table>

<p>Since the matrix A is stored with a zeroth row 
this definition may be extended to
<nobr>i = 0,1,2,&#133;,n.</nobr> Then</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td><img src="MSLDEimg/sigmai0n.bmp"></td>
 <td>y<sub>i</sub> =</td>
 <td><img src="MSLDEimg/sigmai0n.bmp"></td>
 <td colspan=2><img src="MSLDEimg/sigmajn.bmp"></td>
 <td colspan=3>a<sub>ij</sub> x<sub>j</sub></td></tr>
<tr><td>&nbsp;</td><td align=right>=</td>
 <td><img src="MSLDEimg/sigmajn.bmp"></td>
 <td><img src="MSLDEimg/matrixl2.bmp"></td>
 <td><img src="MSLDEimg/sigmai0n.bmp"></td>
 <td>a<sub>ij</sub></td>
 <td><img src="MSLDEimg/matrixr2.bmp"></td>
 <td>x<sub>j</sub>.</td></tr>
</table></td></tr></table>

<p>The quantity in parentheses is zero for each j
so that finally</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td><img src="MSLDEimg/sigmai0n.bmp"></td>
 <td>y<sub>i</sub> = 0.
 <tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> 
 (6.1)</td></tr>
</table>

<p>This last is an identity which checks the whole matrix multiplication.
Note the complete uniformity of the operations.</p>

<p>In machine calculation the round-off error may cause
a discrpancy of as much as (n+1) in the last column
in the identity (6.1).
Hence a tolerance of (n+1) should be allowed in checking.
It should, however, be standard practice to then
&ldquo;correct&rdquo; the sum y<sub>0</sub> so that</p>

<table><tr><td width=28 nowrap>&nbsp;</td>
 <td><img src="MSLDEimg/sigmai0n.bmp"></td>
 <td>y<sub>i</sub> = 0.</td></tr>
</table>

<p>In this way the only error to be allowed at each step
is the maximum round-off error of that step.
Otherwise the error accumulates and it becomes difficult
to estimate what tolerance should be allowed.</p>

<p>A matrix may be stored most compactly by assigning
the first element to the register following the last element
of the proceeding row.
It is necessary, however, to space the rows in slow storage
so that the transfer of a given row to slow storage will not
destroy numbers of the following 
row.<sup><a href="MSLDE1.htm#note6a">[a]</a></sup>
The rth row of a matrix A beginning at location b
(which stores a<sub>00</sub>) is therefore stored 
at location b+rs where s is the minimum number of blocks
of then registers required to store
(n+2)<sup><a href="MSLDE1.htm#note6b">[b]</a></sup>
numbers.  The number s is computed from n
and is stored in register 198.</p>

<p>To illustrate the foregoing consider the matrix A
stored at slow storage location 1830 where</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr>
 <td rowspan=3>A =</td>
 <td rowspan=3><img src="MSLDEimg/matrixl3.bmp"></td>
 <td>1 &nbsp; &nbsp;</td><td>2 &nbsp;</td><td align=right>&ndash; 4</td>
 <td rowspan=3><img src="MSLDEimg/matrixr3.bmp"></td>
 <td rowspan=3>.</td></tr>
<tr><td>0</td> <td>3</td> <td align=right>5</td></tr>
<tr><td>1</td> <td>1</td> <td align=right>2</td></tr>
</table></td></tr></table>

<p>Then n = 3, s = 1 and the complete matrix is stored as shown
below where the numerical quantities on the left of the arrow
are stored in the corresponding registers indicated
on the right.</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr>
 <td rowspan=4><img src="MSLDEimg/matrixl3.bmp"></td>
 <td align=right>11 &nbsp;</td> <td align=right>&ndash; 2 &nbsp;</td>
 <td>&ndash; 6 &nbsp;</td> <td>&ndash; 3</td>
 <td rowspan=4><img src="MSLDEimg/matrixr3.bmp"></td>
 <td rowspan=4>&nbsp; &rarr; &nbsp;</td>
 <td rowspan=4><img src="MSLDEimg/matrixl3.bmp"></td>
 <td>1830 &nbsp;</td> <td>1831 &nbsp;</td> <td>1832 &nbsp;</td> <td>1833</td>
 <td rowspan=4><img src="MSLDEimg/matrixr3.bmp"></td>
 </tr>
<tr><td align=right>1 &nbsp;</td>   <td align=right>1 &nbsp;</td> <td align=right>2 &nbsp;</td> <td align=right>&ndash; 4</td>
 <td>1840 &nbsp;</td> <td>1841 &nbsp;</td> <td>1842 &nbsp;</td> <td>1843</td></tr>
<tr><td align=right>&ndash; 8 &nbsp;</td> <td align=right>0 &nbsp;</td> <td align=right>3 &nbsp;</td> <td align=right>5</td>
 <td>1850 &nbsp;</td> <td>1851 &nbsp;</td> <td>1852 &nbsp;</td> <td>1853</td></tr>
<tr><td align=right>&ndash; 4 &nbsp;</td> <td align=right>1 &nbsp;</td> <td align=right>1 &nbsp;</td> <td align=right>2</td>
 <td>1860 &nbsp;</td> <td>1861 &nbsp;</td> <td>1862 &nbsp;</td> <td>1863</td></tr>
</table></td></tr></table>

<p>The fast storgage registers 190-199 are utilized as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td>Register &nbsp;</td> <td>Contents</td></tr>
<tr><td></td></tr>
<tr><td>&nbsp; 190</td> <td>Unassigned</td></tr>
<tr><td>&nbsp; 191</td> <td>Unassigned</td></tr>
<tr><td>&nbsp; 192</td> <td>Serial</td></tr>
<tr><td>&nbsp; 193</td> <td>Page control</td></tr>
<tr><td>&nbsp; 194</td> <td>Row number</td></tr>
<tr><td>&nbsp; 195</td> <td>10 in low order columns</td></tr>
<tr><td>&nbsp; 196</td> <td>1 &nbsp; in low order columns</td></tr>
<tr><td>&nbsp; 197</td> <td>Column number</td></tr>
<tr><td>&nbsp; 198</td> <td>s</td></tr>
<tr><td>&nbsp; 199</td> <td>n</td></tr>
</table></td></tr></table>

<p>In order to meet the requirement of flexibility without increasing
the number of orders required in calling the subroutine,
the following device is used.
If a certain control number (e.g., the row number r)
is essentially positive then the positive absolute value
may be used where necessary in the routine.
Thus in calling the subroutine this control number
may be entered either positivelly or negatively,
and at some point in the subroutine the sign of the number
may be used to make a choice between two alternative operations.
Examples will be found in the discussion 
of the individual subroutines.</p>

<p>Although checks are prepared in the subroutine
(e.g., the components of a vector transferred from slow storage
will be summed), the final check is left to the main program
if possible.
This makes it possible to program reruns instead of check-stops,
if desired, and also makes it easier to determine at what point
in the main program a failure has occurred.</p>

<a name="6B2"></a>
<p><b>2. Floating Vector Operation</b></p>

<p>Floating point operation (see section 2A) offers decided 
advantages in reducing round-off error and in avoiding
overflow.<sup><a href="MSLDE1.htm#note6c">[c]</a></sup>
&#133;
</p>

<a name="6B3"></a>
<p><b>3. Matrix Programs</b></p>

<a name="6C"></a>
<p><b>C. Matrix Inversions</b></p>

<a name="6C1"></a>
<p><b>1. A New Machine Method</b></p>

<a name="6C2"></a>
<p><b>2. Inversion of a Complex Matrix</b></p>

<a name="6D"></a>
<p><b>D. The Method of Frame</b></p>

<a name="6E"></a>
<p><b>E. Solution of the Characteristic Equation</b></p>

<a name="6E1"></a>
<p><b>1. The Graeffe Process</b></p>

<a name="6E2"></a>
<p><b>2. The Quadratic Factor Method</b></p>

<a name="6F"></a>
<p><b>F. The Principal Vectors</b></p>

<p>If the Frame method (section 4C) is applied to a matrix A
to obtain the matrices F<sub>0</sub>,F<sub>1</sub>,&#133;,F<sub>n</sub>,
then Eq. (4.11) may be used to evaluate the adjoint 
&Delta;(A&nbsp;-&nbsp;&lambda;I)
of the matrix A&nbsp;-&nbsp;&lambda;I. 
Denoting the ith column of the matrix F<sub>j</sub> by f<sub>ij</sub>,
the ith column &nu;<sub>i</sub>(&lambda;) of the adjoint
is given by</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&nu;<sub>i</sub>(&lambda;) = 
- [&lambda;<sup>n-1</sup>f<sub>i0</sub> + 
&lambda;<sup>n-2</sup>f<sub>i1</sub> + &#133; +
f<sub>i,n-1</sub>] .
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>
(6.10)
</td></tr></table>

<p>If &lambda;<sub>k</sub> is a latent root of the matrix A,
any non-trivial columns of the adjoint
&Delta;(A&nbsp;-&nbsp;&lambda;I)
are proportional to the associated eigenvector.
Hence it usually suffices to compute only a single column
&nu;<sub>i</sub>(&lambda;<sub>k</sub>) for each
distinct root &lambda;<sub>k</sub>.
If multiple roots occur the required number of 
independent columns of the derived adjoint may be computed
by differentiating Eq. (6.10).</p>

<p>The polynomial (6.10) may be evaluated most efficiently
by grouping the operations as follows:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&nu;<sub>i</sub>(&lambda;<sub>k</sub>) = 
- (&#133;((&lambda;<sub>k</sub>f<sub>i0</sub> + f<sub>i1</sub>)&lambda;<sub>k</sub>
+ f<sub>i2</sub>)&lambda;<sub>k</sub> + &#133; + 
f<sub>i,n-1</sub>) .
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>
(6.11)
</td></tr></table>

<p>This is simply an extension of the synthetic division process
to a polynomial with vector coefficients.
Because of the extreme range of magnitudes to be encountered
in the quantities f<sub>ij</sub><sup><a href="MSLDE1.htm#note6s">[s]</a></sup>
the calculation requires floating point operation.
The roots &lambda;<sub>k</sub> in general are complex
so that floating point complex operation is required.
Such operation is extremely slow and may be avoided
by the use of floating vector operation as follows:
The root &lambda;<sub>k</sub> is first put in the form
&lambda;<sub>k</sub> = r<sub>k</sub>&mu;<sub>k</sub>
where r<sub>k</sub> is real and |&mu;<sub>k</sub>| = 1.
Equation (6.11) may now be written as</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
&nu;<sub>i</sub>(&lambda;<sub>k</sub>) =  
- (&#133;((r<sub>k</sub><sup>n-1</sup>f<sub>i0</sub>)&mu;<sub>k</sub>
+ r<sub>k</sub><sup>n-2</sup>f<sub>i1</sub>)&mu;<sub>k</sub> 
+ &#133; + f<sub>i,n-1</sub>) .
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt>
(6.12)
</td></tr></table>

<p>The powers of r<sub>k</sub> are formed using
floating point operation and the vectors
r<sub>k</sub><sup>n-1-j</sup>f<sub>ij</sub>,
<nobr>j = 0,1,&#133;,n-1,</nobr> are computed using floating point
vector operation as described in Section A.
If the largest exponent of this set of vectors is p
then p+2 is subtracted from each 
exponent.<sup><a href="MSLDE1.htm#note6t">[t]</a></sup>
Thus the most significant quantities in the expression (6.12)
will be of the order of 1 in the fourteenth machine column.
Each vector may therefore be shifted by the amount indicated
by the corresponding exponent and the subsequent calculations
carried out at a fixed decimal point located to the left
of the highest order column.
Since |&mu;<sub>k</sub>| = 1 the subsequent operations
will not cause the occurrence of over-capacity numbers.</p>

<a name="notes6"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top nowrap><a name="note6a"></a>a. &nbsp;</td><td>
If for example n = 6 the transfer of a vector to slow storage
registers 0-6 also affects registers 7, 8 and 9 since numbers
are transferred in groups of ten.
This difficulty might be avoided by preceding each transfer
to slow storage by a transfer from slow storage.
In this way only the slow storage transfer registers 
which are changed in the interim will cause any change in
slow storage.
This would allow more efficient use of storage
at a cost of some computing time.
 </td></tr>
<tr><td valign=top nowrap><a name="note6b"></a>b. &nbsp;</td><td>
An extra location is reserved for an &ldquo;exponent&rdquo;
as described in section 6B2.
 </td></tr>
<tr><td valign=top nowrap><a name="note6c"></a>c. &nbsp;</td><td>
The occurrence of over-capacity numbers.
 </td></tr>
<tr><td valign=top nowrap><a name="note6d"></a>d. &nbsp;</td><td>
Except for the &ldquo;external transfer&rdquo; orders
which require a simple translation from the printer output.
In the following programs this translation has been made.
 </td></tr>
<tr><td valign=top nowrap><a name="note6e"></a>e. &nbsp;</td><td>
Note the assumption here that y is a multiple of 10 (y &equiv; 0 mod 10).
Hence the limitation noted in Table 0.
 </td></tr>
<tr><td valign=top nowrap><a name="note6f"></a>f. &nbsp;</td><td>
See &ldquo;Call and Conditional Call&rdquo;, section 2B.
 </td></tr>
<tr><td valign=top nowrap><a name="note6g"></a>g. &nbsp;</td><td>
High accuracy addition is used to avoid possible overflow
which may occur in the course of addition even though the final sum
is known to be within capacity.
The read to and from the condition register is to provide a zero
of the proper sign for the high order part of the number
to be added.
 </td></tr>
<tr><td valign=top nowrap><a name="note6h"></a>h. &nbsp;</td><td>
Subject to the limitation x &equiv; 0 mod 10.
 </td></tr>
<tr><td valign=top nowrap><a name="note6i"></a>i. &nbsp;</td><td>
If a sum of n products is rounded off after summing,
the maximum error is one in the last column.
If each product is rounded off before summing,
the maximum error is n in the last column.
 </td></tr>
<tr><td valign=top nowrap><a name="note6j"></a>j. &nbsp;</td><td>
The largest component before normalization then approaches 
the latent root.
 </td></tr>
<tr><td valign=top nowrap><a name="note6k"></a>k. &nbsp;</td><td>
The number of orders must not exceed twenty-three.
See &ldquo;Call and Conditional Call&rdquo;, section 2B.
 </td></tr>
<tr><td valign=top nowrap><a name="note6l"></a>l. &nbsp;</td><td>
Forsythe<sup><acronym title="Forsythe, George E., “Theory of Selected Methods of Finite Matrix Inversion and Decomposition”, Lecture in Math. 136, notes by D.G. Aronson and K. Iverson. INA Report 52-55 (1951).">32</acronym></sup>
has suggested the use of the pivot element whose absolute value
is nearest the quantity <sup>n</sup>&radic;|A|.
The application of this method of course requires a knowledge
of the determinant |A|.
 </td></tr>
<tr><td valign=top nowrap><a name="note6m"></a>m. &nbsp;</td><td>
The solution may of course be obtained by inverting A and computing
A<sup>-1</sup>c.
This is, however, less efficient than the process here described.
 </td></tr>
<tr><td valign=top nowrap><a name="note6n"></a>n. &nbsp;</td><td>
From the example of section 5C it is seen that if the moduli
of two roots are in the ratio 0.99 the cross terms should,
after nine iterations, be reduced to about .005
of the value of the square term.
 </td></tr>
<tr><td valign=top nowrap><a name="note6o"></a>o. &nbsp;</td><td>
The Graeffe process must be carried out with floating point operation.
See section 2A.
 </td></tr>
<tr><td valign=top nowrap><a name="note6p"></a>p. &nbsp;</td><td>
Multiplicity is used here in the sense of roots of equal moduli.
 </td></tr>
<tr><td valign=top nowrap><a name="note6q"></a>q. &nbsp;</td><td>
See section 5C for the definition of the circles.
 </td></tr>
<tr><td valign=top nowrap><a name="note6r"></a>r. &nbsp;</td><td>
If m = 0 the new root is still divided out once as a final check.
 </td></tr>
<tr><td valign=top nowrap><a name="note6s"></a>s. &nbsp;</td><td>
For example in the case of the system of order 20
the components of f<sub>il</sub> are of the order of unity
while the components of f<sub>i,18</sub> are of the
order of 10<sup>-17</sup>.
However, the vector f<sub>i18</sub> may not be considered
negligible since it becomes a dominant term 
for small values of the root &lambda;<sub>k</sub>.
 </td></tr>
<tr><td valign=top nowrap><a name="note6t"></a>t. &nbsp;</td><td>
Any scalar multiple of an eigenvector is also an eigenvector
so that multiplication by 10<sup>-p-2</sup> is a valid operation.
 </td></tr>
</table>
<br>&nbsp;<br>





<a name="7"></a>
<p><b>Chapter 7 &nbsp; Results and Conclusions</b></p>

<p>The tables of initial data and final results
appear in Appendices I-VII and are described
in section A of this chapter.
The chapter is concluded with remarks
of a general nature in section B.</p>

<a name="7A"></a>
<p><b>A. Table of Results</b></p>

<p>Equation (1.2) of Chapter 1 which describes
the economic model under consideration is repeated here
for reference:</p>

<table><tr><td width=28 nowrap>&nbsp;</td><td>
x<sub>i</sub> &ndash;<img src="MSLDEimg/sigmajn.bmp" align=middle> a<sub>ij</sub>x<sub>j</sub> 
&ndash;<img src="MSLDEimg/sigmajn.bmp" align=middle> b<sub>ij</sub> <i>d</i>x<sub>j</sub>/<i>d</i>t
= z<sub>i</sub> ;
&nbsp; &nbsp; i = 1,2,&#133;,n.
<tt>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</tt> 
(1.2)
</td></tr></table>

<p>As described in Chapter 1 the variables 
x<sub>1</sub>,x<sub>2</sub>,&#133;x<sub>n</sub>
represent the outputs of each of n industries making up
a given economy.
Table 27 of Appendix VII shows the industry 
classification<sup><a href="MSLDE1.htm#note7a">[a]</a>&nbsp;</sup>
adopted for three systems of order 21, 11 and 6.
For the 21 industry classification the numbers 1-21
indicate which of the variables x<sub>1</sub>-x<sub>21</sub>
are assigned to represent each industry;
e.g. x<sub>5</sub> represents the output 
of the chemical industry.
For the 11 and 6 industry classifications 
the converging lines indicate 
which industries of the 21 country classification
are grouped together to be represented 
as a single industry in the lower order classification.</p>

<p>Three open<sup><a href="MSLDE1.htm#note7b">[b]</a>&nbsp;</sup>
systems of order 20, 10 and 5 are derived
from the foregoing systems by suppressing in each
the variable representing households
and considering it instead as a final demand
on the remaining system.</p>

<p>Tables 2-26 contain square matrices of various orders.
Except in Tables 8-13 of Appendix III
where the row and column number of each element
is shown directly, the columns of the table represent
the columns of the corresponding matrix.
Because sum checks prove as useful in hand computation
as in machine computation, each matrix is recorded
with a zeroth row containing the negative sums 
of the columns and a zeroth column containing 
the negative sums of the rows as described 
in section 6B1.</p>

<p>The matrix A = [a<sub>ij</sub>] of flow coefficients
derived<sup><a href="MSLDE1.htm#note7a">[a]</a>&nbsp;</sup>
from data supplied by the Bureau of Labor Statistics
is shown in Appendix I.
The corresponding matrix of capital coefficients
B = [b<sub>ij</sub>], supplied by the 
Harvard Economic Research Project,
appears in Appendix II.
The broken lines in each of these tables (2-7)
indicate the submatrix of coefficients
which is retained in the open systems considered.
The column and row sums must of course
be adjusted for these systems.</p>

<p>Four decimal places were given in the initial data
(Tables 2-7) and all results are rounded to four decimal places.
Twelve decimal places were carried in the calculation
of Tables 8-19.
A final check on the results shown in Tables 20-24
and the agreement obtained are described
at the end of this section.</p>

<p>To avoid repetition,
the use of the tables in the solution 
of the economic model will be described
only in terms of the system of order five
and all references to tables will be
to the fifth order system.</p>

<p>Expressed in matrix notation and specialized
to the particular final demand functions to be considered,
Eq. (1.2) appears as</p>

<table>
<tr><td nowrap width=28>&nbsp;</td>
<td>(I &ndash; A)x &ndash; B<img src="MSLDEimg/xdot.bmp"> = ge<sup>&mu;t</sup> .
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(7.1)</td></tr>
</table>

<p>The general solution of Eq. (7.1) is a sum of</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
A particular integral (section 3F) of the form
<table>
<tr><td width=28 nowrap>&nbsp;</td><td>
x = [I &ndash; A &ndash; &mu;B]<sup>-1</sup>ge<sup>&mu;t</sup>, and
</td></tr></table>
 </td></tr>
<tr><td valign=bottom>2.</td><td>
A complementary function of the form x = Se<sup>J<sup>-1</sup>t</sup>c .
 </td></tr>
</table>

<p>(See Eqs. (3.31) and (3.12).)  The general solution is therefore</p>

<table>
<tr><td width=28 nowrap>&nbsp;</td><td>
x = [I &ndash; A &ndash; &mu;B]<sup>-1</sup>ge<sup>&mu;t</sup> + Se<sup>J<sup>-1</sup>t</sup>c
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(7.2)
</td></tr></table>

<p>where g is a given vector determining the demand function
and c is a vector to be determined by the initial conditions
<img src="MSLDEimg/xbar.bmp"> = x(0). Thus</p>

<table>
<tr><td width=28 nowrap>&nbsp;</td><td>
<img src="MSLDEimg/xbar.bmp"> = (I &ndash; A &ndash; &mu;B)<sup>-1</sup>g + Sc
</td></tr>
</table>

and

<table>
<tr><td width=28 nowrap>&nbsp;</td><td>
c = S<sup>-1</sup> [<img src="MSLDEimg/xbar.bmp"> &ndash; (I &ndash; A &ndash; &mu;B)<sup>-1</sup>g].
<tt>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
</tt>(7.3)
</td></tr>
</table>

<p>The matrix<sup><a href="MSLDE1.htm#note7c">[c]</a>&nbsp;</sup> S
is given in Table 20, the inverse S<sup>-1</sup>
in Table 25, and the matrix <nobr>[I &ndash; A &ndash; &mu;B]<sup>-1</sup></nobr>
appears in Table 8 for various values of &mu;.
Thus for a particular final demand specified by given values
of &mu; and g, and a given initial condition <img src="MSLDEimg/xbar.bmp">
it is possible to compute the vector c from Eq. (7.3).</p>

<p>The only quantity now left undefined in Eq. (7.2)
is the matrix J.
Let us denote the kth eigenvalue of the matrix
<nobr>(I &ndash; A)<sup>-1</sup>B</nobr> by &lambda;<sub>k</sub>. 
Then &lambda;<sub>k</sub> appears as the number
at the head of the kth column of the matrix S in Table 20.
The matrix J is then a diagonal matrix with elements
j<sub>kk</sub> = &lambda;<sub>k</sub>.
Thus J<sup>-1</sup> is a diagonal matrix with
&gamma;<sub>k</sub> = 1/&lambda;<sub>k</sub>
and hence e<sup>J<sup>-1</sup>t</sup> is given by</p>

<table>
<tr><td width=28 nowrap>&nbsp;</td><td><table>
<tr><td rowspan=5>e<sup>J<sup>-1</sup>t</sup> =</td>
 <td rowspan=5><img src="MSLDEimg/matrixl5.bmp"></td>
 <td>e<sup>&gamma;<sub>1</sub>t</sup></td> 
  <td>&nbsp;</td> <td>&nbsp;</td> <td>&nbsp;</td> <td>&nbsp;</td>
 <td rowspan=5><img src="MSLDEimg/matrixr5.bmp"></td>
 <td rowspan=5>.</td>
 </tr>
<tr><td>&nbsp;</td><td>e<sup>&gamma;<sub>2</sub>t</sup></td>
  <td>&nbsp;</td> <td>&nbsp;</td> <td>&nbsp;</td> </tr>
<tr><td colspan=2>&nbsp;</td><td>e<sup>&gamma;<sub>3</sub>t</sup></td>
  <td>&nbsp;</td> <td>&nbsp;</td> </tr>
<tr><td colspan=3>&nbsp;</td><td>e<sup>&gamma;<sub>4</sub>t</sup></td>
  <td>&nbsp;</td> </tr>
<tr><td colspan=4>&nbsp;</td><td>e<sup>&gamma;<sub>5</sub>t</sup></td>
  </tr>
</table></td></tr>
</table>

<p>Finally e<sup>J<sup>-1</sup>t</sup>c is the column vector</p>

<table>
<tr><td width=28 nowrap>&nbsp;</td><td>
[ c<sub>1</sub>e<sup>&gamma;<sub>1</sub>t</sup>,
c<sub>2</sub>e<sup>&gamma;<sub>2</sub>t</sup>, &#133;,
c<sub>5</sub>e<sup>&gamma;<sub>5</sub>t</sup> ].
</td></tr>
</table>

<p>If one of the latent roots &lambda;<sub>k</sub>
is zero then the results must be interpreted
according to section 3D as follows:
The corresponding component c<sub>k</sub>
of the vector c is made zero.
This in turn impose a linear restraint on
the initial condition <img src="MSLDEimg/xbar.bmp"> 
according to Eq. (7.3).
For if S<sup>-1</sup><sub>k</sub> is the kth row 
of the matrix S<sup>-1</sup> then</p>

<table>
<tr><td width=28 nowrap>&nbsp;</td><td>
c<sub>k</sub> = S<sup>-1</sup><sub>k</sub> 
[<img src="MSLDEimg/xbar.bmp"> &ndash; (I &ndash; A &ndash; &mu;B)<sup>-1</sup>g] = 0.
</td></tr>
</table>

<p>The complete system of eigenvectors and latent roots
is given for the systems of order 5, 6, 10 and 11.
To each complex conjugate pair of latent roots
there corresponds a conjugate pair of eigenvectors.
These pairs appear in Appendix II as double columns
separated by the symbol &plusmn;i.
Partial solutions were also obtained for the system
of order 20 but due to round-off error accumulation
the complete set was not obtained to satisfactory accuracy.
The final check used on the eigenvectors &nu;<sub>k</sub>
was the identity [A&ndash;&lambda;<sub>k</sub>]&nu;<sub>k</sub> = 0.
For all eigenvectors show in Appendix V
agreement to six or more places was obtained.</p>

<a name="7B"></a>
<p><b>B. Conclusions</b></p>

<p>The results of the present study show the feasibility
of obtaining general solutions of large order systems
of linear differential equations.
The rapid accumulation of round-off error indicates
the need for maintaining rather extreme accuracy
in the calculations and suggests that the rate of
error accumulation in a given process is of major importance.
The method of solution of polynomial equations
(proposed in section 5C) has proven satisfactory
for polynomials of degree twenty and less.
In spite of the rather large amount of calculation
required by the root-squaring process
it is believed that in the absence of initial approximations
to the roots this is perhaps the most satisfactory method
available for equations of high degree.</p>

<p>The matrix subroutines described in section 6B
have proven very valuable in reducing problem preparation
time in the present work.
However, this does not necessarily imply that 
the subroutines possess practical value for general use.
There are two major reasons for this situation:</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
 In using a subroutine the intimate knowledge
 possessed by the original programmer gives him
 a decided advantage over a programmer who wishes
 to use the subroutine according to simple written
 instructions without learning the program in detail.
 </td></tr>
<tr><td valign=top>2.</td><td>
 In case of trouble in checking out a program 
 the mathematician may be at a serious disadvantage
 if his program includes subroutines are unfamiliar to him.
 </td></tr>
</table>

<p>As evidence of the practicability of the matrix subroutines
for general use 
the present routines have been used with profit 
and a minimum of difficulty by another member
of the Staff of the Computation Laboratory 
in the solution of a vibration 
problem.<sup><acronym title="34. Staff of the Computation Laboratory, “Problems of Critical Frequency for a Propeller Blade Subjected to a Pure First Order Aerodynamic Excitation” Harvard Computation Laboratory, Progress Report No. 29, Section I, November 1953.">34</acronym>&nbsp;</sup>
Indeed, many of the difficulties in the use
of subroutines arise in possible conflicts
in the use of sequence or number storage
and these in turn stem in part from the rather limited
storage capacity of most computers.
The advantage of large storage capacity
and of separate sequence and number storage
combine to make the efficient use of a library
of subroutines a practical possibility for Mark IV.</p>

<p>As a result of the experience gained in their use
a number of changes in the subroutines are now proposed.</p>

<table>
<tr><td valign=top>1. &nbsp;</td><td>
Whereas in the initial programming the major emphasis was placed
on efficiency in terms of computing time,
it is believed that more emphasis should be placed
on ease of use at the cost of some loss in efficiency.
In particular the orders required to call a routine
should be as simple and standard as possible.
 </td></tr>
<tr><td valign=top>2.</td><td>
To allow for the possible use of several subroutines
in the same program the subroutines should be so designed
that they may be moved to any section of the sequence storage
simply by assigning a new set of (consecutive) line numbers
to the orders of the program.
The only difficulty arises in the making of calls within
the routine and this may be cared for 
by making all calls relative to the present line number.
This may be done by reading out of the line number register
on any line which is later to be called and placing it in 
some storage register which is later used in making the call.
Alternatively, this may be done by reading out 
of the line number register just prior to making a call
and adding the necessary amount to this number
to give the line number which is to be called.
 </td></tr>
<tr><td valign=top>3.</td><td>
In the present routines allowance was made
for the programming of reruns in the main program
by deferring the completion of checks until
after the main program has been called.
This makes added work in the preparation of the main program.
Also the important advantage of reliability 
of a thoroughly tested and frequently used subroutine
is lost at one of the most crucial points;
namely in the check.
However, the check may be completed in the routine
and allowance made for a rerun by providing for the storage
of a &ldquo;rerun line&rdquo;
in some particular register to be called by the subroutine
in the case of a check failure.
Provision should also be made for introducing
any desired tolerance for use in checks 
within the subroutine.
 </td></tr>
</table>

<p>The machine computations were begun during the final testing
of Mark IV when the machine was first coming into operation.
Hence conditions were far from ideal
and the lack of experience in programming and operation
combined with machine failures to cause a considerable
loss of time.
However it is under just such adverse conditions that 
the relative ease of programming reruns on Mark IV
proves especially valuable.
The reliability of the computer has now been greatly improved
and at present writing it has been operating 
with more than 85% good running time
during the month of November 1953.</p>

<p>The possibilities of the Leontief dynamic economic model
have not been exhausted by the present study.
For example, a step-by-step solution might well be attempted
which could take into account the irreversibility
of certain economic processes such as the investment
in capital goods.
It is believed that the economic interpretation of the present results
will prove valuable in pointing the direction to be taken
by further research.</p>

<a name="notes7"></a>
<p><b>Notes</b></p>

<table>
<tr><td valign=top><a name="note7a"></a>a. &nbsp;</td>
 <td>Further details of the classification schemes 
 and the aggregation of coefficients may be found in 
 <acronym title="33. “Estimates of the capital structure of American industries, 1947”, Harvard Economic Research Project, (hectographed, November, 1953).">references 33</acronym>
 and
 <acronym title="2. Leontief, W., Studies in the Structure of American Economy, Oxford University Press, 1953.">2</acronym>.
 </td></tr>
<tr><td valign=top><a name="note7b"></a>b.</td>
 <td>See Chapter 1 or
 <acronym title="2. Leontief, W., Studies in the Structure of American Economy, Oxford University Press, 1953.">reference 2</acronym>
 for the definitions of the various terms used in this section.
 </td></tr>
<tr><td valign=top><a name="note7c"></a>c.</td>
 <td>The matrix S is the modal matrix whose columns are formed
 of the principal vectors of the matrix (I-A)<sup>-1</sup>B 
 as described in Chapter 3.
 The number at the head of each column of S is the associated latent root.
 These roots therefore make up the diagonal elements
 of the matrix J.
 The principal vectors were obtained from the matrix
 (I-A)<sup>-1</sup>B, (shown in Table 14),
 by the methods described in Chapters 4-6.
 In the tables of Appendix V 
 they are normalized to unit length.
 </td></tr>
</table>
<br>&nbsp;<br>





<a name="bib"></a>
<p><b>Bibliography</b></p>

<table>
<tr><td valign=top>1.</td>
 <td>Leontief, W., <i>The Structure of American Economy, 1919-1939</i>,
 Oxford University Press, 1951.
 </td></tr>
<tr><td valign=top>2.</td>
 <td>Leontief, W., <i>Studies in the Structure of American Economy</i>,
 Oxford University Press, 1953.
 </td></tr>
<tr><td valign=top>3.</td>
 <td>Mitchell, H.F., &ldquo;The Machine Solution of Simultaneous Linear Systems&rdquo;,
 Doctoral Theses, Harvard University, 1948.
 </td></tr>
<tr><td valign=top>4.</td>
 <td>Staff of the Computation Laboratory,
 <i>Description of a Relay Calculator</i>,
 Annals of the Computation Laboratory, Vol. XXIV,
 Harvard University Press, 1949.
 </td></tr>
<tr><td valign=top>5.</td>
 <td>Staff of the Computation Laboratory,
 <i>A Description of the Mark IV Calculator</i>,
 Annals of the Computation Laboratory, Vol. XXVIII,
 Harvard University Press (to be published).
 </td></tr>
<tr><td valign=top>6.</td>
 <td>Staff of the Computation Laboratory,
 &ldquo;Final Report on Functions for Mark IV&rdquo;,
 Harvard Computation Laboratory, 
 <i>Progress Report No. 22</i>, Section II, May 1952.
 </td></tr>
<tr><td valign=top>7.</td>
 <td>Wilkes, M.V., Wheeler, D.J., and Gill, S.,
 <i>The Preparation of Programs for an Electronic Digital Computer</i>,
 Addison-Wesley Press, 1951.
 </td></tr>
<tr><td valign=top>8.</td>
 <td>Aitken, A.C., <i>Determinants and Matrices</i>,
 Oliver and Boyd, Seventh Ed., 1951.
 </td></tr>
<tr><td valign=top>9.</td>
 <td>Frazer, R.A., Duncan, W.J., and Collar, R.A.,
 <i>Elementary Matrices</i>,
 Cambridge University Press, 1950.
 </td></tr>
<tr><td valign=top>10.</td>
 <td>Turnbull, H.W., and Aitken, A.C.,
 <i>The Theory of Canonical Matrices</i>,
 Blackie and Son, Ltd., London, 1952.
 </td></tr>
<tr><td valign=top>11.</td>
 <td>Ger&scaron;gorin, S., 
 &ldquo;&Uuml;ber die Abgrenzung der Eigenwerte einer Matrix&rdquo;,
 <i>Izvestiia Akademie Nauk SSSR, VII, (Mat Klass.)</i>
 (1931) pp. 749-754.
 </td></tr>
<tr><td valign=top>12.</td>
 <td>Aitken, A.C., &ldquo;Studies in Practical Mathematics II.
 The evaluation of the latent roots and latent vectors of a matrix&rdquo;,
 <i>Proc. Roy. Soc., Edinburgh</i> 57, 269-304 (1937).
 </td></tr>
<tr><td valign=top>13.</td>
 <td>Zurmuhl, R., <i>Matrizen</i>, Springer-Verlag, Berlin, 1950.
 </td></tr>
<tr><td valign=top>14.</td>
 <td>Feller, W., and Forsythe, G.E.,
 &ldquo;Matrix Transformations for Obtaining Characteristic Vectors&rdquo;,
 <i>Quarterly Journal of Applied Mathematics</i>, 8 (1950).
 </td></tr>
<tr><td valign=top>15.</td>
 <td>Hotelling, H., &ldquo;Some new methods in matrix calculation&rdquo;,
 <i>Annals of Mathematical Statistics</i>, 14 (1943).
 </td></tr>
<tr><td valign=top>16.</td>
 <td>Dickson, L.E., <i>Theory of Equations</i>,
 John Wiley and Sons, Inc., New York.
 </td></tr>
<tr><td valign=top>17.</td>
 <td>Fettis, D.H.E., &ldquo;A method for obtaining the characteristic equation
 of a matrix and computing the associated modal columns&rdquo;,
 <i>Quart. Jour. Appl. Math.</i> 8, 206-212 (1950).
 </td></tr>
<tr><td valign=top>18.</td>
 <td>Jahn, H.A., &ldquo;Improvement of an approximate set of latent roots
 and modal columns by methods akin to those of classical perturbation theory&rdquo;,
 <i>Quarterly Journal of Mechanics and Applied Mathematics</i>,
 Vol. 1 (1948).
 </td></tr>
<tr><td valign=top>19.</td>
 <td><i>Marchant Methods, MM-225</i>, Marchant Calculating Machine Co.,
 Oakland, California.
 </td></tr>
<tr><td valign=top>20.</td>
 <td>Whittaker, E., and Robinson, G.,
 <i>The Calculation of Observations</i>, 4th Ed., 
 Blackie and Sons, Ltd., 1946.
 </td></tr>
<tr><td valign=top>21.</td>
 <td>Churchill, Ruel, V., <i>Introduction to Complex Variables and Applications</i>,
 McGraw-Hill, 1948.
 </td></tr>
<tr><td valign=top>22.</td>
 <td>Bairstow, L., <i>Applied Aerodynamics</i>,
 Longman&rsquo;s Green &amp; Co., London, 1920.
 </td></tr>
<tr><td valign=top>23.</td>
 <td>Milne, W.E., <i>Numerical Calculus</i>,
 Princeton University Press, 1949.
 </td></tr>
<tr><td valign=top>24.</td>
 <td>Hartree, D.R., <i>Numerical Analysis</i>, 
 Oxford at the Clarendon Press, 1952.
 </td></tr>
<tr><td valign=top>25.</td>
 <td>Aitken, A.C., Studies in Practical Mathematics, VII.,
 &ldquo;On the Theory of Methods of Factorizing Polynomials
 by Iterated Division&rdquo;,
 <i>Proc. Roy. Soc., Edinburgh</i>,
 Vol. 63, p. 326 (1951).
 </td></tr>
<tr><td valign=top>26.<sup>&nbsp;</sup></td>
 <td>Iverson, K.E., 
&ldquo;<a href="http://www.ams.org/journals/mcom/1953-07-043/S0025-5718-1953-0057013-0/S0025-5718-1953-0057013-0.pdf" 
target=_parent>The Zeros of the Partial Sums of e<sup>z&nbsp;</sup></a>&rdquo;,
 <i>Mathematical Tables and Other Aids to Computation</i> 7
 (1953).
 </td></tr>
<tr><td valign=top>27.</td>
 <td>Lovitt, W.V., <i>Elementary Theory of Equations</i>,
 Prentice-Hall, New York 1939.
 </td></tr>
<tr><td valign=top>28.</td>
 <td>Lin, S., &ldquo;A method for finding roots of algebraic equations&rdquo;,
 <i>Journal of Mathematics and Physics</i>,
 Mass. Inst. Tech. 22, (1943).
 </td></tr>
<tr><td valign=top>29.</td>
 <td>Moore, E.F., &ldquo;A new general method for finding roots
 of polynomial equation&rdquo;,
 <i>Math. Tables and Other Aids to Computation</i> 3, (1949).
 </td></tr>
<tr><td valign=top>30.</td>
 <td>Ostrowskis, &ldquo;Recherches sur la methode de Graeffe et les zeros
 des polynomes et des series de Laurent&rdquo;,
 <i>Acta Math.</i> 72, p. 99-155.
 </td></tr>
<tr><td valign=top>31.</td>
 <td>MacDuffee, C.C., &ldquo;Vectors and Matrices&rdquo;,
 The Carus Mathematical Monographs, published by the 
 Mathematical Association of America, revised, January 1949.
 </td></tr>
<tr><td valign=top>32.</td>
 <td>Forsythe, George E., &ldquo;Theory of Selected Methods of Finite Matrix
 Inversion and Decomposition&rdquo;,
 Lecture in Math. 136, notes by D.G. Aronson and K. Iverson.
 INA Report 52-55 (1951).
 </td></tr>
<tr><td valign=top>33.</td>
 <td>&ldquo;Estimates of the capital structure of American industries,
 1947&rdquo;, Harvard Economic Research Project,
 (hectographed, November, 1953).
 </td></tr>
<tr><td valign=top>34. &nbsp;</td>
 <td>Staff of the Computation Laboratory,
 &ldquo;Problems of Critical Frequency for a Propeller Blade Subjected 
 to a Pure First Order Aerodynamic Excitation&rdquo;
 Harvard Computation Laboratory,
 <i>Progress Report No. 29</i>, Section I, November 1953.
 </td></tr>
</table>

<br><hr>

<table>
<tr><td><font size="-1">created: &nbsp;</font></td><td><font size="-1">2012-07-25 06:10</font></td></tr>
<tr><td><font size="-1">updated:</font></td><td><font size="-1">2013-08-02 23:30</font></td></tr>
</table>

</td></tr></table>
<br><br><br><br><br><br><br><br><br>
</body>
</html>
